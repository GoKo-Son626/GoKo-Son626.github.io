{"posts":[{"title":"常见的编译库和编译工具","text":"常见的编译库与编译工具的介绍以及使用场景 1. 编译器 (Compiler) 代码的“翻译官”。将 C/C++ 代码翻译成机器能懂的汇编代码或目标文件 (.o)。 典型代表：GCC, Clang。 使用场景：写任何代码并想让它运行时，第一步就是用编译器进行翻译。 2. 工具链 (Toolchain) 一个完整的“工具箱”。编译器是工具链的核心组件之一**。工具链还包含了链接器 (ld)、汇编器 (as) 等，它们协同工作，将代码和库文件打包成最终的可执行文件。 使用场景： 本地工具链：编译在本机运行的程序 (e.g., gcc)。 交叉工具链：在一种架构（如x86电脑）上，编译给另一种架构（如RISC-V开发板）运行的程序 (e.g., riscv64-linux-gnu-gcc)。 3. libc (C标准库) 一个API标准或规范，不是一个具体的软件。它定义了像 printf, malloc 等基础函数。它是所有C程序的基础依赖。你需要一个具体的实现来使用它。 使用场景：这是一个抽象概念，你写的每一行C代码，只要调用了标准函数，都在与这个“标准”打交道。 4. glibc (GNU C Library) libc 的一种强大、功能全面的实现**。它是为 Linux 操作系统设计的 libc。它不仅包含标准C函数，还包含大量与Linux内核交互的接口（如进程、网络功能）。 使用场景：当你需要开发一个运行在标准Linux系统（如Ubuntu/Debian/CentOS）上的应用程序时，你的程序会链接 glibc。对应的工具链通常叫 ...-linux-gnu-gcc。 5. newlib libc 的一种轻量级、精简的实现**。它是为没有操作系统的环境设计的。因此，它没有 fork 等需要OS支持的复杂功能。 使用场景：开发裸机 (Bare-metal) 程序、固件 (Firmware)、Bootloader，或者在简单的实时操作系统 (RTOS) 上开发。对应的工具链通常叫 ...-elf-gcc。 6. GNU 一个庞大的自由软件生态系统。上面讨论的大部分经典工具都来自GNU项目，包括 GCC (编译器), glibc (C库), GDB (调试器), Make (构建工具)。”GNU Toolchain” 指的就是这一整套工具。 使用场景：Linux 和嵌入式开发的事实标准。 8. ELF (Executable and Linkable Format) 一种文件格式，像 .doc 或 .pdf 一样。它是工具链最终生成的产品。无论是裸机程序还是Linux程序，最终都可以打包成 ELF 格式。 使用场景： 裸机ELF：内部不依赖 glibc，直接在硬件上跑。 Linux ELF：内部依赖 glibc 和Linux内核，必须在Linux系统上跑。 关键：决定它在哪跑的，是它内部链接了什么库，而不是 ELF 这个格式本身。 JIT:(即时编译，Just-In-Time compilation): 动态编译技术，它介于解释执行和提前编译（AOT, Ahead-Of-Time）之间。 解释执行：代码一行行解释运行（如 Python、早期 JavaScript），启动快，但运行慢。 提前编译（AOT）：代码在运行前全部编译成目标机器码（如 C/C++），运行快，但灵活性差。 JIT：在运行时，把中间表示（IR, bytecode）翻译成机器码，并缓存起来，下次直接运行机器码。既能接近原生性能，又保留了灵活性。 JIT 的本质过程：程序一开始以 字节码（或中间表示）形式运行。运行过程中，JIT 编译器发现“这段代码执行得很频繁”（热点代码），于是触发编译。把字节码即时翻译成当前 CPU 架构的机器码（x86、ARM、RISC-V 等）。后续直接执行机器码（免去解释器逐条解释的开销）。 Clang/LLVM: Clang：C/C++ 前端（把 C/C++/Objective-C 源转成通用的标准化的中间语言LLVM IR）。 词法分析：把代码拆成一个个单词（token），比如 int, main, (, ), {, }。 语法分析：检查这些单词组合起来是否符合 C/C++ 的语法规则。如果写了 int main{) 这种错误，Clang 就在这一步报错。 生成中间表示 (IR)：如果语法正确，Clang 会把代码转换成一种通用的、与具体计算机架构无关的格式，这就是 LLVM Intermediate Representation (LLVM IR)。 LLVM：编译器后台/工具链，接收Clang生成的LLVM IR, 进行一系列加工，生成可执行的机器码。 优化 (Optimization)：LLVM 会对 IR 进行大量的优化。比如删除无用的代码、合并重复的计算、展开循环等等，让最终的程序跑得更快、体积更小。这是 LLVM 的核心优势之一。 代码生成 (Code Generation)：这是最关键的一步。LLVM 会根据你指定的 “目标平台 (Target)”，将优化后的 IR 翻译成该平台专属的机器指令。 可移植性：Clang/LLVM 支持很多后端/目标（x86/arm/bpf 等）。当你用 -target bpf 时，Clang 输出的是 eBPF 字节码（虚拟指令），不是 x86 或 arm 机器码。这个字节码理论上可以在任何支持 eBPF 的内核上运行（但内核版本/ABI 细节会影响，CO-RE/BTF 出现就是为了解决这类兼容性问题）。最终在内核里，JIT 会把字节码翻译成当前 CPU 的本地机器码。","link":"/post/Compilation-libraries-and-tools.html"},{"title":"平台总线的结构原理以及基础的代码框架","text":"平台总线是linux系统虚拟出来的一种总线,是一个内核子系统，负责管理 platform_device（硬件描述）和 platform_driver（驱动代码）,使它们先分离.后搭档 平台总线(Platform Bus)(总线控制器信息和控制器驱动之间) 平台总线（Platform Bus）是内核的一条“虚拟”总线。它不像 PCI、USB 那样是物理上存在的总线，而是为了解决一类特殊设备的驱动问题而设计的 软件机制。这类设备通常是 SoC (System on Chip) 芯片内部集成的、不可被自动识别的外设，比如 I2C 控制器、SPI 控制器、GPIO 控制器、LCD 控制器等。 原理：设备与驱动的分离与匹配 (Separation and Matching) 问题： 对于 PCI 或 USB 设备，设备自身带有 ID 信息（Vendor ID, Product ID）。驱动可以根据这些 ID “认领” 设备。但 SoC 上的那些外设，它们的寄存器地址、中断号都是固定的，写死在芯片里了，没法自动发现。 解决： 把 设备信息 和 驱动代码 分开！” 平台设备 (platform_device)： 这是一块纯粹的“数据”，用来描述硬件资源。它告诉内核：“在物理地址 0x12345678 有个设备，它使用中断号 5，它的名字叫 my-i2c-controller”。这些信息通常写在 设备树 (Device Tree, .dts 文件) 中，或者早期的板级配置文件 (board-xxx.c)里。 平台驱动 (platform_driver)： 真正的驱动代码。注册时告诉内核：是一个驱动，我能处理名字叫 my-i2c-controller 的设备。 匹配 (Match)： 当一个 platform_device 和一个 platform_driver 被注册到内核时，平台总线核心会进行匹配。最常见的匹配方式就是看 名字 是否一样。 探测 (Probe)： 一旦匹配成功，总线核心就会调用平台驱动的 .probe 函数。在这个函数里，驱动程序会通过相关的API函数从 platform_device 结构体中获取到设备的硬件资源（如内存地址、中断号），然后用这些信息去初始化硬件，完成驱动的加载。 流程： 系统启动，内核解析设备树。 内核在设备树里读到一段描述 I2C 控制器硬件的节点（包含了寄存器地址、中断号，以及最重要的 compatible = “vendor,i2c-controller-v1”;）。 内核根据这个节点，创建并注册一个 platform_device 到平台总线。 I2C 控制器驱动（platform_driver）在加载时，会告诉平台总线：“我能处理 compatible 是 “vendor,i2c-controller-v1” 的设备”。 平台总线看到两者匹配，于是调用 I2C 控制器驱动的 .probe 函数。 在 I2C 控制器驱动的 .probe 函数中，驱动程序会执行一系列初始化操作，其中最重要的一步是调用 i2c_add_adapter() 或 i2c_add_numbered_adapter()。这个函数调用，才是在内核中“建立”或“注册”了一条 I2C 总线（即一个 i2c_adapter）。这条逻辑上的总线就代表了那条物理的 I2C 总线。内核里的 i2c_adapter 就是物理 I2C 总线在软件层面的抽象。 设备间交互： “其他设备驱动”（比如 I2C 温度传感器驱动）不直接调用 I2C 控制器驱动里的 ops。这是一个分层概念。 正确的交互方式： I2C 控制器驱动把它实现底层 I/O 操作的 ops（struct i2c_algorithm）注册给了 I2C 总线核心。 I2C 温度传感器驱动想通信时，它调用的是 I2C 总线核心提供的标准、统一的 API，如 i2c_master_send() 和 i2c_master_recv()。 I2C 总线核心在收到这些 API 调用后，会找到对应的 i2c_adapter，然后去调用这个 adapter 在注册时提供的 ops 里的具体函数，最终由 I2C 控制器驱动的代码来操作硬件。 platform bus 设备和驱动1. platform_device结构体1234567891011121314151617181920212223struct platform_device { const char *name; // 显示在/sys/bus/platform/devices/name.id(.auto) int id; // 用来区分不同设备：name.id, id = -1: 没有后缀 bool id_auto; // 自动设置id：name.id.auto struct device dev; // 设备的通用属性部分 u64 platform_dma_mask; struct device_dma_parameters dma_parms; u32 num_resources; // 存储的资源的个数 struct resource *resource; // 存储资源 const struct platform_device_id *id_entry; /* * Driver name to force a match. Do not set directly, because core * frees it. Use driver_set_override() to set or clear it. */ const char *driver_override; /* MFD cell pointer */ struct mfd_cell *mfd_cell; /* arch specific additions */ struct pdev_archdata archdata;}; 成员结构体: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123struct device { struct kobject kobj; struct device *parent; struct device_private *p; const char *init_name; /* initial name of the device */ const struct device_type *type; const struct bus_type *bus; /* type of bus device is on */ struct device_driver *driver; /* which driver has allocated this device */ void *platform_data; /* Platform specific data, device core doesn't touch it */ void *driver_data; /* Driver data, set and get with dev_set_drvdata/dev_get_drvdata */ struct mutex mutex; /* mutex to synchronize calls to * its driver. */ struct dev_links_info links; struct dev_pm_info power; struct dev_pm_domain *pm_domain;#ifdef CONFIG_ENERGY_MODEL struct em_perf_domain *em_pd;#endif#ifdef CONFIG_PINCTRL#endif struct dev_pin_info *pins; struct dev_msi_info msi;#ifdef CONFIG_ARCH_HAS_DMA_OPS const struct dma_map_ops *dma_ops;#endif u64 *dma_mask; /* dma mask (if dma'able device) */ u64 coherent_dma_mask;/* Like dma_mask, but for alloc_coherent mappings as not all hardware supports 64 bit addresses for consistent allocations such descriptors. */ u64 bus_dma_limit; /* upstream dma constraint */ const struct bus_dma_region *dma_range_map; struct device_dma_parameters *dma_parms; struct list_head dma_pools; /* dma pools (if dma'ble) */#ifdef CONFIG_DMA_DECLARE_COHERENT struct dma_coherent_mem *dma_mem; /* internal for coherent mem override */#endif#ifdef CONFIG_DMA_CMA struct cma *cma_area; /* contiguous memory area for dma allocations */#endif#ifdef CONFIG_SWIOTLB struct io_tlb_mem *dma_io_tlb_mem;#endif#ifdef CONFIG_SWIOTLB_DYNAMIC struct list_head dma_io_tlb_pools; spinlock_t dma_io_tlb_lock; bool dma_uses_io_tlb;#endif /* arch specific additions */ struct dev_archdata archdata; struct device_node *of_node; /* associated device tree node */ struct fwnode_handle *fwnode; /* firmware device node */#ifdef CONFIG_NUMA int numa_node; /* NUMA node this device is close to */#endif dev_t devt; /* dev_t, creates the sysfs &quot;dev&quot; */ u32 id; /* device instance */ spinlock_t devres_lock; struct list_head devres_head; const struct class *class; const struct attribute_group **groups; /* optional groups */ void (*release)(struct device *dev); // 必须编写，不然驱动编译不过去 struct iommu_group *iommu_group; struct dev_iommu *iommu; struct device_physical_location *physical_location; enum device_removable removable; bool offline_disabled:1; bool offline:1; bool of_node_reused:1; bool state_synced:1; bool can_match:1;#if defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_DEVICE) || \\ defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU) || \\ defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU_ALL) bool dma_coherent:1;#endif#ifdef CONFIG_DMA_OPS_BYPASS bool dma_ops_bypass : 1;#endif#ifdef CONFIG_DMA_NEED_SYNC bool dma_skip_sync:1;#endif#ifdef CONFIG_IOMMU_DMA bool dma_iommu:1;#endif};/* * Resources are tree-like, allowing * nesting etc.. */struct resource { resource_size_t start; // 资源的起始信息和终止信息 resource_size_t end; // etc：中断的起始地址和终止地址 const char *name; // 存储信息名称：etc：中断-irq unsigned long flags; // 存储资源类型：etc: IORESOURCE_IO/MEM/REG/IRQ/DMA/BUS... unsigned long desc; // 描述信息 struct resource *parent, *sibling, *child; // 节点相关}; 2. 简单的platform_device insmod注册成功：ls /sys/bus/platform/devices/mydevice 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#include &lt;linux/module.h&gt;#include &lt;linux/init.h&gt;#include &lt;linux/platform_device.h&gt;static struct resource mydevice_resource[] = { [0] = { .start = 0xFDD60000, .end = 0xFDD50004, .flags = IORESOURCE_IO, }, [1] = { .start = 13, .end = 13, .flags = IORESOURCE_IRQ, },}void mydevice_release(struct device *dev){ printk(&quot;This is mydevice_release\\n&quot;);}static platform_device platform_device_test = { .name = &quot;mydevice&quot;, .id = -1, .resource = mydevice_resource, .num_resources = ARRAY_SIZE(mydevice_resource), .dev = { .release = mydevice_release, },};static int platform_device_init(void){ platform_device_register(&amp;platform_device_test); printk(&quot;platform_device init\\n&quot;); return 0;}static int platform_device_exit(void){ platform_device_unregister(&amp;platform_device_test); printk(&quot;platform_device exit\\n&quot;); return 0;}module init(platform_device_init);module exit(platform_device_exit);MODULE_LICENSE(&quot;GPL&quot;);MODULE_AUTHOR(&quot;goko&quot;);MODULE_VERSION(&quot;V1.0&quot;); 3. platform_driver结构体123456789101112131415161718struct platform_driver { int (*probe)(struct platform_device *); // 匹配成功之后执行 void (*remove)(struct platform_device *); // 设备移除时执行 void (*shutdown)(struct platform_device *); // 设备关闭时执行dy int (*suspend)(struct platform_device *, pm_message_t state); // 设备挂起时执行dy int (*resume)(struct platform_device *); // 设备恢复时执行dy struct device_driver driver; // 设备公用的一些属性 const struct platform_device_id *id_table; // 设备id表 bool prevent_deferred_probe; /* * For most device drivers, no need to care about this flag as long as * all DMAs are handled through the kernel DMA API. For some special * ones, for example VFIO drivers, they know how to manage the DMA * themselves and set this flag so that the IOMMU layer will allow them * to setup and manage their own I/O address space. */ bool driver_managed_dma;}; 4. 简单的platform_driver insmod注册成功：ls /sys/bus/platform/drivers/mydevice 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287#include &lt;linux/module.h&gt; // 所有模块都需要#include &lt;linux/init.h&gt; // __init 和 __exit 宏#include &lt;linux/fs.h&gt; // file_operations 结构体和文件系统相关函数#include &lt;linux/cdev.h&gt; // cdev 结构体和相关函数#include &lt;linux/uaccess.h&gt; // copy_to_user, copy_from_user#include &lt;linux/device.h&gt; // class_create, device_create#include &lt;linux/io.h&gt; // ioremap, iounmap#include &lt;linux/platform_device.h&gt; // platform_driver 和 platform_device#include &lt;linux/of.h&gt; // of_match_ptr, 设备树相关（如果使用设备树匹配）#include &lt;linux/slab.h&gt; // kzalloc, kfree#define DRIVER_NAME &quot;my_platform_device&quot; // 定义驱动名称#define DEVICE_COUNT 1 // 定义设备数量// 驱动的私有数据结构体，用于存储设备相关的所有信息// 这个结构体整合了你截图中`struct device_test`的所有成员struct mydevice_dev { dev_t dev_num; // 设备号 (主设备号 + 次设备号) struct cdev cdev_test; // 字符设备结构体 struct class *class; // 设备类，用于在/sys/class/下创建条目 struct device *device; // 设备实例，用于在/dev/下创建设备文件 char kbuf[32]; // 内核缓冲区，用于与用户空间交换数据 void __iomem *vir_gpio_dr; // 经过ioremap映射后的虚拟地址};// 全局指针，指向我们的私有数据结构体// 在probe中分配，在remove中释放// 注意：更好的做法是通过 platform_set_drvdata/platform_get_drvdata 来管理，这里为了清晰展示，先用一个全局指针// 稍后会展示更标准的做法struct mydevice_dev *global_mydev; // --- 文件操作函数集 (file_operations) ---static int mydevice_open(struct inode *inode, struct file *file){ struct mydevice_dev *dev; printk(KERN_INFO &quot;mydevice: device opened\\n&quot;); // 通过 inode 中的 cdev 指针，找到包含它的父结构体 mydevice_dev // 这是内核中非常常见和重要的技巧 dev = container_of(inode-&gt;i_cdev, struct mydevice_dev, cdev_test); // 将设备私有结构体的指针存放在 file-&gt;private_data 中 // 这样，在后续的 read/write/release 操作中，就可以直接从 file 中获取，无需再次查找 file-&gt;private_data = dev; return 0;}static int mydevice_release(struct inode *inode, struct file *file){ printk(KERN_INFO &quot;mydevice: device closed\\n&quot;); // 这里不需要释放 file-&gt;private_data，因为它指向的是在 probe 中分配的内存 // 该内存的生命周期与驱动绑定，而不是与文件的打开/关闭绑定 return 0;}static ssize_t mydevice_read(struct file *file, char __user *buf, size_t size, loff_t *off){ // 从 file-&gt;private_data 中获取设备私有结构体指针 struct mydevice_dev *dev = file-&gt;private_data; size_t len = strlen(dev-&gt;kbuf); int ret; printk(KERN_INFO &quot;mydevice: reading data: %s\\n&quot;, dev-&gt;kbuf); if (size &gt; len) { size = len; } // 将内核空间的数据 (dev-&gt;kbuf) 拷贝到用户空间 (buf) ret = copy_to_user(buf, dev-&gt;kbuf, size); if (ret != 0) { printk(KERN_ERR &quot;mydevice: copy_to_user failed\\n&quot;); return -EFAULT; // 返回一个标准的错误码 } // 在这里，一个简单的实现是每次读取后返回已读取的字节数 // 一个更完整的实现需要处理 *off，以支持多次读取文件的不同部分 return size;}static ssize_t mydevice_write(struct file *file, const char __user *buf, size_t size, loff_t *off){ // 从 file-&gt;private_data 中获取设备私有结构体指针 struct mydevice_dev *dev = file-&gt;private_data; int ret; if (size &gt;= sizeof(dev-&gt;kbuf)) { printk(KERN_WARNING &quot;mydevice: write size is too large\\n&quot;); // 截断写入的数据，防止缓冲区溢出 size = sizeof(dev-&gt;kbuf) - 1; } // 将用户空间的数据 (buf) 拷贝到内核空间 (dev-&gt;kbuf) ret = copy_from_user(dev-&gt;kbuf, buf, size); if (ret != 0) { printk(KERN_ERR &quot;mydevice: copy_from_user failed\\n&quot;); return -EFAULT; } // 给内核缓冲区加上字符串结束符 dev-&gt;kbuf[size] = '\\0'; printk(KERN_INFO &quot;mydevice: written data: %s\\n&quot;, dev-&gt;kbuf); // 在一个真实的GPIO驱动中，这里会解析 kbuf 中的命令（如&quot;on&quot;或&quot;off&quot;） // 然后通过 dev-&gt;vir_gpio_dr 指针向硬件寄存器写入值 // 例如：iowrite32(1, dev-&gt;vir_gpio_dr); return size; // 返回成功写入的字节数}// 定义 file_operations 结构体，并将我们的函数与之关联static const struct file_operations mydevice_fops = { .owner = THIS_MODULE, .open = mydevice_open, .release = mydevice_release, .read = mydevice_read, .write = mydevice_write,};// --- Platform 驱动核心函数 ---// 当内核匹配到同名的 platform_device 时，会调用此 probe 函数static int mydriver_probe(struct platform_device *pdev){ int ret; struct resource *mem_res; struct mydevice_dev *dev; printk(KERN_INFO &quot;mydriver_probe: device probed!\\n&quot;); // 1. 分配私有数据结构体内存 // 使用 devm_kzalloc, &quot;devm_&quot; 开头的函数是受设备管理的，当设备卸载时会自动释放资源，非常方便 dev = devm_kzalloc(&amp;pdev-&gt;dev, sizeof(struct mydevice_dev), GFP_KERNEL); if (!dev) { return -ENOMEM; } global_mydev = dev; // 赋值给全局指针（仅为示例） // 2. 从 platform_device 获取资源 (这里以内存资源为例) // 参数: platform_device指针, 资源类型, 索引(第0个内存资源) mem_res = platform_get_resource(pdev, IORESOURCE_MEM, 0); if (!mem_res) { printk(KERN_ERR &quot;mydriver: failed to get memory resource\\n&quot;); return -EINVAL; } printk(KERN_INFO &quot;mydriver: mem resource start: 0x%pa, size: %lld\\n&quot;, &amp;mem_res-&gt;start, resource_size(mem_res)); // 3. 将物理地址映射到内核虚拟地址空间 // devm_ioremap_resource 会自动处理 ioremap 和 iounmap，非常推荐使用 dev-&gt;vir_gpio_dr = devm_ioremap_resource(&amp;pdev-&gt;dev, mem_res); if (IS_ERR(dev-&gt;vir_gpio_dr)) { printk(KERN_ERR &quot;mydriver: failed to ioremap memory resource\\n&quot;); return PTR_ERR(dev-&gt;vir_gpio_dr); } // ======== 以下是字符设备创建的标准流程 (来自你的截图逻辑) ======== // 4. 动态申请设备号 ret = alloc_chrdev_region(&amp;dev-&gt;dev_num, 0, DEVICE_COUNT, DRIVER_NAME); if (ret &lt; 0) { printk(KERN_ERR &quot;mydriver: failed to allocate chrdev region\\n&quot;); return ret; } printk(KERN_INFO &quot;mydriver: allocated major=%d, minor=%d\\n&quot;, MAJOR(dev-&gt;dev_num), MINOR(dev-&gt;dev_num)); // 5. 初始化 cdev 结构体，并绑定 file_operations cdev_init(&amp;dev-&gt;cdev_test, &amp;mydevice_fops); dev-&gt;cdev_test.owner = THIS_MODULE; // 6. 将 cdev 添加到内核中 ret = cdev_add(&amp;dev-&gt;cdev_test, dev-&gt;dev_num, DEVICE_COUNT); if (ret &lt; 0) { printk(KERN_ERR &quot;mydriver: failed to add cdev\\n&quot;); goto err_unregister_chrdev; } // 7. 创建设备类 /sys/class/my_platform_device dev-&gt;class = class_create(THIS_MODULE, DRIVER_NAME); if (IS_ERR(dev-&gt;class)) { ret = PTR_ERR(dev-&gt;class); printk(KERN_ERR &quot;mydriver: failed to create class\\n&quot;); goto err_cdev_del; } // 8. 创建设备文件 /dev/my_platform_device dev-&gt;device = device_create(dev-&gt;class, NULL, dev-&gt;dev_num, NULL, DRIVER_NAME); if (IS_ERR(dev-&gt;device)) { ret = PTR_ERR(dev-&gt;device); printk(KERN_ERR &quot;mydriver: failed to create device\\n&quot;); goto err_class_destroy; } // 9. 将私有数据结构体指针与 platform_device 关联 // 这样在 remove 函数中就可以通过 platform_get_drvdata 获取它 platform_set_drvdata(pdev, dev); // 初始化内核缓冲区 strcpy(dev-&gt;kbuf, &quot;Hello from kernel!&quot;); printk(KERN_INFO &quot;mydriver: probe successful, device created at /dev/%s\\n&quot;, DRIVER_NAME); return 0;// 错误处理：按相反的顺序释放已申请的资源err_class_destroy: class_destroy(dev-&gt;class);err_cdev_del: cdev_del(&amp;dev-&gt;cdev_test);err_unregister_chrdev: unregister_chrdev_region(dev-&gt;dev_num, DEVICE_COUNT); // devm_kzalloc 和 devm_ioremap_resource 分配的资源会自动释放，无需手动处理 return ret;}// 当驱动被卸载或设备被移除时，调用此 remove 函数static int mydriver_remove(struct platform_device *pdev){ // 通过 platform_get_drvdata 获取在 probe 中设置的私有数据 struct mydevice_dev *dev = platform_get_drvdata(pdev); printk(KERN_INFO &quot;mydriver_remove: removing device\\n&quot;); // 按照与 probe 相反的顺序销毁和释放资源 // 注意：devm_ 家族函数管理的资源（内存、ioremap）不需要在这里手动释放！ // 驱动核心会在这个函数返回后自动清理它们。 // 销毁设备文件 /dev/my_platform_device device_destroy(dev-&gt;class, dev-&gt;dev_num); // 销毁设备类 /sys/class/my_platform_device class_destroy(dev-&gt;class); // 从内核中删除 cdev cdev_del(&amp;dev-&gt;cdev_test); // 注销设备号 unregister_chrdev_region(dev-&gt;dev_num, DEVICE_COUNT); printk(KERN_INFO &quot;mydriver_remove: remove successful\\n&quot;); return 0;}// ID 表，用于匹配 platform_device// 当一个 platform_device 的 .name 字段与这里的 .name 匹配时，probe 就会被调用static const struct platform_device_id mydriver_id_table[] = { { .name = &quot;my-platform-device-example&quot; }, // 这个名字需要与 platform_device 注册时使用的名字完全一致 { /* sentinel */ }, // 结尾的空条目，表示列表结束};MODULE_DEVICE_TABLE(platform, mydriver_id_table); // 将id_table导出，让内核和用户空间知道// 定义 platform_driver 结构体static struct platform_driver my_platform_driver = { .probe = mydriver_probe, .remove = mydriver_remove, .driver = { .name = &quot;my-platform-device-example&quot;, // 驱动的名字 .owner = THIS_MODULE, }, .id_table = mydriver_id_table, // 关联ID匹配表};// 模块加载函数static int __init my_driver_init(void){ printk(KERN_INFO &quot;my_driver_init: Registering platform driver\\n&quot;); // 注册 platform_driver 到内核 return platform_driver_register(&amp;my_platform_driver);}// 模块卸载函数static void __exit my_driver_exit(void){ printk(KERN_INFO &quot;my_driver_exit: Unregistering platform driver\\n&quot;); // 从内核中注销 platform_driver platform_driver_unregister(&amp;my_platform_driver);}// 注册模块加载和卸载函数module_init(my_driver_init);module_exit(my_driver_exit);// 模块许可和信息MODULE_LICENSE(&quot;GPL&quot;);MODULE_AUTHOR(&quot;Your Name&quot;);MODULE_DESCRIPTION(&quot;A complete platform device driver example&quot;);MODULE_VERSION(&quot;1.0&quot;); 平台总线和设备驱动 启动与准备 (第 0 步): 内核启动，解析设备树。 它看到一个描述 I2C 控制器的节点，于是创建了一个 platform_device。 它看到 I2C 控制器节点下还有一个描述温度传感器的子节点，于是为它创建了一个 i2c_client 的描述信息（但此时还未注册，因为 I2C 总线还不存在）。 第一层匹配 (平台总线): 你加载了 I2C 控制器驱动 (platform_driver)。 平台总线发现这个驱动和之前创建的 platform_device 匹配。 平台总线调用 I2C 控制器驱动 的 .probe() 函数。 桥梁搭建 (控制器驱动的工作): 在 I2C 控制器驱动 的 .probe() 函数中，驱动初始化了硬件，然后调用 i2c_add_adapter()。 这个调用是关键！ 它在内核里创建并注册了一条功能完备的 I2C 总线。 第二层匹配 (I2C 总线): 新的 I2C 总线被注册后，内核的 I2C 核心会把之前为温度传感器准备的 i2c_client 描述信息，正式注册到这条新的 I2C 总线上。 现在，你加载了温度传感器驱动 (i2c_driver)。 I2C 总线发现这个驱动和刚刚注册的 i2c_client 匹配。 I2C 总线调用温度传感器驱动的 .probe() 函数。 最终通信 (设备驱动的工作): 在温度传感器驱动的 .probe() 或其他函数里，它想读取温度。 它调用一个标准的、与硬件无关的函数 i2c_master_recv()。 I2C 核心收到调用，查找该设备挂在哪条 I2C 总线上。 它找到了由I2C 控制器驱动注册的那条总线，然后调用了该控制器驱动提供的底层传输函数。 I2C 控制器驱动的代码开始执行，通过操作寄存器来命令物理 I2C 控制器去和物理温度传感器通信，并取回数据。 总结：关系一览 角色 所在的“层” 它的“驱动”是？ 它的“设备描述”是？ 它和下一层的关系 I2C 控制器 平台总线层 platform_driver platform_device 它的驱动创建了 I2C 总线层 I2C 终端设备 I2C 总线层 i2c_driver i2c_client 它的驱动使用 I2C 总线层提供的服务","link":"/post/Platform-bus.html"},{"title":"Linux bpf技术解析及libbpf的使用(基于riscv-k1)","text":"了解ebpf并在riscv平台上支持ebpf, 最后理解并使用libbpf库中的示例 BPF 和 eBPF BPF （extened Berkeley Packet Filter）: BPF提供了一种在各种内核事件和应用程序事件发生时运行一小段程序的机制。该技术将内核变成完全可编程，允许用户定制和控制它们的系统，以解决现实问题。提供了一套内核接口/bpf() 系统调用集合。 BPF是一项灵活而高效的技术，由指令集、存储对象和辅助函数等几部分组成。由于它采用了虚拟指令集规范，因此也可将它视作一种虚拟机实现。这些指令由Linux内核的BPF运行时模块执行。 eBPF（extened Berkeley Packet Filter）: 扩展版 BPF（64-bit regs、更多指令、复杂验证、maps、helper 函数、更广泛用途）。现在说 BPF/ eBPF 基本都指 eBPF。 原理 eBPF 的工作原理主要分为三个步骤：加载、编译和执行。 eBPF 需要在内核中运行。这通常是由用户态的应用程序完成的，它会通过系统调用来加载 eBPF 程序。在加载过程中，内核会将 eBPF 程序的代码复制到内核空间。 eBPF 程序需要经过编译和执行。这通常是由Clang/LLVM的编译器完成，然后形成字节码后，将用户态的字节码装载进内核，Verifier会对要注入内核的程序进行一些内核安全机制的检查,这是为了确保 eBPF 程序不会破坏内核的稳定性和安全性。在检查过程中，内核会对 eBPF 程序的代码进行分析，以确保它不会进行恶意操作，如系统调用、内存访问等。如果 eBPF 程序通过了内核安全机制的检查，它就可以在内核中正常运行了，其会通过通过一个JIT编译步骤将程序的通用字节码转换为机器特定指令集，以优化程序的执行速度。 JIT:(即时编译，Just-In-Time compilation): 动态编译技术，它介于解释执行和提前编译（AOT, Ahead-Of-Time）之间。 解释执行：代码一行行解释运行（如 Python、早期 JavaScript），启动快，但运行慢。 提前编译（AOT）：代码在运行前全部编译成目标机器码（如 C/C++），运行快，但灵活性差。 JIT：在运行时，把中间表示（IR, bytecode）翻译成机器码，并缓存起来，下次直接运行机器码。既能接近原生性能，又保留了灵活性。 JIT 的本质过程：程序一开始以 字节码（或中间表示）形式运行。运行过程中，JIT 编译器发现“这段代码执行得很频繁”（热点代码），于是触发编译。把字节码即时翻译成当前 CPU 架构的机器码（x86、ARM、RISC-V 等）。后续直接执行机器码（免去解释器逐条解释的开销）。 Clang/LLVM: Clang：C/C++ 前端（把 C/C++/Objective-C 源转成通用的标准化的中间语言LLVM IR）。 词法分析：把代码拆成一个个单词（token），比如 int, main, (, ), {, }。 语法分析：检查这些单词组合起来是否符合 C/C++ 的语法规则。如果写了 int main{) 这种错误，Clang 就在这一步报错。 生成中间表示 (IR)：如果语法正确，Clang 会把代码转换成一种通用的、与具体计算机架构无关的格式，这就是 LLVM Intermediate Representation (LLVM IR)。 LLVM：编译器后台/工具链，接收Clang生成的LLVM IR, 进行一系列加工，生成可执行的机器码。 优化 (Optimization)：LLVM 会对 IR 进行大量的优化。比如删除无用的代码、合并重复的计算、展开循环等等，让最终的程序跑得更快、体积更小。这是 LLVM 的核心优势之一。 代码生成 (Code Generation)：这是最关键的一步。LLVM 会根据你指定的 “目标平台 (Target)”，将优化后的 IR 翻译成该平台专属的机器指令。 可移植性：Clang/LLVM 支持很多后端/目标（x86/arm/bpf 等）。当你用 -target bpf 时，Clang 输出的是 eBPF 字节码（虚拟指令），不是 x86 或 arm 机器码。这个字节码理论上可以在任何支持 eBPF 的内核上运行（但内核版本/ABI 细节会影响，CO-RE/BTF 出现就是为了解决这类兼容性问题）。最终在内核里，JIT 会把字节码翻译成当前 CPU 的本地机器码。 所以，Clang/LLVM 实现了第一层可移植性（源码 -&gt; eBPF 字节码），而内核的 JIT 编译器实现了第二层可移植性（eBPF 字节码 -&gt; 具体 CPU 机器码）。 ebpf结构图 用户空间程序与内核中的 BPF 字节码交互的流程主要如下： 我们可以使用 LLVM 或者 GCC 工具将编写的 BPF 代码程序编译成 BPF 字节码； 然后使用加载程序 Loader 将字节码加载至内核；内核使用验证器（verfier） 组件保证执行字节码的安全性，以避免对内核造成灾难，在确认字节码安全后将其加载对应的内核模块执行；BPF 观测技术相关的程序程序类型可能是 kprobes/uprobes/tracepoint/perf_events 中的一个或多个，其中：kprobes：实现内核中动态跟踪。kprobes 可以跟踪到 Linux 内核中的导出函数入口或返回点，但是不是稳定 ABI 接口，可能会因为内核版本变化导致，导致跟踪失效。uprobes：用户级别的动态跟踪。与 kprobes 类似，只是跟踪用户程序中的函数。tracepoints：内核中静态跟踪。tracepoints 是内核开发人员维护的跟踪点，能够提供稳定的 ABI 接口，但是由于是研发人员维护，数量和场景可能受限。perf_events：定时采样和 PMC。 内核中运行的 BPF 字节码程序可以使用两种方式将测量数据回传至用户空间 maps 方式可用于将内核中实现的统计摘要信息（比如测量延迟、堆栈信息）等回传至用户空间； perf-event 用于将内核采集的事件实时发送至用户空间，用户空间程序实时读取分析； 用途和优势 用途 网络监控：eBPF 可以用于捕获网络数据包，并执行特定的逻辑来分析网络流量。例如，可以使用 eBPF 程序来监控网络流量，并在发现异常流量时进行警报。 安全过滤：eBPF 可以用于对网络数据包进行安全过滤。例如，可以使用 eBPF 程序来阻止恶意流量的传播，或者在发现恶意流量时对其进行拦截。 性能分析：eBPF 可以用于对内核的性能进行分析。例如，可以使用 eBPF 程序来收集内核的性能指标，并通过特定的接口将其可视化。这样，可以更好地了解内核的性能瓶颈，并进行优化。 虚拟化：eBPF 可以用于虚拟化技术。例如，可以使用 eBPF 程序来收集虚拟机的性能指标，并进行负载均衡。这样，可以更好地利用虚拟化环境的资源，提高系统的性能和稳定性。 优势 安全：验证器确保了任何 eBPF 程序都不会搞垮内核。 可移植：同一份 eBPF 字节码，理论上可以在任何架构（x86, ARM, RISC-V）的 Linux 内核上运行，因为 JIT(见下文) 会为它们翻译出对应的原生机器码。 高性能：因为 JIT 的存在，最终运行的是原生机器码，速度接近于原生内核代码。 在k1上安装并配置ebpf的环境和工具 烧录官方提供的bianbu镜像v3.0 但是启动后发现：1234567891011121314151617root@k1:/sys/kernel# cat /boot/config-6.1.15 | grep BPFCONFIG_BPF=yCONFIG_HAVE_EBPF_JIT=yBPF subsystemCONFIG_BPF_SYSCALL=yCONFIG_BPF_JIT is not set # JIT 编译器被关闭 (JIT Compiler is Disabled)CONFIG_BPF_UNPRIV_DEFAULT_OFF=yend of BPF subsystemCONFIG_CGROUP_BPF=yCONFIG_NETFILTER_XT_MATCH_BPF=yCONFIG_BPFILTER is not setCONFIG_NET_CLS_BPF is not set # 缺少网络核心功能CONFIG_BPF_STREAM_PARSER is not setCONFIG_NBPFAXI_DMA is not setCONFIG_BPF_EVENTS=yroot@k1:/sys/kernel# grep BTF /boot/config-x.x.xCONFIG_DEBUG_INFO_BTF=y # 内核没有编译进 BTF (BPF Type Format) 信息 编译源码替换内核镜像 备份旧文件 123cd /bootcp vmlinuz-6.6.63 vmlinuz-6.6.63.bakcp spacemit/6.6.63/k1-x_evb.dtb spacemit/6.6.63/k1-x_evb.dtb.bak vmlinuz只有一个，但是dtb有多个，需要确定是哪一个 这里查看dtb和model输出 1234567891011root@k1:~# ls /boot/spacemit/6.6.63/k1-x_baton-camera.dtb k1-x_fpga.dtb k1-x_MUSE-Card.dtb k1-x_som.dtbk1-x_bit-brick.dtb k1-x_FusionOne.dtb k1-x_MUSE-N1.dtb k1-x_uav.dtbk1-x_deb1.dtb k1-x_InnoBoard-Pi.dtb k1-x_MUSE-Paper2.dtb k1-x_ZT001H.dtbk1-x_deb2.dtb k1-x_lpi3a.dtb k1-x_MUSE-Paper-mini-4g.dtb k1-x_ZT_RVOH007.dtbk1-x_evb.dtb k1-x_LX-V10.dtb k1-x_MUSE-Pi.dtb m1-x_milkv-jupiter.dtbk1-x_evb.dtb.bak k1-x_milkv-jupiter.dtb k1-x_MUSE-Pi-Pro.dtbk1-x_fpga_1x4.dtb k1-x_MINI-PC.dtb k1-x_NetBridge-C1.dtbk1-x_fpga_2x2.dtb k1-x_MUSE-Book.dtb k1-x_RV4B.dtbroot@k1:~# cat /proc/device-tree/modelspacemit k1-x deb1 boardroot 然后就感觉可能是其中的k1-x_deb1.dtb 但是不放心就又去查看Uboot启动时加载的dtb 123456=&gt; printenv bootboot_default boot_device boot_devnum bootcmd bootdelay bootfs_devnamebootfs_part bootmenu_0 bootmenu_1 bootmenu_2 bootmenu_3 bootmenu_4bootmenu_5 bootmenu_6 bootmenu_7 bootmenu_8 bootmenu_9 bootmenu_delay=&gt; printenv bootcmdbootcmd=run autoboot; echo &quot;run autoboot&quot; 再查看autoboot: 显示如果从 eMMC 启动，那么就执行 mmc_boot 这个脚本。 12printenv autobootautoboot=if test ${boot_device} = nand; then run nand_boot; elif test ${boot_device} = nor; then run nor_boot; elif test ${boot_device} = mmc; then run mmc_boot; fi; 查看mmc_boot: 执行 detect_dtb 脚本。 1234=&gt; printenv mmc_boot mmc_boot=echo &quot;Try to boot from ${bootfs_devname}${boot_devnum} ...&quot;; run commonargs; run set_mmc_root; run set_mmc_args; run detect_dtb; run loadknl; run loaddtb; run loadramdisk; bootm ${kernel_addr_r} ${ramdisk_combo} ${dtb_addr}; echo &quot;########### boot kernel failed by d=&gt; printenv detect_dtb detect_dtb=echo &quot;product_name: ${product_name}&quot;; run dtb_env; echo &quot;select ${dtb_name} to load&quot;; 继续查看dev_env 12345=&gt; printenv dtb_envdtb_env=if test -n &quot;${product_name}&quot;; then if test &quot;${product_name}&quot; = k1_evb; then setenv dtb_name ${dtb_dir}/k1-x_evb.dtb; elif test &quot;${product_name}&quot; = k1_deb1; then setenv dtb_name ${dtb_dir}/k1-x_deb1.dtb; elif test &quot;${product_name}&quot; = k1_deb2; then setenv dtb_name ${dt=&gt; printenv product_name product_name=k1-x_deb1=&gt; printenv dtb_namedtb_name=k1-x_evb.dtb 最后发现加载的dtb为k1-x_exb.dtb 最后备份并且替换的是k1-x_exb.dtb文件 clone linux6.6内核源码 git clone https://gitee.com/bianbu-linux/linux-6.6.git –depth = 1 在原有基础上检查并打开配置并编译123456789101112131415161718192021make k1_defconfigmake menuconfig # 建议手动开启，可以打开对应更高级配置# eBPF 核心与性能 (必须开启)./scripts/config --enable BPF_JIT./scripts/config --enable BPF_JIT_ALWAYS_ON./scripts/config --enable DEBUG_INFO_BTF# eBPF 网络功能 (推荐开启)./scripts/config --enable NET_CLS_BPF./scripts/config --enable NET_ACT_BPF./scripts/config --enable XDP_SOCKETS# eBPF 追踪与调试功能 (必须开启)./scripts/config --enable BPF_EVENTS./scripts/config --enable KPROBES./scripts/config --enable UPROBES./scripts/config --enable TRACEPOINTS./scripts/config --enable FTRACE./scripts/config --enable KPROBE_EVENTS./scripts/config --enable UPROBE_EVENTS./scripts/config --enable FTRACE_SYSCALLS 这里编译时报错，发现工具链不支持zicond扩展，应该是当时编译时没有开启，重新编译12345CC scripts/mod/empty.oHOSTCC scripts/mod/mk_elfconfigCC scripts/mod/devicetable-offsets.sAssembler messages:错误： rv64imac_zicond_zihintpause_zba_zbc_zbs: unknown prefixed ISA extension `zicond' 编译 1234# 假设你的安装目录还是 /opt/riscv./configure --prefix=/opt/riscv --with-arch=rv64gc_zba_zbb_zbc_zbs_zicond# 这个过程会比较长，请耐心等待make -j$(nproc) linux 重新编译工具链后内核可完成编译，替换镜像和设备树 12mv /home/share/Image /boot/vmlinuz-6.6.63mv /home/share/k1-x_evb.dtb /boot/spacemit/6.6.63/k1-x_evb.dtb 确保写入磁盘并重启 12sync reboot 配置libbpf并使用libbpf-bootstrap库中示例 clone 仓库123git clone https://github.com/libbpf/libbpf-bootstrap.git --depth=1git submodule update --init --recursivecd libbpf-bootstrap 编译12cd examples/cmake 刚开始打算在pc上交叉编译，但是尝试了好久都没成功编译出来，于是在开发板上直接本地编译了 1234- sudo apt update- sudo apt install -y build-essential clang llvm libelf-dev libz-dev git- cd libbpf-bootstrap/examples/c- make 编译成功 123456789101112131415161718192021222324252627#编译成功root@k1:~# ls repo/libbpf-bootstrap/examples/c/bootstrap kprobe lsm.c minimal_ns sockfilter.c tc.cbootstrap.bpf.c kprobe.bpf.c Makefile minimal_ns.bpf.c sockfilter.h uprobebootstrap.c kprobe.c minimal minimal_ns.c task_iter uprobe.bpf.cbootstrap.h ksyscall minimal.bpf.c profile.bpf.c task_iter.bpf.c uprobe.cCMakeLists.txt ksyscall.bpf.c minimal.c profile.c task_iter.c usdtfentry ksyscall.c minimal_legacy profile.h task_iter.h usdt.bpf.cfentry.bpf.c lsm minimal_legacy.bpf.c sockfilter tc usdt.cfentry.c lsm.bpf.c minimal_legacy.c sockfilter.bpf.c tc.bpf.c xmake.lua# 成功运行 root@k1:~# ./repo/libbpf-bootstrap/examples/c/bootstrap TIME EVENT COMM PID PPID FILENAME/EXIT CODE15:11:17 EXEC ls 692 682 /usr/bin/ls15:11:17 EXIT ls 692 682 [0] (3ms)15:11:23 EXEC cat 693 682 /usr/bin/cat15:11:26 EXIT cat 693 682 [0] (3732ms)15:11:33 EXEC touch 694 682 /usr/bin/touch15:11:33 EXIT touch 694 682 [0] (2ms)15:11:33 EXEC ls 695 682 /usr/bin/ls15:11:33 EXIT ls 695 682 [0] (3ms)15:11:36 EXEC rm 696 682 /usr/bin/rm15:11:36 EXIT rm 696 682 [0] (1ms)15:11:37 EXIT bash 682 681 [0]15:11:38 EXIT sshd-session 681 671 [255]15:11:38 EXIT sshd-session 671 669 [255]15:11:38 EXIT (sd-close) 697 1 [0] 第一个demo 早期无CO-RE模式，见此博客demo bpf_load.c + libelf + 手动链接 libbpf, 需要手动配置头文件，依赖内核版本源码，使用辅助脚本等 现代CO-RE模式 libbpf 库 + BPF Skeleton (.skel.h), libbpf 库和 bpftool 工具为你处理所有底层的繁琐工作（如ELF解析、map创建、程序加载、挂载）。 CO-RE (一次编译，到处运行)：通过 vmlinux.h 和 BTF，代码不强依赖特定内核版本，可移植性极高。 目的: 让一个 eBPF 程序只监控加载它自身的那个进程的 write 系统调用。 minimal_ns.bpf.c: 这是运行在内核中的 eBPF 代码，它会被附加到 write 系统调用的跟踪点上。 12345678910111213141516171819202122232425262728293031323334353637383940// SPDX-License-Identifier: GPL-2.0 OR BSD-3-Clause/* Copyright (c) 2023 Hosein Bakhtiari */#include &lt;linux/bpf.h&gt;#include &lt;bpf/bpf_helpers.h&gt; // BPF 辅助函数库#include &lt;linux/sched.h&gt; // 包含任务相关的结构定义char LICENSE[] SEC(&quot;license&quot;) = &quot;Dual BSD/GPL&quot;;// --- 全局变量 ---// 这些变量位于 .bss 段，意味着它们是未初始化的全局变量。// libbpf 允许用户态程序在加载 BPF 对象前，修改这些变量的值。int my_pid = 0;unsigned long long dev;unsigned long long ino;// --- BPF 程序逻辑 ---// SEC(&quot;tp/syscalls/sys_enter_write&quot;) 表示将此程序附加到// tracepoint 'syscalls:sys_enter_write'，即任何进程进入 write 系统调用时触发。SEC(&quot;tp/syscalls/sys_enter_write&quot;)int handle_tp(void *ctx){ // 定义一个结构体来接收 PID 命名空间信息 struct bpf_pidns_info ns; // --- 身份识别与过滤 --- // 调用 bpf_get_ns_current_pid_tgid 辅助函数。 // 它获取当前进程的 PID，但这个 PID 是相对于 dev 和 ino 所指定的 PID 命名空间而言的。 // dev 和 ino 是我们从用户态传递进来的，用于唯一标识一个命名空间。 bpf_get_ns_current_pid_tgid(dev, ino, &amp;ns, sizeof(ns)); // 过滤逻辑：如果当前触发 write 系统调用的进程的 PID // 不等于我们从用户态传递进来的 my_pid，就直接返回，什么也不做。 if (ns.pid != my_pid) return 0; // 如果 PID 匹配，就打印一条日志到跟踪管道。 bpf_printk(&quot;BPF triggered from PID %d.\\n&quot;, ns.pid); return 0;} 关键点 作用 全局变量 (my_pid, dev, ino) 这不是普通的全局变量。在 eBPF 中，定义在顶层的变量会被编译器放入 ELF 文件的特定段（如 .data, .bss, .rodata）。libbpf 在加载 BPF 程序时，能够识别这些变量，并允许用户态代码在 bpf_object__load() 之前对它们进行修改。这正是用户态向内核态传递配置的桥梁。 bpf_get_ns_current_pid_tgid() 这是一个非常重要的辅助函数。在复杂的容器环境中，一个进程的 PID 在容器内和在宿主机上是不同的。这个函数允许我们查询一个进程在特定PID命名空间中的 PID。它通过设备号（dev）和 inode 号（ino）来唯一确定一个命名空间。 过滤逻辑 这是整个 BPF 程序的核心。它确保了只有“目标”进程（即加载它自己的那个用户态程序）的 write 调用才能触发 bpf_printk。所有其他进程的 write 调用都会在 if 判断那里被提前过滤掉。 minimal_ns.c: 这是运行在用户空间的普通 C 程序，负责加载、设置、附加和销毁 eBPF 程序。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172// SPDX-License-Identifier: (LGPL-2.1 OR BSD-2-Clause)/* Copyright (c) 2023 Hosein Bakhtiari */#include &lt;stdio.h&gt;#include &lt;sys/stat.h&gt; // stat()#include &lt;unistd.h&gt; // getpid(), sleep()#include &lt;bpf/libbpf.h&gt; // libbpf 核心库#include &quot;minimal_ns.skel.h&quot; // 自动生成的 BPF 骨架文件// libbpf 的日志打印回调函数，方便调试static int libbpf_print_fn(enum libbpf_print_level level, const char *format, va_list args){ return vfprintf(stderr, format, args);}int main(int argc, char **argv){ struct minimal_ns_bpf *skel; // BPF 骨架对象指针 int err; struct stat sb; // 用于接收 stat() 系统调用的结果 libbpf_set_print(libbpf_print_fn); // 设置 libbpf 的日志回调 // --- BPF 程序生命周期：打开 --- skel = minimal_ns_bpf__open(); if (!skel) { fprintf(stderr, &quot;Failed to open BPF skeleton\\n&quot;); return 1; } // --- 关键部分 4: 获取自身身份信息并传递给 BPF 程序 --- // /proc/self/ns/pid 是一个特殊的文件，它的 dev 和 ino 号唯一标识了当前进程所在的 PID 命名空间。 if (stat(&quot;/proc/self/ns/pid&quot;, &amp;sb) == -1) { fprintf(stderr, &quot;Failed to acquire namespace information&quot;); return 1; } // 通过 skeleton 对象，访问 BPF 程序中的 .bss 全局变量并赋值。 skel-&gt;bss-&gt;dev = sb.st_dev; // 传递命名空间设备号 skel-&gt;bss-&gt;ino = sb.st_ino; // 传递命名空间 inode 号 skel-&gt;bss-&gt;my_pid = getpid(); // 传递当前进程的 PID // --- BPF 程序生命周期：加载 --- // 此刻，携带了正确配置（PID和命名空间ID）的 BPF 程序被加载到内核中。 err = minimal_ns_bpf__load(skel); if (err) { fprintf(stderr, &quot;Failed to load and verify BPF skeleton\\n&quot;); goto cleanup; } // --- BPF 程序生命周期：附加 --- // 将加载到内核的 BPF 程序附加到它指定的 tracepoint 上。 err = minimal_ns_bpf__attach(skel); if (err) { fprintf(stderr, &quot;Failed to attach BPF skeleton\\n&quot;); goto cleanup; } printf(&quot;Successfully started! Please run `sudo cat /sys/kernel/debug/tracing/trace_pipe` to see output...\\n&quot;); // --- 关键部分 5: 触发 BPF 程序 --- for (;;) { // 通过向 stderr 打印一个字符，来调用 write() 系统调用。 // 这个动作会触发我们刚刚附加的 BPF 程序。 fprintf(stderr, &quot;.&quot;); sleep(1); }cleanup: // --- BPF 程序生命周期：销毁 --- // 程序退出时，清理并卸载 BPF 程序。 minimal_ns_bpf__destroy(skel); return -err;} 关键点 作用 minimal_ns.skel.h 这是 libbpf-bootstrap 的魔法核心。构建系统会使用 bpftool 工具根据你的 .bpf.c 文件自动生成这个头文件。它包含了 struct minimal_ns_bpf 的定义，以及 __open, __load, __attach, __destroy 等生命周期管理函数。 skel-&gt;bss-&gt;… 这就是访问 BPF 全局变量的方式。skel 指向整个 BPF 对象的句柄，skel-&gt;bss 是一个指向 BPF 程序 .bss 段变量的结构体。你可以像访问普通结构体成员一样读写它们。 stat(“/proc/self/ns/pid”, &amp;sb): 这是获取当前进程 PID 命名空间标识符的标准方法。sb.st_dev 和 sb.st_ino 的组合在整个系统中是唯一的。 fprintf(stderr, “.”) 这行代码不仅仅是为了在终端上显示进度。它的本质是调用 write 系统调用。因为 BPF 程序监控的就是 write，并且只对自己这个 PID 感兴趣，所以这个调用就是触发 BPF 程序执行的“扳机”。 执行流程： 用户态程序启动。 它通过 stat 系统调用获取自己所在的 PID 命名空间标识（dev 和 ino），并通过 getpid() 获取自己的 PID。 它通过 BPF skeleton (skel-&gt;bss-&gt;…) 将这三个值写入 BPF 程序的全局变量中。 它调用 __load() 将配置好的 BPF 程序加载进内核。 它调用 __attach() 将 BPF 程序挂载到 write 系统调用的入口。 用户态程序进入无限循环，每秒调用一次 fprintf，这会触发 write 系统调用。 内核中的 BPF 程序被触发，它检查发现是目标进程，于是打印一条日志。 我们在另一个终端通过 cat /sys/kernel/debug/tracing/trace_pipe 就能看到这条日志。 运行结果: 相关仓库 libbpf-bootstrap (仓库) 定位: 一个由 libbpf 官方维护的最佳实践模板和入门脚手架。 价值: 它展示了如何正确地构建一个现代、基于 libbpf + CO-RE 的 eBPF 程序。它的 Makefile 和示例代码封装了所有复杂的构建细节： 如何调用 Clang 将 .bpf.c 编译成 .bpf.o。 如何使用 bpftool 基于 .o 文件生成骨架头文件 (.skel.h)。 如何将用户态的 C 程序与 libbpf 库链接起来。 对于初学者来说，克隆这个仓库是学习 libbpf 开发最直接、最标准的方式。 awesome-ebpf (仓库) 定位: 一个精心策划的 eBPF 资源聚合清单。 价值: 如果你想了解 eBPF 生态的全貌，这里是你的起点。它系统地收集和分类了互联网上几乎所有与 eBPF 相关的优秀资源，包括但不限于： 入门教程和文档 知名开源项目 (如 Cilium, Falco) 实用的开发工具 深度技术博客和文章 会议演讲和视频","link":"/post/ebpf-and-use-libbpf.html"},{"title":"利用 hexo 搭建博客","text":"使用hexo和GitHub Pagtes部署一个自己的博客 1. 安装并初始化Hexo 安装 Hexo CLI 1npm install -g hexo-cli 初始化博客项目目录 123mkdir my-blog &amp;&amp; cd my-bloghexo initnpm install 本地预览 1hexo server 启动本地服务：在浏览器访问 http://localhost:4000 查看效果 2. 配置 GitHub Pages 部署 创建GitHub仓库 创建一个仓库，名字叫 你的GitHub用户名.github.io 比如你是 goko，就叫 goko.github.io 安装部署插件 1npm install hexo-deployer-git --save 修改 _config.yml（根目录下）添加部署配置： 12345deploy: type: git # repo建议使用SSH, SSH免密 repo: https://github.com/你的GitHub用户名/你的GitHub用户名.github.io.git branch: main # 或者 master，看你的默认分支 生成并部署博客 123hexo cleanhexo generatehexo deploy 3. 域名(.com)绑定 添加域名(在my-blog下) 123echo &quot;&lt;xxxx&gt;.com&quot; &gt; source/CNAME# 或者可以：echo &quot;www.&lt;xxxx&gt;.com&quot; &gt; source/CNAME# 只能添加一个，而且两个需要添加不同的域名解析（如下） 重新部署 1hexo clean &amp;&amp; hexo g &amp;&amp; hexo d 设置 DNS 解析指向 GitHub Pages A. 使用裸域名（apex 域名）goku72.com 记录类型 主机记录 记录值 说明 A @ 185.199.108.153 GitHub Pages IP A @ 185.199.109.153 GitHub Pages IP A @ 185.199.110.153 GitHub Pages IP A @ 185.199.111.153 GitHub Pages IP example aliyun: 选择业务需求: 将网站域名解析到服务器IPv4地址 选择网站域名(主机记录): .com（对应设置“@”主机记录） 填写 IP（记录值）： 在输入框里粘贴以下四行（每一行一个 IP）： &gt; 185.199.109.153 &gt; 185.199.108.153 &gt; 185.199.110.153 &gt; 185.199.111.153 B. 使用 www.goku72.com 作为主域名 记录类型 主机记录 记录值 说明 CNAME www &lt;github用户名&gt;.github.io. 指向你的 GitHub 用户页仓库 example aliyun: 选择业务需求: 将网站域名解析到另外的目标域名 选择网站域名(主机记录): www..com（对应设置“www”主机记录） 填写 IP（记录值）：&lt;github用户名&gt;.github.io. (最有有一个符号”.”) 4. 设置主题 cd my-blog/themes git clone -b master https://github.com/jerryc127/hexo-theme-butterfly.git butterfly 修改_config.yml: theme: butterfly hexo clean &amp;&amp; hexo g &amp;&amp; hexo d 更多主题：https://hexo.io/themes/ 注： 如果AB两个方式都添加了，只需要在 Hexo 项目的 source/CNAME 文件中写 www..com，GitHub Pages 就会自动把 goku72.com 重定向过去，无需额外设置！ 后续换域名只需要：阿里云重新解析 + 修改 source/CNAME + 重新部署 Hexo，就能完成域名迁移。 有些主题可能需要下载插件","link":"/post/hexo-blog.html"},{"title":"libbpf, bcc和bpftrace的结构和关联分析","text":"libbpf bcc 和 bpftrace之间的结构以及和内核的关联 关系结构图123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960+-----------------------------------------------------------------------------+| 用户空间 (User Space) ||-----------------------------------------------------------------------------|| [高层抽象/工具] || || +-----------------+ || | bpftrace(C++) | || | (诊断与排障语言)| || +-------+---------+ || | || 内部解析器解析用户脚本，翻译成C代码 || 将这个C代码实时编译成 eBPF 字节码(Clang/LLVM) || 使用 libbpf 的C API 来加载和管理eBPF程序 || | || | +----------------+ || | |BCC(内部实现C++)| python/Lua/c++高级语言封装|| | | (快速原型框架) | header file || |Clang/LLVM +-------------+--+ || | | || | 运行时把c代码编译成eBPF字段，API加载交互 || | 还是写C,只是编译和加载被Python框架封装了 || | | Clang/LLVM || +---------------------------+ ---------------------- || | libbpf(C) | | || | (是纯C开发库, 提供CO-RE) |------------------------------------ || | 封装bpf()和Maps API) | | || +-------------+-------------+ | || | || BPF Maps (e.g., RingBuf, Hash) | System Call || &lt;------------------------------------------------------&gt; (bpf()) || (内核与用户空间的数据通道) | (控制与加载) || | |+------------------------------------------------------------+----------------+| 内核空间 (Kernel Space) | ||-----------------------------------------------------------------------------|| | || BPF Maps (内核中的键值对存储) &lt;-----------------------------+ || | | || +---------------------------------------------------+&lt;---| | || | eBPF 子系统 | | || | | | || | +-----------+ +----------+ +-----------+ | | || | | Verifier | --&gt; | JIT | --&gt; | CPU | | | || | | (安全检查)| | (编译优化) | | (原生执行)| | | || | +-----------+ +----------+ +-----------+ | | || | | | || | ebpf 程序 (你的.bpf.c代码) | | || | | | || +-----------------------+---------------------------+ | || ^ | || | (事件触发) (读取/写入)| || | v || +-----------------------+-------------------------------------------+ || | 内核钩子 (kernel hooks) | || | | || | ftrace (kprobes, tracepoints), LSM, TC, XDP, Sockets ... | || | (BTF - 提供内核元数据, 增强钩子能力) | || +-------------------------------------------------------------------+ || |+-----------------------------------------------------------------------------+ BCC (BPF Compiler Collection) 构成: BCC 是一个强大的 eBPF 开发工具包和框架。其核心是一个 C++ 库，并提供了 Python、C++、Go 等多种语言的前端封装，其中 Python 前端最为流行和成熟。 核心机制: 运行时编译: BCC 的标志性特点是在程序运行时动态编译 eBPF C 代码。开发者在 Python 等脚本中嵌入 C 代码字符串，BCC 框架在后台调用 libclang (Clang 的库版本) 将其编译成 eBPF 字节码。 自有的加载器: BCC 拥有一套自研的加载器逻辑，负责处理字节码的加载、Map 创建以及与内核的交互。 与 libbpf 的关系: BCC 诞生早于 libbpf 的成熟期，因此其核心不依赖 libbpf。 然而，为了拥抱社区标准和利用 CO-RE 等现代特性，新版本的 BCC 已经开始逐步集成 libbpf，并提供基于 libbpf 的新工具和 API。 libbpf 构成: libbpf 是一个由内核社区维护的、用于开发 eBPF 应用的纯 C 语言核心库。它被认为是构建现代、高性能、可移植 eBPF 程序的事实标准。 核心机制: 预编译 (Ahead-of-Time): libbpf 的典型工作流是在开发阶段就将 eBPF C 代码（.bpf.c）编译成包含字节码的 ELF 对象文件（.bpf.o）。 智能加载: 用户态程序通过调用 libbpf 的 API，可以智能地解析 .o 文件，并将 eBPF 程序和 Maps 加载到内核。 CO-RE (一次编译，到处运行): 这是 libbpf 的王牌特性。它利用 BTF (BPF Type Format) 元数据，在加载时动态调整 eBPF 程序，以解决因内核版本不同导致的数据结构差异问题，极大地增强了程序的可移植性。 bpftrace 定位与构成: bpftrace 是一款专为 Linux 设计的高级动态追踪语言和命令行工具。它的语法简洁强大，类似 awk 和 DTrace，让使用者能用极少的代码快速排查系统性能问题。 核心机制: 高级语言到 C 的翻译: bpftrace 的核心是一个C++ 程序，它负责将用户编写的高级脚本实时翻译成 eBPF C 代码。 后端依赖: 它不直接与内核交互，而是依赖一个后端引擎来完成编译和加载。 与 libbpf/BCC 的关系: 历史与现在: 早期 bpftrace 依赖 BCC 作为其后端。为了追求更好的性能、更轻的依赖和 CO-RE 支持，现代版本的 bpftrace 已经默认切换到使用 libbpf 作为其核心后端。 对比表格：libbpf vs BCC vs bpftrace 特性 libbpf BCC bpftrace 定位 核心库 (Core Library) 开发框架 (Framework) 高级工具/语言 (High-level Tool) 主要用途 生产级应用、Agent、底层开发 快速原型、教学、脚本化开发 实时排障、命令行即时查询 编程接口 C/C++ API Python/C++ API 专用脚本语言 (类 awk) 编译时机 预编译 (开发时) 运行时 运行时 CO-RE 可移植性 原生支持 (核心优势) 支持有限/较弱 通过 libbpf 后端获得支持 运行时依赖 极轻量 重量级 中量级 … 需要 libbpf.so? 是 否 (但正在集成) 是 (现代版本) … 需要 Clang/LLVM? 否 是 是 … 需要内核头文件? 否 是 (传统方式) 否 … 需要 Python? 否 是 否 部署产物 单个二进制文件 Python脚本 + 运行时环境 bpftrace 工具 + 运行时环境 性能 最高 (启动快，无运行时编译开销) 中等 (有运行时编译开销) 中高 (比BCC快，但仍有运行时开销) 灵活性 最高 (完全控制) 高 (动态修改 C 代码方便) 中等 (受限于语言特性) 易用性 低 (最复杂，需手写 C) 中等 (Python 封装，较友好) 最高 (最简单，一行命令) 最适合场景 需要嵌入到其他程序中的长期监控Agent，如Cilium、Datadog Agent。 编写一次性的调试脚本，探索内核行为，快速验证想法。 系统管理员在服务器上快速定位一个具体问题，如”哪个进程在大量读写磁盘？”","link":"/post/libbpf-bcc-and-bpftrace.html"},{"title":"linux内核驱动编写时的一些操作","text":"linux内核驱动以及驱动之间的一些操作 模块编写 驱动模块传参 module_param(name, type, perm) 宏来定义一个参数。 name: 参数变量名。 type: 参数类型 (int, charp (字符串指针), bool 等)。 perm: 在 sysfs 中对应的文件权限 (/sys/module//parameters/)。0644 表示 root 可写，所有人可读。 内核模块符号的导出 EXPORT_SYMBOL(symbol_name): 导出符号，任何模块都可以使用。 EXPORT_SYMBOL_GPL(symbol_name): 只为 MODULE_LICENSE(“GPL”) 的模块导出，有许可证限制。 自动创建设备节点 创建设备类 (class)my_class = class_create(THIS_MODULE, “my_class_name”);if (IS_ERR(my_class)) { /* handle error */ } // 在此类下创建设备文件// 这会触发 uevent，让 udev/mdev 工作device_create(my_class, NULL, dev_num, NULL, “my_device_name”);","link":"/post/linux-kernel-drivers.html"},{"title":"linux内核中相较于传统锁的qspinlock锁的优缺点分析","text":"qspinlock 是一种为现代多核系统设计的先进混合自旋锁。它巧妙地融合了两种经典锁的优点：既继承了票据锁（ticket lock）的公平性，又借鉴了 MCS 锁优异的可扩展性。 1. 传统spinlock： 多个等待的 CPU 核心中，谁先获得锁并无保证，存在公平性问题，同时缓存一致性开销大（如MESI），CPU核心越大，cache需求越厉害，缺乏可扩展性 2. Ticket spinlock1234567891011121314151617#define TICKET_NEXT 16typedef struct { union { u32 lock; struct __raw_tickets { /* little endian */ u16 owner; u16 next; } tickets; };} arch_spinlock_t;my_ticket = atomic_fetch_inc(&amp;lock-&gt;tickets.next);while (lock-&gt;tickets.owner != my_ticket) cpu_relax(); 解决了公平问题，防止某些 CPU 永远得不到锁，但所有核都轮询同一个owner变量，read cache line成热点，限制扩展性 3. MCS lock 本质上是一种基于链表结构的自旋锁，每个CPU有一个对应的节点(锁的副本)，基于各自不同的副本变量进行等待，锁本身是共享的，但队列节点是线程自己维护的，每个CPU只需要查询自己对应的本地cache line，仅在这个变量发生变化的时候，才需要读取内存和刷新这条cache line, 不像 classic/ticket对共享变量进行spin 123456789101112131415161718192021222324struct mcs_spinlock { struct mcs_spinlock *next; int locked; /* 1 if lock acquired */ int count; /* nesting count, see qspinlock.c */};static inlinevoid mcs_spin_lock(struct mcs_spinlock **lock, struct mcs_spinlock *node){ struct mcs_spinlock *prev; /* Init node */ node-&gt;locked = 0; node-&gt;next = NULL; prev = xchg(lock, node); if (likely(prev == NULL)) { return; } WRITE_ONCE(prev-&gt;next, node); /* Wait until the lock holder passes the lock down. */ arch_mcs_spin_lock_contended(&amp;node-&gt;locked);} 每个 CPU 线程创建的node 是独立的，每个线程都有自己的 node 实例。但是结构体中多了一个指针使结构体变大了，导致了“内存开销问题”：MCS 锁把竞争带来的 cache-line 抖动降低了，但牺牲了一些内存和部分结构管理的成本。 4. qspinlockinclude/asm-generic/qspinlock_types.h: 锁数据结构 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758typedef struct qspinlock { union { atomic_t val; /* * By using the whole 2nd least significant byte for the * pending bit, we can allow better optimization of the lock * acquisition for the pending bit holder. */#ifdef __LITTLE_ENDIAN struct { u8 locked; u8 pending; }; struct { u16 locked_pending; u16 tail; };#else struct { u16 tail; u16 locked_pending; }; struct { u8 reserved[2]; u8 pending; u8 locked; };#endif };} arch_spinlock_t;/* * Initializier */#define __ARCH_SPIN_LOCK_UNLOCKED { { .val = ATOMIC_INIT(0) } }/* * Bitfields in the atomic value: * * When NR_CPUS &lt; 16K * 0- 7: locked byte * 8: pending * 9-15: not used * 16-17: tail index * 18-31: tail cpu (+1) * * When NR_CPUS &gt; = 16K * 0- 7: locked byte * 8: pending * 9-10: tail index * 11-31: tail cpu (+1) */#define _Q_SET_MASK(type) (((1U &lt;&lt; _Q_ ## type ## _BITS) - 1)\\&lt;&lt; _Q_ ## type ## _OFFSET)#define _Q_LOCKED_OFFSET 0#define _Q_LOCKED_BITS 8#define _Q_LOCKED_MASK _Q_SET_MASK(LOCKED) When NR_CPUS &lt; 16K： locked：用来表示这个锁是否被人持有（0：无，1：有） pending：可以理解为最优先持锁位，即当unlock之后只有这个位的CPU最先持锁，也有1和0 tail：有idx+CPU构成，用来标识等待队列的最后一个节点。 tail_idx：就是index，它作为mcs_nodes数组的下标使用 tail_CPU：用来表示CPU的编号+1，+1因为规定tail为0的时候表示等待队列中没有成员 kernel/locking/mcs_spinlock.h 12345struct mcs_spinlock { struct mcs_spinlock *next; int locked; /* 1 if lock acquired */ int count; /* nesting count, see qspinlock.c */}; locked = 1:只是说锁传到了当前加节点，但是当前节点还需要主动申请锁(qspinlock -&gt; locked = 1)count：针对四种上下文用于追踪当前用了第几个 node（即 idx），最大为4,不够用时就fallback不排队直接自旋 kernel/locking/qspinlock.c: 123456789101112131415161718#define MAX_NODES 4struct qnode { struct mcs_spinlock mcs;#ifdef CONFIG_PARAVIRT_SPINLOCKS long reserved[2];#endif};/* * Per-CPU queue node structures; we can never have more than 4 nested * contexts: task, softirq, hardirq, nmi. * * Exactly fits one 64-byte cacheline on a 64-bit architecture. * * PV doubles the storage and uses the second cacheline for PV state. */static DEFINE_PER_CPU_ALIGNED(struct qnode, qnodes[MAX_NODES]); 一个 CPU 上可能嵌套多个锁, qnodes针对四种上下文情况下，例：进程上下文中发生中断后再次获取锁 PER_CPU的优点是快，可防止抢锁时再mallock或临时分配导致延迟，成本等问题 申请锁： 快速申请include/asm-generic/qspinlock.h 12345678910111213/** * queued_spin_lock - acquire a queued spinlock * @lock: Pointer to queued spinlock structure */static __always_inline void queued_spin_lock(struct qspinlock *lock){ int val = 0; if (likely(atomic_try_cmpxchg_acquire(&amp;lock-&gt;val, &amp;val, _Q_LOCKED_VAL))) return; queued_spin_lock_slowpath(lock, val);} 中速申请 快速申请失败，queue中为空时，设置锁的pending位 再次检测（检查中间是否有其它cpu进入） 一直循环检测locked位 当locked位为0时，清除pending位获得锁 慢速申请 申请 操作 快速申请 这个锁当前没有人持有，直接通过cmpxchg()设置locked域即可获取了锁 中速申请 锁已经被人持有，但是MCS链表没有其他人，有且仅有一个人在等待这个锁。设置pending域，表示是第一顺位继承者，自旋等待lock-&gt; locked清0，即锁持有者释放锁 慢速申请 进入到queue中自旋等待，若为队列头（队列中没有等待的cpu），说明它已排到最前，可以开始尝试获取锁；否则，它会自旋等待前一个节点释放锁，并通知它可以尝试获取锁了 end: 如果只有1个或2个CPU试图获取锁，那么只需要一个4字节的qspinlock就可以了，其所占内存的大小和ticket spinlock一样。当有3个以上的CPU试图获取锁，则需要(N-2)个MCS node qspinlock中加入”pending”位域的意义，如果是两个CPU试图获取锁，那么第二个CPU只需要简单地设置”pending”为1，而不用创建一个MCS node 试图加锁的CPU数目超过3个，使用ticket spinlock机制就会造成多个CPU的cache line刷新的问题，而qspinlock可以利用MCS node队列来解决这个问题 在多核争用严重场景下，qspinlock 让等待者在本地内存区域自旋，减少了锁的缓存抖动和对总线的竞争消耗 RISCV_QUEUED_SPINLOCKS 只应在平台(RISC-V)具有 Zabha 或 Ziccrse 时启用，不支持的情况不要选用 优先级反转问题，queue会保证了FIFO提高了公平性，但它无法感知任务的优先级，可能因为排在队列前方的低优先级任务未释放锁而发生等待，从而导致 优先级反转","link":"/post/qspinlock.html"},{"title":"riscv 工具链的了解和使用","text":"在 RISC-V 开发中, 交叉编译工具链允许我们在一个平台（如 x86 主机）上，为另一个平台（如RISC-V 开发板）生成可执行代码。 1. 核心概念：工具链的“三元组” (Triplet)你经常会看到像 riscv64-unknown-linux-gnu- 这样的名称，这就是工具链的“三元组”，其标准格式为： 1&lt;arch&gt;-&lt;vendor&gt;-&lt;os&gt; &lt;arch&gt; (架构)：指定目标 CPU 架构，例如 riscv64 或 riscv32。 &lt;vendor&gt; (供应商)：通常是 unknown 或公司名。 &lt;os&gt; (操作系统/环境)：这是最关键的部分，它决定了工具链的目标环境和使用的 C 标准库 (libc)。最常见的两个是： elf: 面向裸机 (Bare-metal) 或嵌入式实时操作系统 (RTOS)。 linux-gnu: 面向完整的 GNU/Linux 操作系统。 2. 两大主流工具链详解1. riscv64-unknown-elf用于裸机和嵌入式开发的标准工具链。 目标系统: 没有任何操作系统的环境（裸机），或者使用了轻量级实时操作系统（如 FreeRTOS, RT-Thread）的环境。 C 标准库 (Libc): 使用 Newlib。 Newlib 是一个轻量级的 C 库，专为嵌入式系统设计。它只提供最基础的 C 语言函数（如 strcpy, printf），并且不依赖任何操作系统的系统调用（Syscall）。如果需要文件操作或内存管理，需要实现底层的“桩函数”(stubs)。 应用场景: 编写 Bootloader（如 U-Boot）。 开发 RISC-V 的“特权二进制接口”固件（如 OpenSBI）。 为微控制器 (MCU) 编写固件。 开发简单的操作系统内核。 2. riscv64-linux-gnu用于在 RISC-V 平台上开发 Linux 应用的工具链。 目标系统: 运行完整 Linux 内核的系统。 C 标准库 (Libc): 使用 glibc (GNU C Library)。 glibc 是功能完备的标准 C 库，提供了丰富的 POSIX API 支持（如 fork, pthread, 文件系统操作等）。它深度依赖 Linux 内核提供的系统调用来完成工作。 典型应用场景: 编译一个标准的 C/C++ 应用程序（如 Nginx, Redis），让它运行在 RISC-V 架构的 Linux 发行版上（如 Ubuntu, Debian for RISC-V）。 开发 Linux 用户态驱动或服务程序。 Tip: riscv64-unknown-linux-gnu- 和 riscv64-linux-gnu- 在功能上是等价的，可以互换使用。unknown 字段在这里没有实际影响。 3. 如何获取和安装工具链方式一：使用包管理器 (简单快捷)对于 linux-gnu 工具链，这是最简单的方法。以 Ubuntu/Debian 为例： 123# 安装 C 和 C++ 交叉编译器sudo apt updatesudo apt install gcc-riscv64-linux-gnu g++-riscv64-linux-gnu 优点: 安装简单 缺点: 版本可能不是最新的 方式二：从源码编译 (推荐，灵活且最新)获取最新版本工具链（包括 elf 和 linux-gnu）的最佳方式。 安装相关依赖 12sudo apt install libncurses-dev libncursesw5-dev pkg-config autoconf automake bison flex gawk gcc g++ libtool make patch python3-dev texinfo wgetsudo apt-get install autoconf automake autotools-dev curl libmpc-dev libmpfr-dev libgmp-dev gawk build-essential bison flex texinfo gperf libtool patchutils bc zlib1g-dev libexpat-dev make bison flex texinfo gawk libncurses5-dev libexpat1-dev libgmp-dev libmpfr-dev libmpc-dev libgmp-dev libmpfr-dev libmpc-dev 克隆官方仓库 123#`--recursive` 参数至关重要，它会同时下载 `gcc`, `binutils` 等所有子模块。git clone --recursive https://github.com/riscv-collab/riscv-gnu-toolchaincd riscv-gnu-toolchain 1234567* 检查当前子模块情况。git submodule status* 拉取子模块(init: 子模块未初始化时初始化，recursive: 嵌套子模块也一起拉取)* 主仓库换分支时同步子模块git submodule update --init --recursive 配置与编译需要指定安装路径 (--prefix) 和目标架构 (--with-arch, --with-abi)。 编译 linux-gnu 工具链 (用于Linux): 12345678# 创建安装目录mkdir -p /opt/riscv-linux# 配置: 目录，目标是为linux构建工具链./configure --prefix=/opt/riscv-linux --enable-linux# `make linux` 会自动处理多阶段编译的复杂流程（构建临时gcc-&gt;构建glibc-&gt;构建最终gcc）time make -j$(nproc) linux# 安装sudo make install 编译 elf 工具链 (用于裸机):riscv64-unknown-elf- 123456789# 创建一个安装目录mkdir -p /opt/riscv-elf# 配置: 其中 `rv64gc` 指支持 64 位基础整数指令集（I）、乘除法（M）、原子（A）、浮点（F、D）、压缩（C）等扩展；# `lp64d` 表示 long 和 pointer 为 64 位，使用 double 精度浮点。./configure --prefix=/opt/riscv-elf --with-arch=rv64gc --with-abi=lp64d# 编译 (-j`nproc` 使用所有CPU核心加速)time make -j$(nproc)# 安装sudo make install 添加到环境变量为了方便使用，将工具链的 bin 目录添加到 PATH。编辑 ~/.bashrc 或 ~/.zshrc 文件： 123456# 添加这行到文件末尾 (根据编译的类型选择)export PATH=&quot;/opt/riscv-elf/bin:$PATH&quot; # For elf toolchainexport PATH=&quot;/opt/riscv-linux/bin:$PATH&quot; # For linux toolchain# 使配置生效source ~/.bashrc 4. 简单使用1234567// hello.c#include &lt;stdio.h&gt;int main() { printf(&quot;Hello, RISC-V World!\\n&quot;); return 0;} 使用 elf 工具链编译1234567# 编译riscv64-unknown-elf-gcc -o hello.elf hello.c# 查看文件类型file hello.elf# 输出会类似:# hello.elf: ELF 64-bit LSB executable, UCB RISC-V, version 1 (SYSV), statically linked, not stripped 这个 hello.elf 是一个静态链接的裸机程序。它不能直接在 x86 Linux 主机上运行，也不能在 RISC-V Linux 系统上直接运行，因为它缺少操作系统加载器所需的信息。它需要被烧录到裸机环境或通过模拟器（如 QEMU-system）加载执行。 这个 hello.elf 文件虽然是标准的 ELF 格式，但它与 Linux 可执行文件有本质区别： 不含 INTERP 段：它不指定动态链接器，因为它不依赖任何动态库。 静态链接: 它静态链接了轻量级的 newlib C 库，而非 glibc。 无系统调用: 其中的 printf 函数最终依赖开发者实现的底层 I/O 桩函数（如通过 UART 发送字符），而不是 Linux 的 write 系统调用。 不同的程序入口: 它的启动代码 (_start) 负责初始化 C 运行环境后调用 main，但 main 返回后程序通常会进入死循环，因为它没有“退出”到操作系统的概念。 使用 linux-gnu 工具链编译1234567# 编译riscv64-linux-gnu-gcc -o hello.linux hello.c# 查看文件类型file hello.linux# 输出会类似:# hello.linux: ELF 64-bit LSB executable, UCB RISC-V, version 1 (SYSV), dynamically linked, interpreter /lib/ld-linux-riscv64-lp64d.so.1, for GNU/Linux 4.15.0, not stripped 这个 hello.linux 是一个动态链接的 Linux 程序。它需要一个 RISC-V Linux 环境来运行，因为它依赖于该环境中的动态链接器 (ld-linux-riscv64-lp64d.so.1) 和 glibc 库。 总结： 特性 riscv64-unknown-elf riscv64-linux-gnu 目标平台 裸机 (Bare-metal)、RTOS GNU/Linux 系统 C 库 newlib (轻量级，无 OS 依赖) glibc (功能完备，依赖 Linux 内核) 核心用途 固件、Bootloader、RTOS 应用、简单操作系统内核 编译可在 RISC-V Linux 上运行的应用程序 选择场景 “为一块开发板从零开始写程序。” “在启动的 Linux 上面运行软件。”","link":"/post/riscv-toolchains.html"},{"title":"bpftrace环境的配置和使用(基于riscv-k1)","text":"在k1上配置并使用bpftrace,基于上一篇的配置 bpftrace配置过程大部分就是依赖的安装和编译问题，不再详细说明 安装依赖 1234567891011sudo apt-get update &amp;&amp; sudo apt-get install -y \\cmake flex bison \\python3-dev python3-setuptools \\libclang-20-dev libedit-dev libcurl4-openssl-dev libdebuginfod-dev liblzma-dev \\zip unzip \\llvm-20-tools llvm-20-dev libpolly-20-dev \\libbpf-dev libcereal-dev \\binutils-dev libdw-dev libpcap-dev \\curl \\dwarves \\libgtest-dev 配置编译和安装BCC（因为全部编译bpftrace需要依赖BCC的文件） 12345678910111213cd ~/repogit clone https://github.com/iovisor/bcc.gitcd bccgit submodule update --init --recursive# 创建一个干净的编译目录rm -rf build &amp;&amp; mkdir build &amp;&amp; cd build# 运行 cmake，并禁用示例以防止内存耗尽cmake -DCMAKE_INSTALL_PREFIX=/usr -DENABLE_EXAMPLES=OFF ..# 以较低的并行度进行编译，防止系统卡死make -j4 # 安装sudo make installsudo ldconfig 配置编译和安装bpftrace GTest需要手动编译123456cd /usr/src/googletestsudo cmake .sudo make -j$(nproc)sudo cp lib/*.a /usr/lib/sudo cp -r googletest/include/gtest /usr/include/sudo cp -r googlemock/include/gmock /usr/include/ 编译并安装bpftool123cd bpftool/src # 从 libbpf/src 目录回到 bpftool/srcmakesudo make install 编译并配置 blazesym1234567891011121314# 安装 Rust 环境(网速慢可用国内镜像)export RUSTUP_DIST_SERVER=https://mirrors.tuna.tsinghua.edu.cn/rustupexport RUSTUP_UPDATE_ROOT=https://mirrors.tuna.tsinghua.edu.cn/rustup/rustupwget https://sh.rustup.rs -O rustup-init.shchmod +x rustup-init.sh sudo ./rustup-init.sh -yrustc --versioncargo --versioncd ~/repo/libbpf-bootstrap/blazesym/capicargo build --release sudo cp target/release/libblazesym_c.a /usr/local/lib/sudo cp capi/include/blazesym.h /usr/local/include/blazesym.h sudo ldconfig clong bpftrace并编译安装123456789cd ~/repogit clone https://github.com/iovisor/bpftrace.gitcd bpftracegit submodule update --init --recursiverm -rf build &amp;&amp; mkdir build &amp;&amp; cd buildcmake ..make -j$(nproc)sudo make installbpftrace --version 安装完成！ 命令测试12# 输出sudo bpftrace -e 'BEGIN { printf(&quot;Hello, World! I have conquered eBPF on RISC-V!\\n&quot;); }' 12# 追踪所有打开文件的操作sudo bpftrace -e 'tracepoint:syscalls:sys_enter_openat { printf(&quot;PID %d opening: %s\\n&quot;, pid, str(args-&gt;filename)); }'","link":"/post/use-bpftrace-on-k1.html"},{"title":"三种常见的 linux 设备的驱动介绍及框架","text":"按照读写存储数据方式，我们可以把设备分为以下几种：字符设备、块设备和网络设备。而Linux三大驱动就是指对这些设备的驱动，即字符设备、块设备驱动和网络设备驱动。 1. 字符设备 (Character Devices) 字符设备是一种按字节流（character stream）进行访问的设备，不可寻址，没有缓冲。你请求 5 个字节，它就给你 5 个字节（如果设备里有的话）。它不支持随机访问，数据只能顺序读写。 原理 核心： file_operations 结构体。里面定义了当用户空间程序对设备文件调用 open(), read(), write(), ioctl() 等系统调用时，内核应该执行的对应驱动函数。 VFS (虚拟文件系统)： 当 open(&quot;/dev/mydevice&quot;, ...) 时，VFS 会根据路径找到对应的 inode（索引节点），inode 中包含了设备号（主设备号和次设备号）。 驱动注册： 驱动在加载时，会通过 register_chrdev() 或 alloc_chrdev_region() + cdev_add() 来告诉内核能处理主设备号为 X 的设备，操作函数菜单是file_operations 结构体。 连接： VFS 通过主设备号找到驱动和 file_operations，然后调用实现的 my_open(), my_read() 等函数，从而将用户空间的操作连接到了驱动代码上。 典型例子： 串口 (/dev/ttyS*)、控制台 (/dev/console)、鼠标 (/dev/input/mouse0)、键盘 (/dev/input/event*)。 I2C/SPI 设备： 虽然挂在特定总线上，但最终给用户提供的接口往往是字符设备，如一个 I2C 接口的温湿度传感器，可能会表现为 /dev/i2c-1 或通过 sysfs 访问。 裸设备驱动： 各种自定义的、简单的控制类设备 代码示例 CLICK 2. 块设备 (Block Devices) 块设备是按“块”（Block）为单位进行数据访问的设备，块是固定大小的（如 512 字节、4KB）。与字符设备最大的不同是： 支持随机寻址它可以随机访问（直接读写第 N 个块），并且 有内核I/O缓冲区。 原理 核心： block_device_operations 结构体和 请求队列 (Request Queue)。 I/O 调度器： 当用户程序请求读写数据时，请求不会立即发送给硬件。而是被分解成一个个对“块”的操作请求（struct request），放入一个请求队列中。内核的 I/O 调度器 会对队列里的请求进行合并、排序，以提高磁盘寻道效率（比如把对相邻块的请求放在一起处理）。 缓冲/缓存 (Buffer Cache)： 内核会把频繁访问的块设备数据缓存在内存中（Page Cache/Buffer Cache）。当用户请求读取数据时，如果缓存里有，就直接从内存返回，速度极快，根本不需要访问物理设备。写操作也可能先写入缓存，稍后再“刷”到磁盘上。 驱动的角色： 块设备驱动的主要工作不是直接处理 read/write，而是从请求队列中取出已经由 I/O 调度器优化好的 request，然后根据 request 里的信息（起始块号、块数量、方向），操作硬件来完成真正的数据传输。 使用： 通常不直接用 read/write 对 /dev/sda 这样的裸设备进行操作（虽然也可以）。但更常见的用法是：在块设备上创建文件系统（mkfs.ext4 /dev/sda1），然后 mount 到一个目录上。之后，用户和程序就通过文件系统来访问，享受到了文件系统和块设备层共同带来的高效和便利。 典型例子： 硬盘 (HDD/SSD)(/dev/sda)、U盘 (/dev/sdb)、SD卡 (/dev/mmcblk0)、RAM disk（内存模拟的块设备）、Flash 存储 (通过 MTD): NAND/NOR Flash 在 MTD 层之上也可以表现为块设备。 代码示例 CLICK 3. 网络设备 (Network Devices) 网络设备是用于收发数据包（Packet）的设备。它和其他两类设备有本质区别，它不对应 /dev 目录下的文件节点。而是通过单独的网络接口来代表。 原理 核心： net_device_ops 结构体和 sk_buff (Socket Buffer)。 接口而非文件： 网络设备在内核中被抽象成一个接口（Interface），如 eth0, wlan0。用户空间程序通过 Socket API（socket(), bind(), sendto(), recvfrom()）等内核协议栈来与内核的 TCP/IP 协议栈交互，而不是操作设备文件。 数据流： 发送： 用户数据通过 Socket API 进入内核协议栈，被层层打包（加上 TCP/UDP 头、IP 头等），最终形成一个 sk_buff 结构体。这个 sk_buff 被交给网络设备驱动。驱动的 ndo_start_xmit 函数（定义在 net_device_ops 中）负责将 sk_buff 里的数据包通过物理网卡发送出去。 接收： 网卡收到一个数据包，产生硬件中断。驱动的中断处理程序把数据从硬件接收到内存，封装成一个新的 sk_buff，然后把它交给内核网络协议栈。协议栈逐层解包，最后通过 Socket 将数据送达正确的应用程序。 驱动的角色： 网络设备驱动是硬件和内核协议栈之间的“搬运工”，主要负责：初始化网卡、启动/停止数据收发、在 sk_buff 和硬件之间传递数据包。 典型例子： 有线网卡 (eth0, enp3s0)、无线网卡 (wlan0)虚拟网络接口、 CAN总线设备、 USB网络适配器。 代码示例 CLICK 对比 特性 字符设备 (Char) 块设备 (Block) 网络设备 (Net) 平台驱动 (Platform) 数据单位 字节流 (Stream) 数据块 (Block) 数据包 (Packet) 不直接处理数据流 访问方式 顺序访问 随机访问 Socket API N/A I/O 缓冲 无 (或很简单) 有内核缓冲/缓存和I/O调度 有 Socket 缓冲 N/A 用户接口 /dev 文件节点 /dev 文件节点, 文件系统 Socket 接口, ifconfig 通常是为其他驱动提供服务 核心结构体 file_operations block_device_operations net_device_ops platform_driver 核心机制 VFS 文件操作映射 请求队列和I/O调度 协议栈和sk_buff 设备与驱动的分离、匹配、探测 主要用途 简单、串行 I/O 设备 存储设备 网络通信 SoC 内部集成外设的管理框架 字符设备代码示例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177#include &lt;linux/module.h&gt;#include &lt;linux/kernel.h&gt;#include &lt;linux/init.h&gt;#include &lt;linux/fs.h&gt; // 包含 file_operations 结构体#include &lt;linux/cdev.h&gt; // 包含 cdev 结构体和相关函数#include &lt;linux/device.h&gt; // 包含 class_create 和 device_create#include &lt;linux/uaccess.h&gt; // 包含 copy_to_user 和 copy_from_user#include &lt;linux/slab.h&gt; // 包含 kmalloc 和 kfree#define DEVICE_NAME &quot;mymem_char&quot;#define CLASS_NAME &quot;mymem_class&quot;#define MAX_BUFFER_SIZE 1024// --- 驱动核心数据结构 ---static int major_number; // 主设备号static char *kernel_buffer; // 内核数据缓冲区static struct class* my_class = NULL; // 设备类static struct cdev my_cdev; // 字符设备结构// --- file_operations 函数实现 ---// open 函数：当设备文件被打开时调用static int my_open(struct inode *inode, struct file *file){ printk(KERN_INFO &quot;MyCharDev: Device opened.\\n&quot;); // 通常可以在这里为每个打开实例分配私有数据 // file-&gt;private_data = ... return 0;}// release 函数：当设备文件被关闭时调用static int my_release(struct inode *inode, struct file *file){ printk(KERN_INFO &quot;MyCharDev: Device closed.\\n&quot;); // 清理 open 时分配的私有数据 // kfree(file-&gt;private_data); return 0;}// read 函数：从设备读取数据static ssize_t my_read(struct file *filp, char __user *user_buf, size_t len, loff_t *offset){ int bytes_to_read; // 检查读取长度是否有效 if (*offset &gt;= MAX_BUFFER_SIZE) return 0; // End of file if (*offset + len &gt; MAX_BUFFER_SIZE) len = MAX_BUFFER_SIZE - *offset; bytes_to_read = len; // 使用 copy_to_user 将内核数据拷贝到用户空间 if (copy_to_user(user_buf, kernel_buffer + *offset, bytes_to_read) != 0) { printk(KERN_ERR &quot;MyCharDev: Failed to copy data to user.\\n&quot;); return -EFAULT; } *offset += bytes_to_read; // 更新文件偏移 printk(KERN_INFO &quot;MyCharDev: Read %d bytes.\\n&quot;, bytes_to_read); return bytes_to_read;}// write 函数：向设备写入数据static ssize_t my_write(struct file *filp, const char __user *user_buf, size_t len, loff_t *offset){ int bytes_to_write; // 检查写入位置是否有效 if (*offset &gt;= MAX_BUFFER_SIZE) { printk(KERN_WARNING &quot;MyCharDev: No space left on device.\\n&quot;); return -ENOSPC; // No space left on device } if (*offset + len &gt; MAX_BUFFER_SIZE) len = MAX_BUFFER_SIZE - *offset; bytes_to_write = len; // 使用 copy_from_user 将用户数据拷贝到内核空间 if (copy_from_user(kernel_buffer + *offset, user_buf, bytes_to_write) != 0) { printk(KERN_ERR &quot;MyCharDev: Failed to copy data from user.\\n&quot;); return -EFAULT; } *offset += bytes_to_write; // 更新文件偏移 printk(KERN_INFO &quot;MyCharDev: Wrote %d bytes.\\n&quot;, bytes_to_write); return bytes_to_write;}// --- file_operations 结构体定义 ---// 将实现的函数与标准文件操作关联起来static struct file_operations fops = { .owner = THIS_MODULE, .open = my_open, .release = my_release, .read = my_read, .write = my_write,};// --- 模块初始化函数 ---static int __init memchar_init(void){ dev_t dev_num; // 1. 分配内核缓冲区 kernel_buffer = kmalloc(MAX_BUFFER_SIZE, GFP_KERNEL); if (!kernel_buffer) { printk(KERN_ERR &quot;MyCharDev: Failed to allocate kernel buffer.\\n&quot;); return -ENOMEM; } // 2. 动态分配主设备号 if (alloc_chrdev_region(&amp;dev_num, 0, 1, DEVICE_NAME) &lt; 0) { printk(KERN_ERR &quot;MyCharDev: Failed to allocate major number.\\n&quot;); kfree(kernel_buffer); return -1; } major_number = MAJOR(dev_num); printk(KERN_INFO &quot;MyCharDev: Major number allocated: %d\\n&quot;, major_number); // 3. 初始化 cdev 结构体，并与 file_operations 关联 cdev_init(&amp;my_cdev, &amp;fops); my_cdev.owner = THIS_MODULE; // 4. 将 cdev 添加到内核 if (cdev_add(&amp;my_cdev, dev_num, 1) &lt; 0) { printk(KERN_ERR &quot;MyCharDev: Failed to add cdev to the kernel.\\n&quot;); unregister_chrdev_region(dev_num, 1); kfree(kernel_buffer); return -1; } // 5. 创建设备类 my_class = class_create(THIS_MODULE, CLASS_NAME); if (IS_ERR(my_class)) { printk(KERN_ERR &quot;MyCharDev: Failed to create device class.\\n&quot;); cdev_del(&amp;my_cdev); unregister_chrdev_region(dev_num, 1); kfree(kernel_buffer); return PTR_ERR(my_class); } // 6. 创建设备文件 (/dev/mymem_char) if (device_create(my_class, NULL, dev_num, NULL, DEVICE_NAME) == NULL) { printk(KERN_ERR &quot;MyCharDev: Failed to create device file.\\n&quot;); class_destroy(my_class); cdev_del(&amp;my_cdev); unregister_chrdev_region(dev_num, 1); kfree(kernel_buffer); return -1; } printk(KERN_INFO &quot;MyCharDev: Driver loaded successfully.\\n&quot;); return 0;}// --- 模块卸载函数 ---static void __exit memchar_exit(void){ dev_t dev_num = MKDEV(major_number, 0); // 逆序清理资源 device_destroy(my_class, dev_num); // 销毁设备文件 class_destroy(my_class); // 销毁设备类 cdev_del(&amp;my_cdev); // 从内核移除 cdev unregister_chrdev_region(dev_num, 1); // 释放设备号 kfree(kernel_buffer); // 释放内核缓冲区 printk(KERN_INFO &quot;MyCharDev: Driver unloaded.\\n&quot;);}module_init(memchar_init);module_exit(memchar_exit);MODULE_LICENSE(&quot;GPL&quot;);MODULE_AUTHOR(&quot;Your Name&quot;);MODULE_DESCRIPTION(&quot;A simple character device driver for memory simulation.&quot;); 块设备代码示例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128#include &lt;linux/module.h&gt;#include &lt;linux/kernel.h&gt;#include &lt;linux/init.h&gt;#include &lt;linux/genhd.h&gt; // 包含 gendisk#include &lt;linux/fs.h&gt; // 包含 block_device_operations#include &lt;linux/blkdev.h&gt; // 包含请求队列相关函数#include &lt;linux/vmalloc.h&gt; // 使用 vmalloc 分配大块内存#define DEVICE_NAME &quot;myram_block&quot;#define SECTOR_SIZE 512#define DEVICE_SECTORS 20480 // 10MB (20480 * 512 bytes)// --- 驱动核心数据结构 ---static int major_number; // 主设备号static u8 *device_data; // 模拟磁盘的内存区域static struct gendisk *my_disk; // gendisk 结构，代表一个独立的磁盘static struct request_queue *my_queue; // 请求队列static spinlock_t lock; // 用于保护请求队列的自旋锁// --- 请求处理函数 ---// 这是块设备驱动的核心，处理来自I/O调度器的请求static void my_request_fn(struct request_queue *q){ struct request *req; // 循环处理队列中的所有请求 while ((req = blk_fetch_request(q)) != NULL) { // 检查请求是否合法（这里简化处理，只检查读写请求） if (req == NULL || (rq_data_dir(req) != READ &amp;&amp; rq_data_dir(req) != WRITE)) { printk(KERN_NOTICE &quot;MyRamBlock: Skipping non-RW request\\n&quot;); __blk_end_request_all(req, -EIO); continue; } // 计算物理地址和大小 // blk_rq_pos(req) 返回起始扇区号 // blk_rq_cur_bytes(req) 返回请求的总字节数 unsigned long offset = blk_rq_pos(req) * SECTOR_SIZE; unsigned long num_bytes = blk_rq_cur_bytes(req); // 模拟数据传输 if (rq_data_dir(req) == WRITE) { // bio_for_each_segment 遍历请求中的所有段 (segment) // 将请求缓冲区中的数据拷贝到我们的模拟磁盘内存 memcpy(device_data + offset, bio_data(req-&gt;bio), num_bytes); } else { // 将模拟磁盘内存中的数据拷贝到请求缓冲区 memcpy(bio_data(req-&gt;bio), device_data + offset, num_bytes); } // 标记请求完成 __blk_end_request_all(req, 0); // 0 表示成功 }}// --- block_device_operations ---// 对于简单的驱动，这个结构体可以为空static struct block_device_operations my_bops = { .owner = THIS_MODULE,};// --- 模块初始化函数 ---static int __init ramblock_init(void){ // 1. 分配模拟磁盘的内存 device_data = vmalloc(DEVICE_SECTORS * SECTOR_SIZE); if (!device_data) { return -ENOMEM; } // 2. 注册块设备，获取主设备号 major_number = register_blkdev(0, DEVICE_NAME); if (major_number &lt; 0) { vfree(device_data); return major_number; } // 3. 初始化自旋锁和请求队列 spin_lock_init(&amp;lock); my_queue = blk_init_queue(my_request_fn, &amp;lock); if (!my_queue) { unregister_blkdev(major_number, DEVICE_NAME); vfree(device_data); return -ENOMEM; } // 4. 分配和初始化 gendisk 结构 my_disk = alloc_disk(1); // 1个次设备 (分区) if (!my_disk) { blk_cleanup_queue(my_queue); unregister_blkdev(major_number, DEVICE_NAME); vfree(device_data); return -ENOMEM; } // 5. 填充 gendisk 信息 my_disk-&gt;major = major_number; my_disk-&gt;first_minor = 0; my_disk-&gt;fops = &amp;my_bops; my_disk-&gt;queue = my_queue; snprintf(my_disk-&gt;disk_name, 32, DEVICE_NAME); set_capacity(my_disk, DEVICE_SECTORS); // 设置磁盘容量（以扇区为单位） // 6. 将 gendisk 添加到系统，使其可见 add_disk(my_disk); printk(KERN_INFO &quot;MyRamBlock: Driver loaded. Major: %d\\n&quot;, major_number); return 0;}// --- 模块卸载函数 ---static void __exit ramblock_exit(void){ del_gendisk(my_disk); // 从系统移除 gendisk put_disk(my_disk); // 释放 gendisk 引用 blk_cleanup_queue(my_queue); // 清理请求队列 unregister_blkdev(major_number, DEVICE_NAME); // 注销块设备 vfree(device_data); // 释放内存 printk(KERN_INFO &quot;MyRamBlock: Driver unloaded.\\n&quot;);}module_init(ramblock_init);module_exit(ramblock_exit);MODULE_LICENSE(&quot;GPL&quot;);MODULE_AUTHOR(&quot;Your Name&quot;);MODULE_DESCRIPTION(&quot;A simple RAM-based block device driver.&quot;); 网络设备代码示例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133#include &lt;linux/module.h&gt;#include &lt;linux/kernel.h&gt;#include &lt;linux/init.h&gt;#include &lt;linux/netdevice.h&gt; // 包含 net_device 和相关函数#include &lt;linux/etherdevice.h&gt; // 包含 alloc_etherdev#define DEVICE_NAME &quot;mynet&quot;// --- 驱动核心数据结构 ---// 我们将自定义的统计信息和设备指针放在一个结构体中struct mynet_priv { struct net_device_stats stats; struct net_device *dev;};static struct net_device *my_net_dev;// --- net_device_ops 函数实现 ---// open 函数：当接口被 &quot;ifconfig up&quot; 启动时调用static int mynet_open(struct net_device *dev){ // 启动传输队列 netif_start_queue(dev); printk(KERN_INFO &quot;%s: Device opened.\\n&quot;, dev-&gt;name); return 0;}// stop 函数：当接口被 &quot;ifconfig down&quot; 关闭时调用static int mynet_stop(struct net_device *dev){ // 停止传输队列 netif_stop_queue(dev); printk(KERN_INFO &quot;%s: Device stopped.\\n&quot;, dev-&gt;name); return 0;}// 发包函数：这是网络驱动的核心，负责发送数据包static netdev_tx_t mynet_start_xmit(struct sk_buff *skb, struct net_device *dev){ struct mynet_priv *priv = netdev_priv(dev); printk(KERN_INFO &quot;%s: Transmitting packet (len: %u)\\n&quot;, dev-&gt;name, skb-&gt;len); // 更新统计信息 priv-&gt;stats.tx_packets++; priv-&gt;stats.tx_bytes += skb-&gt;len; // --- 模拟环回 --- // 正常驱动会在这里把 skb 的数据通过 DMA 发送到硬件 // 我们直接将 skb 重新送回收包路径 skb-&gt;protocol = eth_type_trans(skb, dev); // 设置协议类型 skb-&gt;dev = dev; netif_rx(skb); // 将 skb 传递给内核协议栈的接收部分 // 告诉内核数据包已发送，可以释放 skb // dev_kfree_skb(skb); // 真实驱动中发送完会释放 skb // 但因为我们环回了，协议栈会负责释放它 return NETDEV_TX_OK; // 返回 OK 表示发送成功}// 获取统计信息函数static struct net_device_stats *mynet_get_stats(struct net_device *dev){ struct mynet_priv *priv = netdev_priv(dev); return &amp;priv-&gt;stats;}// --- net_device_ops 结构体定义 ---static const struct net_device_ops mynet_ops = { .ndo_open = mynet_open, .ndo_stop = mynet_stop, .ndo_start_xmit = mynet_start_xmit, .ndo_get_stats = mynet_get_stats,};// --- setup 函数，用于初始化设备 ---void mynet_setup(struct net_device *dev){ // 设置为以太网设备 ether_setup(dev); // 关联我们的操作函数 dev-&gt;netdev_ops = &amp;mynet_ops; // 分配一个随机的 MAC 地址 eth_hw_addr_random(dev); // 其他设备特性标志 dev-&gt;flags |= IFF_NOARP;}// --- 模块初始化函数 ---static int __init netloop_init(void){ struct mynet_priv *priv; // 1. 分配 net_device 结构体，并为私有数据分配空间 my_net_dev = alloc_netdev(sizeof(struct mynet_priv), DEVICE_NAME, NET_NAME_UNKNOWN, mynet_setup); if (!my_net_dev) { return -ENOMEM; } // 获取私有数据指针 priv = netdev_priv(my_net_dev); priv-&gt;dev = my_net_dev; // 2. 注册网络设备到内核 if (register_netdev(my_net_dev)) { printk(KERN_ERR &quot;Failed to register net device\\n&quot;); free_netdev(my_net_dev); return -1; } printk(KERN_INFO &quot;%s: Driver loaded.\\n&quot;, my_net_dev-&gt;name); return 0;}// --- 模块卸载函数 ---static void __exit netloop_exit(void){ printk(KERN_INFO &quot;%s: Unloading driver.\\n&quot;, my_net_dev-&gt;name); unregister_netdev(my_net_dev); // 从内核注销 free_netdev(my_net_dev); // 释放 net_device}module_init(netloop_init);module_exit(netloop_exit);MODULE_LICENSE(&quot;GPL&quot;);MODULE_AUTHOR(&quot;Your Name&quot;);MODULE_DESCRIPTION(&quot;A simple loopback network device driver.&quot;);","link":"/post/the-Three-Basic-Linux-Driver-Models.html"}],"tags":[{"name":"Toolchain","slug":"Toolchain","link":"/tags/Toolchain/"},{"name":"kernel","slug":"kernel","link":"/tags/kernel/"},{"name":"driver","slug":"driver","link":"/tags/driver/"},{"name":"ebpf","slug":"ebpf","link":"/tags/ebpf/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"bpftrace","slug":"bpftrace","link":"/tags/bpftrace/"},{"name":"libbpf","slug":"libbpf","link":"/tags/libbpf/"},{"name":"bcc","slug":"bcc","link":"/tags/bcc/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"lock","slug":"lock","link":"/tags/lock/"},{"name":"risc-v","slug":"risc-v","link":"/tags/risc-v/"}],"categories":[{"name":"note","slug":"note","link":"/categories/note/"},{"name":"kernel","slug":"kernel","link":"/categories/kernel/"},{"name":"blog","slug":"blog","link":"/categories/blog/"},{"name":"compiler","slug":"note/compiler","link":"/categories/note/compiler/"},{"name":"driver","slug":"kernel/driver","link":"/categories/kernel/driver/"},{"name":"qspinlock","slug":"kernel/qspinlock","link":"/categories/kernel/qspinlock/"},{"name":"risc-v","slug":"risc-v","link":"/categories/risc-v/"},{"name":"ebpf","slug":"kernel/ebpf","link":"/categories/kernel/ebpf/"},{"name":"hexo","slug":"blog/hexo","link":"/categories/blog/hexo/"},{"name":"Toolchain","slug":"risc-v/Toolchain","link":"/categories/risc-v/Toolchain/"}],"pages":[{"title":"","text":"Troy's Blog >>> 欢迎交换友链~ 请通过邮件联系我。","link":"/friend/index.html"}]}