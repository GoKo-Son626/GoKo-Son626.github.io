{"posts":[{"title":"Trace-do-chenxiaosong-1","text":"ftrace使用 位置 作用 你在 RISC‑V trace 任务中主要用途 current_tracer 读/写当前激活的 tracer (nop/function_graph/irqsoff…) 切换 tracer，常用 function_graph、nop tracing_on 全局开关（0/1） 一键暂停/继续收集 trace, trace_pipe 输出缓冲区• trace：静态快照• trace_pipe：流式实时读取 消费跟踪结果： `cat trace less&lt;br&gt; cat trace_pipe &gt; out.log` available_tracers 支持的 tracer 列表 选型时查看硬件是否支持 function_graph 等 events/ 所有 tracepoint 目录树每个事件都有 enable、format • 启/停特定 tracepoint• 查看字段布局，写 eBPF/bpftrace 时要读 format set_event 批量开启事件，写 subsys:event 比循环 echo 更快 set_ftrace_filter, set_ftrace_notrace 选择跟踪/排除哪些函数 function/function_graph 模式下做白/黑名单 available_filter_functions 可被动/静态跟踪的全部符号 搜函数地址做动态 FTRACE 打桩 kprobe_events, uprobe_events 动态插桩描述文件 向内核注册/注销 kprobe、uprobe synthetic_events 创建用户自定义事件 复杂场景用来把多事件关联聚合 instances/ 支持创建多实例 buffer 需要隔离多组 trace 时使用 trace_clock 选择时间戳来源（local, global, mono） 跟多核/跨系统比较时间戳时切换 options/, trace_options 各类细粒度选项（打印 pid、latency …） 开启 funcgraph-proc, sym-offset 等提高可读性 buffer_size_kb, buffer_total_size_kb 每‑CPU 或全局 ring‑buffer 大小 长时间采样前调大避免丢包 per_cpu/, cpumask 每 CPU 的 buffer/开关 跟踪特定核或查看局部 stats osnoise/, hwlat_detector/ 专用延迟分析 tracer 若排查 IRQ/Jitter 可用 dynamic_events 综合视图（kprobe/uprobe/synth 等） 快速列出现有动态事件","link":"/post/Trace-do-chenxiaosong-1.html"},{"title":"Trace-use-2","text":"快速构建 Trace 子系统“认知” 2 Using the TRACE_EVENT() macro（part 1） 在第一部分中，我们解释了在核心内核中创建跟踪点的过程。本文将继续介绍使用 DECLARE_EVENT_CLASS()宏来降低跟踪点占用空间的技巧。此外，本文还介绍了用于构建TP_STRUCT__entry字段的宏，并 解释了 TP_printk辅助函数。 如果两个事件具有相同的TP_PROTO、TP_ARGS和TP_STRUCT__entry ，就应该有一种方法让这些事件共享它们使用的函数。这就是新宏DECLARE_EVENT_CLASS()（最初称为TRACE_EVENT_TEMPLATE()）和DEFINE_EVENT()的动机。 内核里很多跟踪事件（TRACE_EVENT()）结构几乎一模一样，只是名字不同而已。例如：sched_wakeup和sched_wakeup_new。DECLARE_EVENT_CLASS()（声明一个模板）：它定义一套“通用模板”——参数、结构体布局、赋值逻辑、打印格式都写好，供多个事件共享。 例如：DECLARE_EVENT_CLASS(sched_wakeup_template, …)可定义： 1234567DEFINE_EVENT(sched_wakeup_template, sched_wakeup, TP_PROTO(struct task_struct *p), TP_ARGS(__perf_task(p)));DEFINE_EVENT(sched_wakeup_template, sched_wakeup_new, TP_PROTO(struct task_struct *p), TP_ARGS(__perf_task(p))); 预处理器preprocessor：是 C 编译器在真正编译代码前，先干的一些“文本替换”活：宏系统，就是指你用 #define 写的那些“模板规则”：DECLARE_EVENT_CLASS(sched_wakeup_template, TP_PROTO(struct task_struct *p), TP_ARGS(p), …); DEFINE_EVENT(sched_wakeup_template, sched_wakeup, TP_PROTO(struct task_struct *p), // ← 这里 TP_ARGS(p)); // ← 这里指向的两行，预处理宏就会重复写，传入参数 TP_STRUCT__entry宏第一篇文章提到了__field和__array宏，它们用于创建存储在环形缓冲区中的事件的结构格式。__ field(type, item)声明了结构体中一个名为item的字段，其类型为 type （ 即type item;）。__ array(type, item, len)声明了一个名为item的静态数组， 其元素个数为 len （即type item[len];）。这两个宏最为常见，但还有其他宏允许将事件存储到环形缓冲区中，实现更复杂的操作。 123456789101112131415161718// 普通字段__field(type, name)__field_ext(type, name, filter_type)// 动态数据字段__string(name, src) // 可变长字符串__dynamic_array(type, name, len) // 任意动态数组// 数据赋值__assign_str(name, src)memcpy(__get_dynamic_array(name), src, len)// 打印输出__get_str(name)__get_dynamic_array(name)__print_flags(val, delim, {mask, name}...) // 位掩码打印__print_symbolic(val, {value, name}...) // 枚举值打印 1234567891011121314151617◆ 启用 / 关闭 echo 1 &gt; events/&lt;sub&gt;/&lt;event&gt;/enable # 开启单个 echo 0 &gt; events/enable # 关闭所有 echo e1 e2 e3 &gt; set_event # 批量开启 e1,e2,e3◆ 跟踪输出 cat trace # 直接读 trace-cmd record / report # 更灵活、支持保存文件 perf record -e &lt;sub&gt;:&lt;event&gt; # perf 也能收◆ 事件头文件关键宏 __field(type, name) # 固定字段 __string(name, src) / __assign_str ... # 动态字符串 __dynamic_array(type, name, len) # 任意可变数组 __get_str(name) / __get_dynamic_array(name)# 访问可变数据 __print_flags(val, delim, {mask,&quot;name&quot;},…) # 位掩码友好打印 __print_symbolic(val, {value,&quot;name&quot;},…) # 枚举值打印","link":"/post/Trace-use-2.html"},{"title":"Trace-structure","text":"快速构建 Trace 子系统“认知” 1 Using the TRACE_EVENT() macro（part 1）1234567891011121314151617181920212223242526272829303132┌──────────────────────────────────────────────────────────────────┐│ 🌍 用户空间工具层 ││ strace* trace-cmd perf bpftrace bpftool kernelshark … ││ · strace 仅靠 ptrace，不入内核 Trace 体系 │└───────────────┬──────────────────────────────────────────────────┘ │ 调用 / 录制 / 加载┌───────────────▼──────────────────────────────────────────────────┐│ 🧩 统一 Trace 控制接口 (tracefs) ││ /sys/kernel/debug/tracing/* (ftrace debugfs) ││ tracefs APIs (kernel/trace/*.c) │└───────────────┬──────────────────────────────────────────────────┘ │ 读写控制文件或 ioctl┌───────────────▼──────────────────────────────────────────────────┐│ 🏗 内核 Trace 基础框架 (TRACE SUBSYS) ││ • ftrace core —— function/function_graph/irqsoff/… ││ • trace events —— 事件注册表、filters、ring‑buffer ││ • hook dispatch —— 把数据写 ring buffer 或调用 eBPF │└───────────────┬──────────────────────────────────────────────────┘ │ 调用 / 附着┌───────────────▼──────────────────────────────────────────────────┐│ 🔧 低层插桩钩子 / Instrumentation 点 ││ ├─ tracepoints (由 TRACE_EVENT / DEFINE_EVENT 生成) ││ ├─ kprobes (动态插内核指令) ││ ├─ uprobes (动态插用户进程指令) ││ └─ fentry/fexit (BPF_FENTRY, 比 kprobe 更轻量的 BPF 钩子) │└───────────────┬──────────────────────────────────────────────────┘ │ attach┌───────────────▼──────────────────────────────────────────────────┐│ ⚙️ eBPF 执行层 (BPF VM) ││ • BPF 程序可挂 tracepoints / kprobe / fentry / perf events ││ • 运行后可把数据写 perf‑ring‑buffer / maps → 用户空间 │└──────────────────────────────────────────────────────────────────┘ 关键说明 层 说明 用户空间工具层 负责“控制/采集/解析”。trace-cmd, perf, bpftrace 都经由 tracefs 或 perf_event 系统调用与内核交互。strace 仅基于 ptrace()，并不依赖内核 Trace 框架。 tracefs 控制接口 /sys/kernel/debug/tracing 暴露一堆文件，如 current_tracer, events/*/enable, 任何工具都可以直接 echo 或 ioctl；trace-cmd 就是批量操作这些文件。 内核 Trace 基础框架 把 ftrace（函数级）与 trace events（事件级）统一；决定如何写 ring buffer、如何做过滤；同层还包括 irqsoff、preemptoff、wakeup 等特定 tracer。 低层插桩钩子 真正“被内核代码调用或打补丁”的地方：• tracepoints = 静态宏 TRACE_EVENT() 生成的函数；• kprobe/uprobe = 运行时在指令头插入 int3 等陷阱；• fentry/fexit = BPF 直接在函数 prologue/epilogue 附钩；这些钩子把采样数据交给上一层框架处理。 eBPF 执行层 属于内核，是一种运行在内核里的“虚拟机”。它不是进程，而是内核中的程序运行环境，支持加载、运行小程序（字节码）。这些小程序由用户态工具编译/加载，比如：bpftrace -e ‘tracepoint:syscalls:sys_enter_open { printf(“open called\\n”); }’ bpftool prog load prog.o /sys/fs/bpf/… 这些程序被“加载到内核内存中，并绑定到某个 hook 点（比如 tracepoint）上”。但可加载/卸载字节码；可以挂到 tracepoint/kprobe 等；执行逻辑后把结果写 ring buffer 或 BPF maps，用户空间工具再读。 eBPF 程序是用户空间编译 → 系统调用传给内核 → verifier 验证 → 挂到钩子点 → 运行 → 可以卸载 工具( trace-cmd / perf / bpftrace ) → 操作 tracefs 接口 → 驱动 ftrace + trace events 框架 → 依托 tracepoint / kprobe / fentry 等钩子收数 → (可选) 交给 eBPF 做实时处理 → 数据落到 ring buffer → 工具解析显示 钩子函数是一种机制：提供“挂钩”位置，回调函数是一个函数（你写的），函数挂到钩子上后钩子被触发可以调用函数钩子函数：“钩子函数”本质上就是一种 特定用途的回调函数。我们在内核里用“钩子”这个词，意思是：“我在这里留了一个钩子（hook point），你可以挂上自己的函数，当某个行为发生时，这个函数就会被调用。”回调函数：回调函数就是你提前定义好的函数指针，然后在某个时机由别人（系统、库、框架）调用它。你把 my_callback 注册进去；当“事件”触发时，系统调用你的my_callback。ex: 123456789101112void my_callback(int value) { printf(&quot;callback called with value = %d\\n&quot;, value);}void trigger_event(void (*cb)(int)) { cb(42); // 在“事件”发生时，调用你的回调函数}int main() { trigger_event(my_callback); // 注册回调 return 0;} trace marker 是一种早期的内核跟踪手段：直接把 printf(“some event happened: %d”, value); 这样的格式化字符串写进了内核代码里，非常像是“调试信息”。但这做法污染了代码，看起来像是 debug 没删干净。 后来 Mathieu Desnoyers 设计了 tracepoints，做法是： 在内核的某些逻辑点放一个函数调用，比如 trace_my_event(foo, bar); 这个函数不会直接打印任何信息，而是会去查有没有人注册了回调函数（也就是挂钩子）； 如果有人注册了，就调用注册者的回调函数，把参数 foo, bar 传进去。就像我上面举的 trigger_event(cb) 的例子。这样做有两个好处： 内核代码本身 不再关心调试或跟踪逻辑，只留了个钩子点； 回调函数可以接收类型明确的结构体指针，效率更高，不需要去解析格式化字符串了。 但是问题是：你每次想使用 tracepoint，就要写一堆 callback 函数，重复又繁琐。 为了解决“写回调太麻烦”的问题：TRACE_EVENT() 宏诞生：这个宏帮你自动生成： - tracepoint 的定义； - 对应的 callback 函数（钩子函数）； - 数据格式化的逻辑。你只需要写一个宏描述，比如： 12345678910111213TRACE_EVENT(my_event, TP_PROTO(int a, int b), TP_ARGS(a, b), TP_STRUCT__entry( __field(int, a) __field(int, b) ), TP_fast_assign( __entry-&gt;a = a; __entry-&gt;b = b; ), TP_printk(&quot;a=%d b=%d&quot;, __entry-&gt;a, __entry-&gt;b)); 然后一切都自动生成，Ftrace、perf、LTTng、SystemTap 都能用这套系统来进行跟踪。TRACE_EVENT()宏的剖析自动化跟踪点有各种必须满足的要求： 它必须创建一个可以放置在内核代码中的跟踪点。 它必须创建一个可以挂接到该跟踪点的回调函数。 回调函数必须能够以最快的方式将传递给它的数据记录到跟踪器环形缓冲区中。 它必须创建一个函数，可以解析记录到环形缓冲区的数据并将其转换为跟踪器可以显示给用户的人类可读的格式。为了实现这一点，TRACE_EVENT()宏被分解为六个部分，它们与宏的参数相对应：TRACE_EVENT（名称、协议、参数、结构、分配、打印） name——要创建的跟踪点的名称。 原型- 跟踪点回调的原型 args - 与原型匹配的参数。 struct - 跟踪器可以使用（但不是必须）来存储传递到跟踪点的数据的结构。 分配——以类似 C 的方式将数据分配给结构。 print - 以人类可读的 ASCII 格式输出结构的方式。 Tracepoint 名称 用途简介 sched_switch 核心 tracepoint，任务切换时触发 sched_wakeup 有任务被唤醒（通常进入可运行队列）时触发 sched_wakeup_new 新创建任务唤醒时触发（区别于已有任务） sched_migrate_task 任务在 CPU 之间迁移时触发 sched_kthread_stop / _ret 内核线程停止相关 sched_kthread_work_* 内核线程 workqueue 调度过程 sched_process_fork 创建子进程时触发 sched_process_exec exec 调用替换程序映像时触发 sched_process_exit 任务退出时触发 核心tracepoint: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/* * Tracepoint for task switches, performed by the scheduler: */TRACE_EVENT(sched_switch, TP_PROTO(bool preempt, struct task_struct *prev, struct task_struct *next, unsigned int prev_state), TP_ARGS(preempt, prev, next, prev_state), TP_STRUCT__entry( __array( char, prev_comm, TASK_COMM_LEN ) __field( pid_t, prev_pid ) __field( int, prev_prio ) __field( long, prev_state ) __array( char, next_comm, TASK_COMM_LEN ) __field( pid_t, next_pid ) __field( int, next_prio ) ), TP_fast_assign( memcpy(__entry-&gt;prev_comm, prev-&gt;comm, TASK_COMM_LEN); __entry-&gt;prev_pid = prev-&gt;pid; __entry-&gt;prev_prio = prev-&gt;prio; __entry-&gt;prev_state = __trace_sched_switch_state(preempt, prev_state, prev); memcpy(__entry-&gt;next_comm, next-&gt;comm, TASK_COMM_LEN); __entry-&gt;next_pid = next-&gt;pid; __entry-&gt;next_prio = next-&gt;prio; /* XXX SCHED_DEADLINE */ ), TP_printk(&quot;prev_comm=%s prev_pid=%d prev_prio=%d prev_state=%s%s ==&gt; next_comm=%s next_pid=%d next_prio=%d&quot;, __entry-&gt;prev_comm, __entry-&gt;prev_pid, __entry-&gt;prev_prio, (__entry-&gt;prev_state &amp; (TASK_REPORT_MAX - 1)) ? __print_flags(__entry-&gt;prev_state &amp; (TASK_REPORT_MAX - 1), &quot;|&quot;, { TASK_INTERRUPTIBLE, &quot;S&quot; }, { TASK_UNINTERRUPTIBLE, &quot;D&quot; }, { __TASK_STOPPED, &quot;T&quot; }, { __TASK_TRACED, &quot;t&quot; }, { EXIT_DEAD, &quot;X&quot; }, { EXIT_ZOMBIE, &quot;Z&quot; }, { TASK_PARKED, &quot;P&quot; }, { TASK_DEAD, &quot;I&quot; }) : &quot;R&quot;, __entry-&gt;prev_state &amp; TASK_REPORT_MAX ? &quot;+&quot; : &quot;&quot;, __entry-&gt;next_comm, __entry-&gt;next_pid, __entry-&gt;next_prio)); 除第一个参数外，所有参数都封装在另一个宏中（TP_PROTO、TP_ARGS、TP_STRUCT__entry、 TP_fast_assign和TP_printk ）。这些宏在处理过程中提供了更多控制，并且允许在TRACE_EVENT()宏中使用逗号。第一个参数是名称:第二个参数是原型: 它既是添加到内核代码的 tracepoint 的原型，也是回调函数的原型。tracepoint 调用回调函数，就像回调函数在 tracepoint 的位置被调用一样。第三个参数是原型使用的参数:这看起来可能有点奇怪，但这不仅是TRACE_EVENT() 宏所必需的，也是底层 tracepoint 基础架构所必需的。tracepoint 代码在激活时会调用回调函数（一个 tracepoint 可能被分配多个回调函数）。创建 tracepoint 的宏必须能够访问原型和参数。下面展示了 tracepoint 宏实现此目的所需的步骤： 123456#define TRACE_POINT(name, proto, args) \\void trace_##name(proto) \\{ \\ if (trace_##name##_active) \\ callback(args); \\} 第四个参数是结构: 这个结构 决定了每次 tracepoint 被触发时要记录什么内容到 trace buffer（跟踪缓冲区）中。定义 trace buffer 结构体字段; 就是告诉内核 ring buffer 要有哪几个字段、每个字段占多大第五个参数是任务: TP_fast_assign() 的主要作用就是将事件采样时的数据，写入到 tracepoint 对应的环形缓冲区中，其填充的对象就是 TP_STRUCT__entry 中定义的结构体字段。 __entry 是指向 TP_STRUCT__entry 中定义结构的指针第六个参数是打印: 定义 ftrace / trace-cmd / perf 等工具在输出 trace 事件时的格式字符串，相当于 printf 格式。 eBPF（Extended Berkeley Packet Filter） 是 Linux 内核中的一个强大机制，它允许你在内核空间中安全、受控地运行小程序，实现诸如： 性能分析（比如替代 perf 工具） 系统调用跟踪（比如替代 strace） 网络包过滤/监控（替代 iptables, tcpdump 等） 安全监控、沙箱 它最牛的地方在于：无需改内核代码、无需加载内核模块，就可以“在内核里运行代码”。 eBPF 程序： 是写在用户空间的程序（用 C 写，或者用更高级语言生成） 编译成 eBPF 字节码（像汇编一样） 加载到内核中 在某些钩子点（比如 tracepoint、kprobe、syscall）运行 eBPF 可以挂载到 tracepoint 上： 监听调度器行为（比如哪个进程切了谁） 拿到 sched_switch 提供的各种数据字段（prev_pid, next_pid, prev_state, …） 再把这些信息统计、上报、过滤、可视化 在 TP_STRUCT__entry(…) 中定义了 tracepoint 产生的数据结构： 123456789struct { char prev_comm[TASK_COMM_LEN]; pid_t prev_pid; int prev_prio; long prev_state; char next_comm[TASK_COMM_LEN]; pid_t next_pid; int next_prio;}; 这些就是 eBPF 程序“能看到、能读取”的字段。因为： 内核会把这些字段以结构体形式写入 ring buffer（跟踪缓冲区） eBPF 程序附加上去之后，会被传一个 ctx（上下文指针） 程序通过读取 ctx 中的字段来做分析处理 strace 是 纯用户态的工具，它基于 Linux 提供的 ptrace() 系统调用，通过 “截获进程的系统调用入口和返回” 实现功能。它并不知道 tracepoint 的存在，也不使用它。 什么叫“动态注入 eBPF 所需字段布局”？意思就是： 你不需要在写内核代码时就把 eBPF 写进去 内核只需要在 tracepoint 里用 TRACE_EVENT 正确定义了字段布局 eBPF 程序在运行时 attach 到该 tracepoint，就能动态读取这些字段 这就是“动态注入”： 不改内核 不重启系统 eBPF 程序运行时 attach 按 tracepoint 给出的字段布局访问数据 内容 意义 TRACE_EVENT() 定义一个 tracepoint 的结构体格式、打印格式 /sys/kernel/debug/tracing/events/*/format 描述 tracepoint 的字段结构和 printf 格式，供工具解析使用 define_trace.h 把 TRACE_EVENT 宏展开为函数定义，必须放在 #endif 外面。 CREATE_TRACE_POINTS 告诉编译器在这个 C 文件中生成函数定义，只能有一个文件这样写 tracepoint 的使用 只需调用 trace_xxx() 函数就能在内核中记录事件","link":"/post/Trace-use-1.html"},{"title":"Troy-interview","text":"1. linux启动1. 存储介质 SRAM &gt; DRAM&gt;&gt; ROM HDD sram: 静态随机存取存储器： 快 成本高: 用于高速缓存 dram： 动态随机存取存储器： 略慢 成本低： 内存条 rom： 只读存储器： 掉电不丢失 最慢： BIOS/UEFI/bootloader… ram includes sram and dram ROM (Read-Only Memory) 是一种只能读取不能修改的存储器，早期用于存放固定的启动代码。 NOR Flash 是一种特殊的闪存，它在存储启动代码方面取代了传统的ROM和一些早期的EEPROM。NOR Flash最大的特点是支持“执行到位”（Execute In Place, XIP），这意味着处理器可以直接从NOR Flash中读取并执行代码，而不需要先将代码加载到RAM中。这对于启动过程至关重要。 在PC上，BIOS (Basic Input/Output System) 或更现代的UEFI (Unified Extensible Firmware Interface) 就是存储在主板上的NOR Flash中，负责电脑开机时的自检、硬件初始化以及引导操作系统。 2. 系统上电 Soc会从固化的启动介质（如SPI NOR/NAND:「SPI」是指 Serial Peripheral Interface（串行外设接口），是一种通信总线协议，通常用于连接 Flash 存储器。SPI NOR / NAND Flash 是常见的非易失性存储器，上电后Soc中的BootROM会从固化的启动介质（如SPI NOR/NAND/eMMC/SD Card）读取并加载第一阶段的Bootloader， 通常是 SPL（Secondary Program Loader）二级程序加载器。 +——————+ | 上电复位 | +——–+———+ | v +——————+ | 执行BootROM | &lt;– 固化在 SoC 内部的不可修改代码（类似计算机的BIOS ROM） | | | **核心任务：** | | -&gt; 极简硬件初始化 | (例如：配置时钟、基本GPIO、电源管理单元) | -&gt; 初始化SoC内部SRAM | (这是CPU在外部DRAM可用前的唯一工作内存，容量通常几十到几百KB) | -&gt; 配置外部启动设备控制器| (例如：SPI控制器、SD/eMMC控制器，以便能从这些设备读取数据) | | | 从外部启动介质（如 SPI NOR/NAND Flash/eMMC/SD Card） | | 读取 SPL 的头部或整个 SPL -&gt; SoC内部SRAM | | 解释： | | * SPI NOR Flash： 理论上支持XIP（Execute In Place），CPU可以直接在其上执行代码。 | 但出于性能、通用性和后续阶段流程的统一性考虑，通常仍会将SPL加载到更快的SoC内部SRAM中执行。 | * NAND Flash/eMMC/SD Card： 它们是块设备，不支持XIP，因此必须先将SPL | （或其一部分）加载到SoC内部SRAM中才能执行。 | * 目的： 提供一个快速、可靠、上电即用的临时工作区，以执行下一阶段的启动代码。 | &lt;–加载第一阶段Bootloader:SPL:几十KB到几百KB +——–+———+ | v SPL = Secondary Program Loader +——————++————————————————-+ | 执行SPL | &lt;– 此时SPL在SoC内部SRAM中运行，执行效率高 | | | **核心任务：** | | -&gt; 初始化外部DRAM控制器和DRAM内存 | (这是SPL最关键的任务，使数GB的主内存可用) | -&gt; 初始化更多关键外设 | (例如：USB控制器、UART用于调试、MMC/SD控制器等) | -&gt; 加载 U-Boot 到外部DRAM | (U-Boot通常较大，需要外部DRAM提供足够空间) +——–+———+ | v +——————+ | 执行U-Boot | &lt;– 第二阶段 Bootloader，此时已在外部DRAM中运行 | | | **核心任务：** | | -&gt; 初始化更复杂的外设 | (例如：网络、显示、存储设备文件系统等) | -&gt; 提供命令行接口 | (供用户交互和调试，如设置环境变量) | -&gt; 加载 Linux Kernel、Device Tree Blob (dtb)、 | | Initial Ramdisk (initrd) 到外部DRAM | | -&gt; 根据配置，执行 booti/bootm 等命令，将控制权移交给Linux内核 | +——–+———+ | +———+———-+ | 可选中间阶段（平台相关，如TF-A或OpenSBI） | | | 解释： 这些阶段通常在U-Boot之前或与U-Boot并行工作，而非严格地在U-Boot之后。 | | 它们管理CPU的权限级别和安全状态，在U-Boot或Linux内核之前加载并运行。 | | 当U-Boot或Linux内核需要执行安全操作时，会通过EL3的Secure Monitor（ATF）或 | | M-mode的SBI（OpenSBI）来完成。 | | | ARM: TF-A (Trusted Firmware-A) | -&gt; BL31 (EL3 Secure Monitor) 负责管理安全世界和非安全世界， | (包含BL31, BL32) | 提供PSCI (Power State Coordination Interface)等服务。 | | -&gt; BL32 (EL1 Secure World) 可选，用于运行TEE OS (如OP-TEE)。 | | -&gt; BL33 通常是U-Boot。如果某些配置中，ATF直接加载Linux内核， | | 则BL33可以指向裸格式的Linux内核镜像。 | | | RISC-V: OpenSBI (M-mode) | -&gt; 提供标准的SBI (Supervisor Binary Interface)，允许操作系统在S-mode | (Machine Mode) | 调用底层硬件功能（如中断处理、定时器、系统关机等）。 | | -&gt; fw_payload.elf 通常是OpenSBI和Linux内核的组合，由SPL直接加载并执行。 +———+———-+ | v +——————+ | Linux Kernel | &lt;– 操作系统的核心部分 | start_kernel() | | -&gt; 初始化更多硬件 | (例如：驱动程序、文件系统等) | -&gt; 初始化内核子系统 | (例如：进程管理、内存管理、调度器等) | -&gt; 挂载根文件系统 | | -&gt; 启动 init 进程 | (用户空间第一个进程) +——–+———+ | v +——————+ | /sbin/init | &lt;– 用户空间起点，开始加载用户程序和服务 | 用户空间起点 | +——————+ 阶段 权限级别 世界 作用 举例 BL1 EL3 Secure ROM引导 (Primary Bootloader) 出厂固化在 SoC 中（如STM32MP157的BootROM），其任务是从外部存储（SPI NOR / eMMC / SD）中寻找并加载下一阶段的BL2到内部SRAM执行。 BL2 EL3 Secure 固件加载器 (Secondary Bootloader) 由BL1加载并执行，其主要任务是初始化一些必要的硬件，并加载后续的固件（如BL31和BL33）到DRAM，然后将控制权交给它们。STM32MP15 使用 TF-A 的 bl2.bin。 BL31 EL3 Secure 安全运行时固件 (Secure Monitor) 运行在EL3，提供Secure Monitor Call (SMC) 接口，响应来自非安全世界的请求（如系统电源管理PSCI、核间通信、切换安全/非安全模式等）。它是管理安全与非安全世界转换的核心。 BL32 Secure World (通常为EL1或EL0) Secure 可选的安全执行环境 (TEE) 运行在Secure World，通常是一个独立的“迷你操作系统”（如OP-TEE），用于处理敏感操作，如加密、密钥管理、指纹识别等，与主操作系统（非安全世界）隔离。这是一个可选的阶段。 BL33 EL2/EL1 (或更低) Non-secure 最终跳转目标 通常是主操作系统的前导加载器（如U-Boot）或直接就是Linux内核。它运行在非安全世界，负责非安全侧的硬件初始化和加载Linux内核。EL2是管理程序，EL1是操作系统内核。 2. 设备树 (Device Tree - DT) 设备树是一种用于描述硬件信息的数据结构，它以文本形式（DTS）描述了系统中的CPU、内存、总线、各种外设及其连接关系、中断、时钟等所有硬件细节。编译后生成二进制文件（DTB），供引导程序（Bootloader）和内核使用。 .dts/.dtsi -&gt; dtb： 使用 dtc (Device Tree Compiler) 编译器将设备树源文件 (.dts) 及其包含的片段文件 (.dtsi) 编译成二进制的设备树文件 (.dtb)。这个编译过程是架构无关的。 *通过of_系列API读取信息： Linux内核通过一套of_*（Open Firmware）API来解析.dtb文件，并在运行时获取各种硬件信息，供内核子系统和驱动程序使用。 设备树语言本身是架构无关的： 无论是ARM、RISC-V、PowerPC、MIPS等任何CPU架构，都可以使用相同的DTS语法来描述硬件。dtc编译器是通用的。 设备树内容与CPU架构高度相关： 虽然语法通用，但设备树中描述的具体硬件信息（如CPU节点下的ISA扩展、特权模式；中断控制器如RISC-V的PLIC/CLINT；内存映射；外设的寄存器地址等）是与目标CPU架构和SoC设计紧密绑定的。 1. 设备树结构一个典型的设备树文件以根节点 / 开始，并包含多个子节点，每个节点代表一个硬件设备或逻辑单元。属性则用于描述设备的具体信息。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160/dts-v1/; // 声明设备树语言版本，固定写法// 包含文件：通常用于导入SoC厂商提供的芯片级通用设备树定义// 例如：#include &lt;dt-bindings/interrupt-controller/irq.h&gt; // 包含中断控制器宏定义// #include &quot;riscv_vendor_soc.dtsi&quot; // 假设SoC厂商提供芯片级通用DTSi// #include &quot;riscv_vendor_board_common.dtsi&quot; // 假设板卡厂商提供公共DTSi/ { // 根节点，必须有且唯一，代表整个系统 compatible = &quot;your_vendor,your_board_model&quot;, &quot;vendor,soc_model&quot;, &quot;generic-platform&quot;; // 核心属性，用于板级驱动匹配。 // 多个值时按顺序匹配，越具体越靠前。 model = &quot;Your Specific Board Name and Model&quot;; // 可读的板子型号名称 interrupt-parent = &lt;&amp;plic&gt;; // 根节点或总线节点通常会指定其默认的中断控制器 // 定义当前节点及其子节点`reg`属性中地址和长度单元的长度。 // &lt;1&gt; 表示一个32位单元；&lt;2&gt; 表示一个64位单元。 #address-cells = &lt;1&gt;; // 用于`reg`属性中物理地址的单元数量 #size-cells = &lt;1&gt;; // 用于`reg`属性中内存区域大小的单元数量 aliases { // 别名节点，为常用设备节点提供短名称，方便访问和引用（例如：/soc/serial@... 可以别名为 serial0） serial0 = &amp;uart0; spi0 = &amp;spi0_controller; ethernet0 = &amp;ethernet_mac; mmc0 = &amp;sd_controller; // ... }; chosen { // 用于引导程序（Bootloader）和操作系统内核之间传递参数的节点 stdout-path = &quot;serial0:115200&quot;; // 指定内核启动时的控制台输出设备（引用alias）和波特率 bootargs = &quot;console=ttyS0,115200 earlycon root=/dev/mmcblk0p2 rootwait ro&quot;; // 传递给内核的启动参数 // linux,initrd-start = &lt;0x...&gt;; // 可选：指定Initrd的物理内存起始地址 // linux,initrd-end = &lt;0x...&gt;; // 可选：指定Initrd的物理内存结束地址 }; memory@80000000 { // 物理内存区域节点，节点名应包含起始地址 device_type = &quot;memory&quot;; // 节点类型，固定为&quot;memory&quot;，表示这是一个内存区域 reg = &lt;0x80000000 0x40000000&gt;; // 物理起始地址和大小 (例如：从0x80000000开始，大小1GB) // 必须与板载DRAM的实际配置相符 }; cpus { // 定义系统中所有 CPU 核的数量、拓扑、启动方式等 #address-cells = &lt;1&gt;; // CPU ID的长度（通常是hart ID） #size-cells = &lt;0&gt;; // CPU节点通常没有大小信息 cpu0: cpu@0 { // 每个CPU核一个节点，节点名通常包含其ID compatible = &quot;riscv&quot;; // RISC-V CPU的通用兼容性 device_type = &quot;cpu&quot;; reg = &lt;0&gt;; // hart id (硬件线程ID) clock-frequency = &lt;1000000000&gt;; // CPU主频，单位Hz (例如1GHz) riscv,isa = &quot;rv64imafdc&quot;; // RISC-V指令集扩展（例如：64位、整数、乘除、原子、浮点、双精度浮点、压缩指令） riscv,priv-modes = &lt;0x3&gt;; // 支持的特权模式 (M-mode, S-mode) // interrupt-controller; // 如果该CPU内部有核心本地中断控制器(CLINT)，可以标记 // clock-names = &quot;cpu_clk&quot;; // 如果CPU有特定的时钟名称 // clocks = &lt;&amp;bus_clk&gt;; // 引用CPU的时钟源 // d-cache-size = &lt;...&gt;; // L1 D-Cache大小 // i-cache-size = &lt;...&gt;; // L1 I-Cache大小 // l2-cache = &lt;&amp;l2_cache&gt;; // 引用L2 Cache节点 }; // 如果有多个CPU核（如多核处理器），依此类推：cpu1: cpu@1 { ... }; }; soc { // 通常代表SoC内部的总线或集成外设，作为其他外设的父节点 compatible = &quot;your_vendor,your_soc_model&quot;, &quot;simple-bus&quot;; // SoC总线节点兼容性，&quot;simple-bus&quot;是通用总线 #address-cells = &lt;1&gt;; #size-cells = &lt;1&gt;; ranges; // 用于地址映射。空值表示子节点地址空间与父节点相同，无需映射。 // 如果需要地址转换，例如 `ranges = &lt;0x0 0x80000000 0x10000000&gt;;` // 解释：子节点地址0x0映射到父节点地址0x80000000，长度0x10000000。 // **RISC-V特定中断控制器节点** plic: interrupt-controller@c000000 { // Platform-Level Interrupt Controller (PLIC) compatible = &quot;riscv,plic0&quot;; // 匹配PLIC驱动 reg = &lt;0xc000000 0x4000000&gt;; // PLIC寄存器地址范围 interrupt-controller; // 标记这是一个中断控制器 #interrupt-cells = &lt;3&gt;; // PLIC需要3个单元：中断类型(1)、中断ID(2)、中断标志(3) riscv,max-harts = &lt;1&gt;; // 支持的最大hart数量 riscv,ndev = &lt;64&gt;; // 支持的最大设备中断数量 // 中断类型通常为0表示IRQ，1表示软件中断。 // 中断标志通常表示中断的触发方式（电平触发/边缘触发、高/低电平）。 }; clint: timer@2000000 { // Core Local Interruptor (CLINT) - 提供定时器和处理器间中断 compatible = &quot;riscv,clint0&quot;; reg = &lt;0x2000000 0x10000&gt;; // CLINT寄存器地址范围 // CLINT通常作为CPU的中断源，不需要#interrupt-cells。 }; // **UART控制器节点** uart0: serial@10000000 { // 节点名称和寄存器物理地址 compatible = &quot;ns16550a&quot;, &quot;riscv,uart0&quot;; // 通用UART驱动匹配字符串，或SoC特定UART兼容字符串 reg = &lt;0x10000000 0x100&gt;; // 寄存器基地址和大小 interrupts = &lt;0x4&gt;; // 中断号 (具体值取决于中断控制器，通常是中断请求线ID) interrupt-parent = &lt;&amp;plic&gt;; // 明确指定中断来源（非必须，但推荐） clocks = &lt;&amp;clk_controller 0&gt;; // 引用时钟源（假设clk_controller的第一个时钟） status = &quot;okay&quot;; // 启用设备 }; // **SPI控制器节点** spi0_controller: spi@10001000 { compatible = &quot;spi-controller-compatible&quot;; // 匹配SPI主控驱动 (例如 &quot;vendor,spi-v1&quot;) reg = &lt;0x10001000 0x100&gt;; interrupts = &lt;0x5&gt;; interrupt-parent = &lt;&amp;plic&gt;; #address-cells = &lt;1&gt;; // 定义子设备（SPI从设备）的片选线号 #size-cells = &lt;0&gt;; // SPI从设备没有大小信息 status = &quot;okay&quot;; // SPI 从设备节点（例如一个SPI Nor Flash芯片） flash@0 { // 节点名通常包含片选号 compatible = &quot;jedec,spi-nor&quot;; // 匹配通用SPI Nor Flash驱动 reg = &lt;0&gt;; // 片选线号 spi-max-frequency = &lt;50000000&gt;; // 最大SPI频率 }; }; // **GPIO控制器节点** gpio0: gpio@10002000 { compatible = &quot;gpio-controller-compatible&quot;; // 匹配GPIO控制器驱动 (例如 &quot;vendor,gpio-v1&quot;) reg = &lt;0x10002000 0x100&gt;; gpio-cells = &lt;2&gt;; // 每个GPIO PIN在引用时需要2个单元：PIN号和标志（GPIO_ACTIVE_HIGH/LOW等） interrupt-controller; // 如果GPIO控制器也能产生中断 #interrupt-cells = &lt;2&gt;; // 如果作为中断控制器，需要2个单元：GPIO中断号和标志 status = &quot;okay&quot;; }; // **外部连接的LEDs (通过GPIO控制)** leds { compatible = &quot;gpio-leds&quot;; // 匹配GPIO LED驱动 led0: led@0 { // 节点名和LED索引 label = &quot;system-led&quot;; // 用户友好的标签 gpios = &lt;&amp;gpio0 0 GPIO_ACTIVE_HIGH&gt;; // 引用gpio0控制器，使用GPIO 0，高电平有效 default-state = &quot;off&quot;; // 默认开机状态 (&quot;on&quot;, &quot;off&quot;, &quot;keep&quot;, &quot;blinking&quot;) }; led1: led@1 { label = &quot;user-led&quot;; gpios = &lt;&amp;gpio0 1 GPIO_ACTIVE_LOW&gt;; // 使用GPIO 1，低电平有效 }; }; // **其他外设...** (如I2C控制器、以太网MAC、MMC/SD控制器、USB控制器、PWM控制器、时钟控制器等) // clocks: clock-controller@... { // 时钟控制器节点 // compatible = &quot;vendor,clock-controller&quot;; // reg = &lt;...&gt;; // #clock-cells = &lt;1&gt;; // 如果有时钟ID，需要一个单元 // // ... // }; // pinctrl@... { // 引脚控制器节点，用于配置引脚复用和电气特性 // compatible = &quot;vendor,pinctrl&quot;; // reg = &lt;...&gt;; // pinctrl-names = &quot;default&quot;; // 默认的引脚配置组名称 // pinctrl-0 = &lt;&amp;pinmux_group_uart0&gt;; // 引用具体的引脚配置组 // // pinmux_group_uart0: pinmux-group-uart0 { // 定义引脚配置组 // pins = &lt;0 1&gt;; // 具体的引脚编号 // // 其他引脚配置，如驱动强度、上拉/下拉等 // }; // }; };}; 2. 常用节点/属性 节点/属性 用途 (补充与修正) 举例 compatible 驱动匹配关键： 内核和驱动中通过这个字段进行匹配，实现驱动的硬件无关性。多个值时按顺序匹配，越具体越靠前。 &quot;vendor,device-v1&quot;, &quot;generic-device&quot; reg 硬件寄存器地址范围： 描述设备的物理地址和大小。格式通常为 &lt;起始地址 长度&gt;，单位数量由父节点的#address-cells和#size-cells决定。 &lt;0x10000000 0x100&gt; (32位地址，长度256字节) interrupts 中断号、触发方式等： 驱动用来注册中断处理程序。具体含义和单元数量由父中断控制器节点的#interrupt-cells决定。 &lt;0x4&gt; (单个中断ID)；&lt;0 4 IRQ_TYPE_EDGE_RISING&gt; (RISC-V PLIC中，指定中断类型、ID、标志) interrupt-parent 指定中断控制器： 明确该设备的中断连接到哪个中断控制器（节点引用）。如果未指定，默认继承父节点。 &lt;&amp;plic&gt; #address-cells 定义子节点地址单元长度： 决定其子节点reg属性中“地址”部分有多少个32位单元。 &lt;1&gt; (32位地址)；&lt;2&gt; (64位地址) #size-cells 定义子节点长度单元长度： 决定其子节点reg属性中“大小”部分有多少个32位单元。 &lt;1&gt; (32位长度)；&lt;2&gt; (64位长度) #interrupt-cells 定义中断单元长度： 标记该节点是一个中断控制器，并决定其子设备interrupts属性中中断描述符的单元数量。 &lt;1&gt; (简单中断ID)；&lt;3&gt; (RISC-V PLIC) #gpio-cells 定义GPIO单元长度： 标记该节点是一个GPIO控制器，并决定其子设备gpios属性中GPIO描述符的单元数量。 &lt;2&gt; (GPIO号和标志) #clock-cells 定义时钟单元长度： 标记该节点是一个时钟控制器，并决定其子设备clocks属性中时钟描述符的单元数量（通常是时钟ID）。 &lt;1&gt; clocks 时钟源引用： 引用设备所需的一个或多个时钟源（需要配合clk framework使用）。 &lt;&amp;clk_controller 0&gt; clock-names 时钟源名称： 为clocks属性中引用的时钟源提供名称，当设备需要多个不同功能的时钟时使用。 &quot;bus&quot;, &quot;peripheral&quot; pinctrl-names 引脚配置组名称： 定义引脚配置的状态名称，例如“default”, “sleep”, “idle”等。 &quot;default&quot; pinctrl-0, pinctrl-1 引脚配置组引用： 引用具体的引脚配置组（由pinctrl节点中的子节点定义）。pinctrl-0通常是默认配置。 &lt;&amp;pinmux_group_uart0&gt; status 设备启用状态： &quot;okay&quot;表示启用设备；&quot;disabled&quot;表示禁用设备；&quot;reserved&quot;表示预留。 &quot;okay&quot; linux,phandle 内核内部引用句柄： 由dtc编译器在编译时自动生成，用于内核内部节点之间的引用，不应手动设置。 (自动生成) ranges 地址映射： 定义父子总线之间的地址转换规则。空值表示子节点地址空间与父节点相同，无需映射。 ranges; 或 &lt;0x0 0x80000000 0x10000000&gt; device_type 设备类型： 特定节点（如memory和cpu）的固定属性，用于标识其类型。 &quot;memory&quot;, &quot;cpu&quot; model 可读型号： 设备的可读型号描述，通常用于根节点和SoC节点。 &quot;My Awesome RISC-V Board&quot; label 用户友好标签： 为设备提供一个可读的字符串标签，方便在用户空间或调试时识别。 &quot;system-led&quot; gpios GPIO引用： 引用一个或多个GPIO引脚。格式取决于#gpio-cells，通常为 &lt;&amp;gpio_controller gpio_pin_number gpio_flags&gt;。 &lt;&amp;gpio0 0 GPIO_ACTIVE_HIGH&gt; default-state GPIO LED默认状态： 对于gpio-leds，定义LED的默认状态。 &quot;on&quot;, &quot;off&quot;, &quot;blinking&quot;, &quot;keep&quot; riscv,isa RISC-V ISA扩展： 对于CPU节点，描述RISC-V指令集架构的扩展集合。 &quot;rv64imafdc&quot; riscv,priv-modes RISC-V特权模式： 对于CPU节点，描述CPU支持的RISC-V特权模式（M-mode, S-mode, U-mode）。 &lt;0x3&gt; (表示M-mode和S-mode) clock-frequency 设备工作频率： 通常用于CPU节点，描述其工作频率。 &lt;1000000000&gt; (1GHz) reserved-memory 预留内存区域： 定义内核不应使用的内存段（例如，用于GPU、VPU、DSP或安全区域等）。 (一个单独的节点，内部定义子节点描述各预留区域) dma-names, dmas DMA通道引用： 引用DMA控制器及其通道。 &lt;&amp;dma_controller 0&gt; 3. 拿到一个RISC-V开发板，如何写设备树？1. 阶段一：信息收集与环境准备 (地基) 1.1 硬件文档是金： SoC 数据手册/参考手册： 必读！获取所有内部外设（CPU、PLCI、CLINT、UART、SPI、I2C、GPIO等）的寄存器基地址、中断号、时钟源信息、引脚复用配置。 开发板原理图 (Schematic)： 了解板载DRAM容量、外部Flash型号、以太网PHY、LEDs、Buttons、传感器等外部设备如何连接到SoC的特定引脚和接口。 已有参考： 查找SoC厂商或开发板厂商提供的参考设备树文件 (.dtsi/.dts)。这是最快的起点，可以避免从零开始。 Bootloader 源码： 查阅U-Boot或其它Bootloader中对硬件的初始化代码，通常能反推出设备树的结构和属性。 1.2 Linux内核文档： Linux内核源码/Documentation/devicetree/bindings/：核心参考！ 这个目录包含了各种标准设备（如GPIO控制器、UART、SPI控制器、PHY、LED等）的绑定规范。它会告诉你某个compatible字符串对应的节点应该有哪些属性，以及这些属性的含义、类型和格式。严格遵循这些绑定规范是确保驱动正常工作的关键。 1.3 工具： 确保dtc编译器已安装并可用。 2. 阶段二：构建基础设备树框架 (骨架) 2.1 创建顶层 .dts 文件： 通常以板子的名称命名，例如 your_riscv_board.dts。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/dts-v1/;#include &quot;riscv_vendor_soc.dtsi&quot; // 引入SoC芯片级的通用定义#include &quot;riscv_vendor_board_common.dtsi&quot; // 引入板级公共定义（如果有）/ { // 根节点定义 compatible = &quot;your_vendor,your_board_model&quot;, &quot;vendor,soc_model&quot;, &quot;simple-bus&quot;; model = &quot;My Custom RISC-V Development Board&quot;; #address-cells = &lt;1&gt;; // 假设地址是32位 #size-cells = &lt;1&gt;; // 假设大小是32位 // 内存节点：根据原理图填写DRAM的实际物理地址和大小 memory@80000000 { device_type = &quot;memory&quot;; reg = &lt;0x80000000 0x40000000&gt;; // 例如：从0x80000000开始，1GB }; // chosen 节点：最关键的调试信息，确保console可用 chosen { stdout-path = &amp;uart0; // 假设uart0是控制台 bootargs = &quot;console=ttyS0,115200 root=/dev/mmcblk0p2 rootwait earlycon&quot;; }; aliases { serial0 = &amp;uart0; // 方便chosen节点引用 // ... }; // CPU节点：通常在SoC的dtsi中定义，此处可覆盖或添加属性 &amp;cpu0 { clock-frequency = &lt;1000000000&gt;; // 实际的CPU频率 // ... }; // SoC内部外设节点：通过引用dtsi中已有的节点并添加/修改板级特定配置 // 例如，启用串口0 &amp;uart0 { status = &quot;okay&quot;; // 确保启用 // 如果与dtsi中定义不同，可覆盖reg, interrupts, clocks等 }; // PLIC/CLINT等中断控制器节点也应在dtsi中被引用和确保启用 &amp;plic { status = &quot;okay&quot;; }; &amp;clint { status = &quot;okay&quot;; }; // ...其他必要的SoC内部模块}; 3. 阶段三：逐步添加外设与调试 (填充血肉) 3.1 编译DTB：1dtc -I dts -O dtb -o your_riscv_board.dtb your_riscv_board.dts 错误处理： dtc会报告语法错误。确保没有警告或错误。 3.2 集成与启动： 将编译好的 your_riscv_board.dtb 文件放置到Bootloader（如U-Boot）能够访问的位置（例如SD卡、TFTP服务器）。 在U-Boot命令行中，确保正确加载了DTB并将其地址传递给Linux内核：1234fatload mmc 0:1 0x80200000 Image // 加载内核镜像fatload mmc 0:1 0x82000000 your_riscv_board.dtb // 加载设备树setenv bootargs &quot;console=ttyS0,115000 root=/dev/mmcblk0p2 rootwait&quot; // 设置启动参数booti 0x80200000 - 0x82000000 // 启动内核 (RISC-V booti 命令) 3.3 验证与调试（迭代过程）： 串口输出： 内核启动时会打印大量日志，这是验证设备树最直接的方式。关注与你添加的设备相关的行。 dmesg： 内核启动后，使用 dmesg 命令查看完整的内核日志。搜索你设备的compatible字符串，或者设备名称，看它是否被识别和初始化。 /sys/firmware/devicetree/base/： 这是设备树在Linux运行时的表现。 ls -R /sys/firmware/devicetree/base/：查看整个设备树的目录结构。 cat /sys/firmware/devicetree/base/&lt;node_path&gt;/&lt;property_name&gt;：查看特定节点的属性值，验证是否与你DTS中定义的一致。 /proc/interrupts： 检查中断是否正确注册。 /sys/kernel/debug/gpio (需启用debugfs)： 检查GPIO引脚的状态。 lsmod / modprobe： 尝试手动加载/卸载相关驱动模块，观察内核日志。 逐步排查： 如果设备未被识别： 检查compatible字符串是否与驱动匹配、status = &quot;okay&quot;是否设置、节点路径是否正确。 如果设备被识别但功能异常： 检查reg地址和大小是否正确、interrupts中断号和标志是否正确、clocks是否引用了正确的时钟源且已启用、gpios引用是否准确。 最常见问题： 地址、中断、时钟和GPIO的配置错误。 查阅绑定文档： 再次强调，遇到问题时，Documentation/devicetree/bindings/目录下的官方绑定文档是解决问题的关键。它详细说明了每个属性的语义和期望值。 4. 阶段四：高级配置与完善 (优化) 引脚复用 (Pinmux)： 根据SoC的pinctrl驱动和原理图，添加pinctrl节点及其子节点，定义不同的引脚复用组，并在设备节点中通过pinctrl-names和pinctrl-0等属性引用。这是确保引脚功能正确的关键。 电源管理与时钟门控： 添加power-domains、clocks、clock-names等属性，配合内核的电源管理和时钟框架，实现设备的低功耗和性能调优。 DMA配置： 为需要DMA（Direct Memory Access）的设备（如USB、Ethernet）添加dmas和dma-names属性，引用DMA控制器和通道。 复杂总线： 对于I2C、SPI等总线，如果连接了多个从设备，需要在其控制器节点下添加子节点。 错误处理： 如果某个设备无法正常工作，应先排除设备树配置问题，再考虑驱动或硬件问题。 123456789101112131415/dts-v1/;/ { // 定义板子整体的模型、兼容性 compatible = &quot;xxx&quot;; model = &quot;xxx board&quot;; memory { ... }; // 定义物理内存区域 cpus { ... }; // 定义所有 CPU 核的数量、拓扑、启动方式等 chosen { ... }; // 内核启动参数、console、initrd 地址等 soc { // 芯片内部挂载的所有外设（UART、SPI 等） serial@xxxx { ... }; i2c@xxxx { ... }; spi@xxxx { ... }; };}; 节点/属性 用途 compatible 驱动匹配关键：驱动中会通过这个字段绑定 reg 硬件寄存器地址范围（告诉驱动怎么访问） interrupts 中断号、触发方式等（驱动用来注册 handler） clocks 时钟源、速率（需要配合 clk framework 使用） pinctrl-0 使用哪组 pinmux 设置 status = &quot;okay&quot; 是否启用设备 linux,phandle 内核内部处理引用的句柄 &amp;aliases 给节点起别名（影响路径） reserved-memory 定义内核不能用的内存段（比如给 VPU、TPU） 3. platform_device / platform_driver / platform_bus 设备（platform_device）和驱动（platform_driver）都注册到了 platform_bus 上。 当它们都出现时，调用总线的 match() 方法： 比较设备名 vs 驱动名（pdev-&gt;name == pdrv-&gt;name） 匹配成功 → 调用 driver.probe()，开始驱动设备。 每个设备或驱动都会挂到一条总线上，platform_device/driver是挂到了虚拟的platform_bus上，当有驱动模块插入或者设备创建的时候，就会去调用bus-&gt;match，如果匹配就会调用到probe LED 驱动的简化写法 设备树片段： 1234led@40000000 { compatible = &quot;goko,led&quot;; reg = &lt;0x40000000 0x1000&gt;;}; 驱动代码： 12345678910111213141516171819202122static int led_probe(struct platform_device *pdev){ struct resource *res = platform_get_resource(pdev, IORESOURCE_MEM, 0); void __iomem *base = devm_ioremap_resource(&amp;pdev-&gt;dev, res); // 注册中断/初始化硬件... return 0;}static const struct of_device_id led_of_match[] = { { .compatible = &quot;goko,led&quot;, }, {},};static struct platform_driver led_driver = { .probe = led_probe, .driver = { .name = &quot;goko_led&quot;, .of_match_table = led_of_match, },};module_platform_driver(led_driver); 4. 中断处理流程(从硬件中断到ISR) 中断产生 响应中断 保存当前上下文 调转到中断入口（中断向亮表） 进入trap流程，riscv是stvec GIC/PLIC识别中断来源 linux调用handle_irq 最终调用驱动注册的irq 结合RISC-V的架构特性（如stvec、scause、PLIC中断控制器）和Linux内核的通用中断子系统来描述。 核心概念： 中断 (Interrupt): 外部设备或定时器发出的异步信号，请求CPU的注意和处理。 异常 (Exception): CPU执行指令时遇到的同步事件（如缺页、非法指令、除以零）。 Trap: RISC-V中用于统称中断和异常的术语。CPU遇到Trap时会暂停当前执行，跳转到特定的处理入口。 PLIC (Platform-Level Interrupt Controller): RISC-V系统中常见的外部中断控制器，负责接收来自各个设备的物理中断信号，进行优先级排序、屏蔽，并将最高优先级的中断信号送往CPU。 ISR (Interrupt Service Routine): 驱动程序中注册的函数，用于处理特定硬件设备产生的中断。通常分为”上半部”（中断处理函数本身，要求快、原子）和”下半部”（延迟处理，如Tasklet、Workqueue）。 中断处理详细流程： 硬件中断产生 (Hardware Interrupt Generation) 某个外部硬件设备（如网卡收到数据、磁盘完成读写、定时器到期）需要CPU处理，它会断言（拉高或改变状态）其连接到中断控制器的物理中断线。例如：网卡收到数据后，其内部控制器发出一个中断请求信号到连接它的PLIC引脚。 中断控制器 (PLIC) 接收并仲裁 (PLIC Receives and Arbitrates) PLIC接收到来自一个或多个设备的物理中断信号。它会根据预设的优先级和使能状态，选择当前最高优先级的、已使能的中断。 PLIC 动作: 接收来自设备的物理中断。 查找该中断源的优先级和使能状态（通过读写PLIC的寄存器配置）。 如果该中断已使能且优先级高于当前CPU正在处理的或PLCT声明（Claim）的优先级，PLIC会向连接的CPU核心发送一个中断信号（通常是断言CPU的中断输入线，对于Supervisor模式，是Supervisor External Interrupt，通过scause体现为某个特定的值）。 该信号最终会反映在CPU的sip (Supervisor Interrupt Pending) 寄存器的对应位上。对于外部中断，是sip.SEIP位。 CPU检测到中断并响应 (CPU Detects and Responds to Interrupt) CPU在执行完当前指令后，会检查中断是否发生以及是否被当前CPU模式（这里是Supervisor模式，S-mode）所屏蔽。 检查条件: 全局中断使能：sstatus寄存器的SIE位必须为1。 中断委托：该中断源必须被委托给S模式处理（sedeleg寄存器中对应位为1）。PLIC外部中断通常是委托给S模式的。 中断挂起：sip寄存器中对应中断的挂起位必须为1 (sip.SEIP)。 优先级：如果RISC-V实现了中断优先级，挂起中断的优先级必须高于当前CPU的优先级阈值。 如果满足以上条件，CPU会暂停当前程序执行。这是一个硬件自动完成的过程。 硬件保存部分上下文 (Hardware Saves Partial Context) 描述: 在跳转到Trap处理入口之前，RISC-V CPU硬件会自动保存当前执行状态的关键信息，并将CPU模式切换到Supervisor模式（如果之前不是）。 保存内容: sepc (Supervisor Exception Program Counter): 保存发生中断时下一条待执行指令的地址。 scause (Supervisor Cause): 保存Trap发生的原因。对于S模式外部中断，scause的高位为1，低位部分指示是外部中断（一个特定的值，如8或9，取决于是否使用中断向量）。 sstatus (Supervisor Status): sstatus的部分位被修改： 当前的SIE (Supervisor Interrupt Enable) 状态被保存到sstatus.SPIE (Supervisor Previous Interrupt Enable)。 当前的CPU模式（如果之前是U模式）被保存到sstatus.SPP (Supervisor Previous Privilege)。 SIE位被清零，禁用中断，防止嵌套中断（直到软件重新使能）。 当前模式设置为S模式。 相关概念: sepc, scause, sstatus, sstatus.SPIE, sstatus.SPP。 CPU跳转到Trap入口 (stvec) (CPU Jumps to Trap Entry (stvec)) 描述: CPU根据stvec (Supervisor Trap Vector) 寄存器的值来确定Trap处理程序的入口地址。 stvec 模式: Direct Mode (Mode = 0): 所有Trap（包括所有中断和异常）都跳转到stvec指定的同一个地址。Linux通常配置为这种模式。 Vectored Mode (Mode = 1): 中断会根据scause中的中断号偏移到stvec基地址 + (中断号 * 4) 的位置（假设每个向量是4字节指令）。异常仍然跳转到stvec基地址。 CPU 动作: CPU加载stvec的值到PC寄存器，开始执行Trap处理程序的代码。 相关概念: stvec, Direct Mode, Vectored Mode。 进入内核Trap入口处理程序 (Enter Kernel Trap Entry Handler) 描述: 这是内核中的第一个代码段（通常是汇编语言）被执行的地方。它负责保存剩余的CPU上下文，并调用更高层的C语言中断处理函数。 汇编处理程序动作 (Conceptual Assembly - e.g., kernel_trap): 保存所有通用寄存器（a0-a7, t0-t6, s0-s11等）到当前任务的内核栈中，形成一个完整的上下文结构（在Linux中通常是struct pt_regs）。 将scause和sepc等CSR寄存器的值保存到pt_regs结构中。 根据需要调整栈指针。 调用更高层的C语言Trap处理函数，例如 handle_exception 或 handle_interrupt_common，将pt_regs结构体的地址作为参数传递。 为什么需要汇编? C语言函数调用会使用寄存器和栈，汇编程序负责在调用C函数之前保存所有可能被C函数修改的寄存器，并在C函数返回后恢复它们。 相关代码 (Conceptual):123456789101112131415161718192021.globl kernel_trapkernel_trap: # Save all general purpose registers onto the stack SAVE_ALL # Macro that pushes registers x1-x31 # Save CSRs like scause, sepc, sstatus into the pt_regs structure on stack # ... logic to read CSRs and store into pt_regs ... # Load stack pointer into a0 (first arg for C function) mv a0, sp # Call the generic C handler call handle_exception # Or handle_interrupt_common # Restore all registers from the stack RESTORE_ALL # Macro that pops registers x1-x31 # Restore scause, sepc, sstatus from pt_regs if needed (e.g. for exception return) # Return from trap sret 相关概念: struct pt_regs, 保存/恢复寄存器宏 (SAVE_ALL, RESTORE_ALL), 内核栈。 高层C语言Trap处理 (High-Level C Trap Handling) 描述: handle_exception (或类似函数) 是第一个被调用的C函数。它检查scause来确定Trap的类型（中断还是异常），以及具体的原因。 处理逻辑: 读取scause寄存器的值。 检查scause的最高位：如果为1，表示是中断；如果为0，表示是异常。 如果是中断: 进一步检查scause的低位，判断是哪种类型的中断（例如，S模式软件中断、S模式定时器中断、S模式外部中断）。 如果是S模式外部中断（通常通过PLIC到达），则调用外部中断的处理逻辑，例如 handle_interrupt_common 或体系结构相关的中断分发函数。 如果是异常: 根据scause的低位判断异常类型（如缺页、非法指令、访问错误等）。 调用相应的异常处理函数（如do_page_fault, do_illegal_instruction等）。 相关函数 (Conceptual): handle_exception, handle_interrupt_common。 Linux通用中断分发 (Linux Generic Interrupt Dispatch) 描述: 这是Linux中断子系统的核心部分。对于外部中断，这里需要与中断控制器(PLIC)交互，确定是哪个设备产生了中断，并将中断请求路由到正确的Linux IRQ描述符和处理函数。 处理步骤: 与PLIC交互 (Claim): 读取PLIC的Claim寄存器。PLIC返回当前CPU核心上最高优先级的、已使能的挂起外部中断源的ID。这是硬件设备的中断源ID。 查找Linux IRQ号: 使用中断域 (irq_domain) 子系统，将PLIC返回的硬件中断源ID映射到Linux内核内部使用的虚拟IRQ号。这是Linux抽象设备中断的方式。 查找 irq_desc: 使用Linux IRQ号作为索引，查找对应的 struct irq_desc 结构体。这个结构体包含中断的状态、统计信息、以及指向中断流处理函数(handle_irq)和注册的中断动作链表 (irq_action) 的指针。 调用中断流处理函数: 调用 irq_desc 中指向的流处理函数（如 handle_level_irq, handle_edge_irq）。这些函数负责处理特定类型中断的低级细节，例如： 在此处与PLIC交互 (Completion): 在适当的时机（通常对于电平触发在调用ISR前，对于边缘触发在ISR后），向PLIC的Completion寄存器写入刚刚处理的PLIC源ID，告诉PLIC该中断已由当前CPU处理。这允许PLIC为同一源生成新的中断（如果是电平触发），或者清除挂起状态。 屏蔽/解除屏蔽该IRQ线在PLIC中的中断。 更新中断统计信息。 调用下一层的动作处理函数。 调用动作处理函数: 流处理函数内部会调用 handle_irq_event_percpu 或类似函数。这个函数负责遍历挂在该IRQ描述符上的所有 struct irq_action 结构体。 相关概念/函数: PLIC Claim/Completion Registers, irq_domain, Linux IRQ Number, struct irq_desc, handle_irq (指针在 irq_desc 中), handle_level_irq, handle_edge_irq, handle_irq_event_percpu, struct irq_action. 相关代码 (Conceptual):12345678910111213141516// Inside a function called by handle_interrupt_common for external interruptsu32 plic_id = plic_claim(cpu); // Read PLIC claim registerif (plic_id) { // Map PLIC ID to Linux IRQ number unsigned int irq = irq_find_mapping(irq_domain, plic_id); if (irq) { struct irq_desc *desc = irq_to_desc(irq); if (desc &amp;&amp; desc-&gt;handle_irq) { // Call the flow handler (e.g., handle_level_irq) desc-&gt;handle_irq(irq, desc); // This will eventually call the action handler } } // plic_complete(cpu, plic_id); // This is usually done *inside* the flow handler (e.g. handle_level_irq)} 执行驱动注册的ISR (Execute Driver’s Registered ISR) 描述: handle_irq_event_percpu 遍历 irq_desc 中的 action 链表，逐一调用驱动程序通过 request_irq 函数注册的中断处理函数（ISR）。 request_irq 函数: 驱动程序使用此函数向内核注册其中断处理函数。12345678910111213141516171819202122232425262728// Driver code examplestatic irqreturn_t my_device_isr(int irq, void *data){ // data is often a pointer to the device structure struct my_device *dev = (struct my_device *)data; // --- Top Half (ISR) --- // Must be FAST and ATOMIC (cannot sleep, acquire mutexes, etc.) // Acknowledge the device hardware (clear interrupt status bits on the device) // Read necessary status/data from device (quickly) // Schedule bottom half work if needed (tasklet_schedule, schedule_work) // --- End Top Half --- // Return status: // IRQ_HANDLED: This handler processed the interrupt. // IRQ_NONE: This handler did not recognize/handle the interrupt (e.g., shared interrupt line). return IRQ_HANDLED;}// In driver initialization code:int ret = request_irq(device_irq_number, // The Linux IRQ number for this device my_device_isr, // Pointer to the ISR function IRQF_SHARED, // Flags, e.g., allow sharing &quot;my_device&quot;, // Name for /proc/interrupts my_device_struct_ptr); // Data passed to the ISRif (ret) { // Error handling} irq_handler_t 类型: 这是ISR函数的类型定义：typedef irqreturn_t (*irq_handler_t)(int, void *); 执行流程: handle_irq_event_percpu 会像这样调用注册的函数（简化）：12345678910111213141516// Inside handle_irq_event_percpustruct irq_action *action;irqreturn_t action_ret;irqreturn_t overall_ret = IRQ_NONE;list_for_each_entry(action, &amp;desc-&gt;action, list) { // Call the driver's ISR action_ret = action-&gt;handler(irq, action-&gt;dev_id); // Update overall return status if handled if (action_ret == IRQ_HANDLED) overall_ret = IRQ_HANDLED; // Handle return values (e.g., logging if IRQ_NONE on non-shared IRQ)}// The overall_ret is often returned up the call stack 相关概念/函数: request_irq, irq_handler_t, irqreturn_t (IRQ_HANDLED, IRQ_NONE), struct irq_action, 中断共享 (IRQF_SHARED), 上半部 vs. 下半部。 中断返回 (Interrupt Return) 描述: 驱动ISR执行完毕并返回后，控制权层层返回到最初的汇编Trap入口程序。 汇编处理程序动作 (Conceptual Assembly - kernel_trap return path): handle_exception (或类似函数) 返回到汇编程序。 汇编程序从栈中恢复所有通用寄存器的值（之前 SAVE_ALL 保存的）。 根据需要恢复sstatus和sepc的值（例如，如果Trap是异常，可能需要调整sepc）。 执行 sret (Supervisor Return) 指令。 CPU 动作: sret 指令是RISC-V中从Trap返回的指令。它会自动： 将 sepc 的值加载到PC寄存器。 将 sstatus.SPIE 的值复制到 sstatus.SIE（恢复中断使能状态）。 将 sstatus.SPP 的值复制到当前CPU模式（恢复到Trap发生前的模式，通常是U或S）。 清除 sstatus.SPIE 和 sstatus.SPP。 最终结果: CPU恢复到Trap发生之前的模式，并从sepc指向的指令处继续执行中断前的程序。 相关概念/指令: sret, 寄存器恢复 (RESTORE_ALL)。 总结流程图示 (简化): 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061硬件设备 -&gt; PLIC -&gt; CPU (检查SIE, SEDELEG, SEIP) | | v v (发出物理中断) (响应中断) | v 硬件保存上下文 (sepc, scause, sstatus) | v CPU读取 stvec | v 跳转到内核 Trap 入口 (汇编) | v 汇编保存完整上下文 (pt_regs) | v 调用 C 语言 Trap 分发 (handle_exception) | v (如果 scause 指示是外部中断) 检查 scause, 调用外部中断处理 (handle_interrupt_common) | v 读取 PLIC Claim -&gt; 获取 PLIC ID | v irq_domain 映射 PLIC ID -&gt; Linux IRQ 号 | v 查找 irq_desc -&gt; 获取 handle_irq (流处理函数) | v 调用 流处理函数 (handle_level_irq/handle_edge_irq) | (流处理函数在适当时候写 PLIC Complete) v 调用 动作处理函数 (handle_irq_event_percpu) | v (遍历 struct irq_action 链表) 调用 驱动注册的 ISR (irq_handler_t function) &lt;------- request_irq 注册的函数 | v ISR 返回 (IRQ_HANDLED/IRQ_NONE) | v 动作处理函数返回 | v 流处理函数返回 | v C 语言 Trap 分发返回 | v 汇编 Trap 入口程序恢复上下文 (RESTORE_ALL) | v 执行 sret 指令 | v CPU 返回到中断前的程序继续执行 5. 内核空间和用户空间的通信方式总结与对比： 特性/通信方式 ioctl procfs sysfs netlink 用途 复杂控制命令、设备特定操作 文本形式的内核信息、调试 属性管理、设备模型暴露、配置参数 复杂、异步、双向通信、事件通知 数据格式 任意结构体或原始数据 文本字符串 文本字符串 (单值属性) 结构化二进制消息 (Netlink协议) 通信方向 双向 (同步请求-响应) 双向 (同步读写) 双向 (同步读写) 双向、异步、支持单播/多播/广播 API 基于文件描述符和 ioctl() 基于文件 I/O (open, read, write) 基于文件 I/O (open, read, write) 基于套接字 (socket, sendmsg, recvmsg) 复杂性 中等偏上 (需要手动拷贝) 简单 (文本处理) 中等 (Kobject/Attribute 机制) 较高 (套接字、消息解析) 性能 较高 (直接拷贝数据) 较低 (文本解析开销) 中等 (文本解析开销) 较高 (原生套接字、二进制消息) 典型示例 显卡驱动模式设置，网卡MAC地址 /proc/cpuinfo, /proc/meminfo /sys/class/gpio/gpiochipX/direction ip 命令配置网络，udev 监视事件 RISC-V 特性 作为系统调用，API通用，底层ABI遵循RISC-V约定 完全通用 完全通用，与Linux设备模型紧密结合 完全通用，作为网络栈组件 6. Linux 的内存管理机制7. 驱动中的 probe 函数, 调用流程是nameof_match_tableacpi","link":"/post/Troy-interview.html"},{"title":"hexo博客搭建","text":"使用hexo和GitHub Pagtes部署一个自己的博客 1. 安装并初始化Hexo 安装 Hexo CLI 1npm install -g hexo-cli 初始化博客项目目录 123mkdir my-blog &amp;&amp; cd my-bloghexo initnpm install 本地预览 1hexo server 启动本地服务：在浏览器访问 http://localhost:4000 查看效果 2. 配置 GitHub Pages 部署 创建GitHub仓库 创建一个仓库，名字叫 你的GitHub用户名.github.io 比如你是 goko，就叫 goko.github.io 安装部署插件 1npm install hexo-deployer-git --save 修改 _config.yml（根目录下）添加部署配置： 12345deploy: type: git # repo建议使用SSH, SSH免密 repo: https://github.com/你的GitHub用户名/你的GitHub用户名.github.io.git branch: main # 或者 master，看你的默认分支 生成并部署博客 123hexo cleanhexo generatehexo deploy 3. 域名(.com)绑定 添加域名(在my-blog下) 123echo &quot;&lt;xxxx&gt;.com&quot; &gt; source/CNAME# 或者可以：echo &quot;www.&lt;xxxx&gt;.com&quot; &gt; source/CNAME# 只能添加一个，而且两个需要添加不同的域名解析（如下） 重新部署 1hexo clean &amp;&amp; hexo g &amp;&amp; hexo d 设置 DNS 解析指向 GitHub Pages A. 使用裸域名（apex 域名）goku72.com 记录类型 主机记录 记录值 说明 A @ 185.199.108.153 GitHub Pages IP A @ 185.199.109.153 GitHub Pages IP A @ 185.199.110.153 GitHub Pages IP A @ 185.199.111.153 GitHub Pages IP example aliyun: 选择业务需求: 将网站域名解析到服务器IPv4地址 选择网站域名(主机记录): .com（对应设置“@”主机记录） 填写 IP（记录值）： 在输入框里粘贴以下四行（每一行一个 IP）： &gt; 185.199.109.153 &gt; 185.199.108.153 &gt; 185.199.110.153 &gt; 185.199.111.153 B. 使用 www.goku72.com 作为主域名 记录类型 主机记录 记录值 说明 CNAME www &lt;github用户名&gt;.github.io. 指向你的 GitHub 用户页仓库 example aliyun: 选择业务需求: 将网站域名解析到另外的目标域名 选择网站域名(主机记录): www..com（对应设置“www”主机记录） 填写 IP（记录值）：&lt;github用户名&gt;.github.io. (最有有一个符号”.”) 4. 设置主题 cd my-blog/themes git clone -b master https://github.com/jerryc127/hexo-theme-butterfly.git butterfly 修改_config.yml: theme: butterfly hexo clean &amp;&amp; hexo g &amp;&amp; hexo d 更多主题：https://hexo.io/themes/ 注： 如果AB两个方式都添加了，只需要在 Hexo 项目的 source/CNAME 文件中写 www..com，GitHub Pages 就会自动把 goku72.com 重定向过去，无需额外设置！ 后续换域名只需要：阿里云重新解析 + 修改 source/CNAME + 重新部署 Hexo，就能完成域名迁移。 有些主题可能需要下载插件","link":"/post/hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA.html"},{"title":"qspinlock","text":"qspinlock is a hybrid spinlock combining the fairness of ticket locks with the scalability of MCS locks: it uses only 4 bytes under low contention, falls back to an MCS queue under heavy load, and optimizes the second contender with a pending bit. It improves fairness and scalability but should not be enabled on RISC-V platforms lacking Ziccrse or Zabha. 1. 传统spinlock： 多个等待的 CPU 核心中，谁先获得锁并无保证，存在公平性问题，同时缓存一致性开销大（如MESI），CPU核心越大，cache需求越厉害，缺乏可扩展性 2. Ticket spinlock1234567891011121314151617#define TICKET_NEXT 16typedef struct { union { u32 lock; struct __raw_tickets { /* little endian */ u16 owner; u16 next; } tickets; };} arch_spinlock_t;my_ticket = atomic_fetch_inc(&amp;lock-&gt;tickets.next); while (lock-&gt;tickets.owner != my_ticket) cpu_relax(); 解决了公平问题，防止某些 CPU 永远得不到锁，但所有核都轮询同一个owner变量，read cache line成热点，限制扩展性 3. MCS lock 本质上是一种基于链表结构的自旋锁，每个CPU有一个对应的节点(锁的副本)，基于各自不同的副本变量进行等待，锁本身是共享的，但队列节点是线程自己维护的，每个CPU只需要查询自己对应的本地cache line，仅在这个变量发生变化的时候，才需要读取内存和刷新这条cache line, 不像 classic/ticket对共享变量进行spin 123456789101112131415161718192021222324struct mcs_spinlock { struct mcs_spinlock *next; int locked; /* 1 if lock acquired */ int count; /* nesting count, see qspinlock.c */};static inlinevoid mcs_spin_lock(struct mcs_spinlock **lock, struct mcs_spinlock *node){ struct mcs_spinlock *prev; /* Init node */ node-&gt;locked = 0; node-&gt;next = NULL; prev = xchg(lock, node); if (likely(prev == NULL)) { return; } WRITE_ONCE(prev-&gt;next, node); /* Wait until the lock holder passes the lock down. */ arch_mcs_spin_lock_contended(&amp;node-&gt;locked);} 每个 CPU 线程创建的node 是独立的，每个线程都有自己的 node 实例。但是结构体中多了一个指针使结构体变大了，导致了“内存开销问题”：MCS 锁把竞争带来的 cache-line 抖动降低了，但牺牲了一些内存和部分结构管理的成本。 4. qspinlockinclude/asm-generic/qspinlock_types.h: 锁数据结构 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758typedef struct qspinlock { union { atomic_t val; /* * By using the whole 2nd least significant byte for the * pending bit, we can allow better optimization of the lock * acquisition for the pending bit holder. */#ifdef __LITTLE_ENDIAN struct { u8 locked; u8 pending; }; struct { u16 locked_pending; u16 tail; };#else struct { u16 tail; u16 locked_pending; }; struct { u8 reserved[2]; u8 pending; u8 locked; };#endif };} arch_spinlock_t;/* * Initializier */#define __ARCH_SPIN_LOCK_UNLOCKED { { .val = ATOMIC_INIT(0) } }/* * Bitfields in the atomic value: * * When NR_CPUS &lt; 16K * 0- 7: locked byte * 8: pending * 9-15: not used * 16-17: tail index * 18-31: tail cpu (+1) * * When NR_CPUS &gt; = 16K * 0- 7: locked byte * 8: pending * 9-10: tail index * 11-31: tail cpu (+1) */#define _Q_SET_MASK(type) (((1U &lt;&lt; _Q_ ## type ## _BITS) - 1)\\ &lt;&lt; _Q_ ## type ## _OFFSET)#define _Q_LOCKED_OFFSET 0#define _Q_LOCKED_BITS 8#define _Q_LOCKED_MASK _Q_SET_MASK(LOCKED) When NR_CPUS &lt; 16K： locked：用来表示这个锁是否被人持有（0：无，1：有） pending：可以理解为最优先持锁位，即当unlock之后只有这个位的CPU最先持锁，也有1和0 tail：有idx+CPU构成，用来标识等待队列的最后一个节点。 tail_idx：就是index，它作为mcs_nodes数组的下标使用 tail_CPU：用来表示CPU的编号+1，+1因为规定tail为0的时候表示等待队列中没有成员 kernel/locking/mcs_spinlock.h 12345struct mcs_spinlock { struct mcs_spinlock *next; int locked; /* 1 if lock acquired */ int count; /* nesting count, see qspinlock.c */}; locked = 1:只是说锁传到了当前加节点，但是当前节点还需要主动申请锁(qspinlock -&gt; locked = 1)count：针对四种上下文用于追踪当前用了第几个 node（即 idx），最大为4,不够用时就fallback不排队直接自旋 kernel/locking/qspinlock.c: 123456789101112131415161718#define MAX_NODES 4struct qnode { struct mcs_spinlock mcs;#ifdef CONFIG_PARAVIRT_SPINLOCKS long reserved[2];#endif};/* * Per-CPU queue node structures; we can never have more than 4 nested * contexts: task, softirq, hardirq, nmi. * * Exactly fits one 64-byte cacheline on a 64-bit architecture. * * PV doubles the storage and uses the second cacheline for PV state. */static DEFINE_PER_CPU_ALIGNED(struct qnode, qnodes[MAX_NODES]); 一个 CPU 上可能嵌套多个锁, qnodes针对四种上下文情况下，例：进程上下文中发生中断后再次获取锁 PER_CPU的优点是快，可防止抢锁时再mallock或临时分配导致延迟，成本等问题 申请锁： 快速申请include/asm-generic/qspinlock.h 12345678910111213/** * queued_spin_lock - acquire a queued spinlock * @lock: Pointer to queued spinlock structure */static __always_inline void queued_spin_lock(struct qspinlock *lock){ int val = 0; if (likely(atomic_try_cmpxchg_acquire(&amp;lock-&gt;val, &amp;val, _Q_LOCKED_VAL))) return; queued_spin_lock_slowpath(lock, val);} 中速申请 快速申请失败，queue中为空时，设置锁的pending位 再次检测（检查中间是否有其它cpu进入） 一直循环检测locked位 当locked位为0时，清除pending位获得锁 慢速申请 申请 操作 快速申请 这个锁当前没有人持有，直接通过cmpxchg()设置locked域即可获取了锁 中速申请 锁已经被人持有，但是MCS链表没有其他人，有且仅有一个人在等待这个锁。设置pending域，表示是第一顺位继承者，自旋等待lock-&gt; locked清0，即锁持有者释放锁 慢速申请 进入到queue中自旋等待，若为队列头（队列中没有等待的cpu），说明它已排到最前，可以开始尝试获取锁；否则，它会自旋等待前一个节点释放锁，并通知它可以尝试获取锁了 end: 如果只有1个或2个CPU试图获取锁，那么只需要一个4字节的qspinlock就可以了，其所占内存的大小和ticket spinlock一样。当有3个以上的CPU试图获取锁，则需要(N-2)个MCS node qspinlock中加入”pending”位域的意义，如果是两个CPU试图获取锁，那么第二个CPU只需要简单地设置”pending”为1，而不用创建一个MCS node 试图加锁的CPU数目超过3个，使用ticket spinlock机制就会造成多个CPU的cache line刷新的问题，而qspinlock可以利用MCS node队列来解决这个问题 在多核争用严重场景下，qspinlock 让等待者在本地内存区域自旋，减少了锁的缓存抖动和对总线的竞争消耗 RISCV_QUEUED_SPINLOCKS 只应在平台具有 Zabha 或 Ziccrse 时启用，不支持的情况不要选用 优先级反转问题，queue会保证了FIFO提高了公平性，但它无法感知任务的优先级，可能因为排在队列前方的低优先级任务未释放锁而发生等待，从而导致 优先级反转","link":"/post/qspinlock.html"},{"title":"riscv-toolchains","text":"riscv-toolchain介绍 GNU 工具链的三元组（triplet），格式是： 1&lt;目标架构&gt;-&lt;供应商&gt;-&lt;目标系统&gt; riscv64-unknown-elf-： 适用于 riscv64 架构、面向嵌入式/裸机平台（使用 newlib 标准库）的工具链。 可从 riscv-collab/riscv-gnu-toolchain 构建。 不推荐通过包管理工具安装此编译链。发行版软件源中的此工具链常常缺少关键的 newlib 标准库。riscv64-unknown-linux-gnu-： 适用于 riscv64 架构、面向 linux-gnu 平台（使用 glibc 标准库）的工具链，可以与riscv64-linux-gnu- 相互替换。可从 riscv-collab/riscv-gnu-toolchain 构建。riscv64-linux-gnu-： 适用于 riscv64 架构、面向 linux-gnu 平台（使用 glibc 标准库）的工具链。 可通过包管理工具（如 Ubuntu 的 apt）安装。受限于发行版软件源限制，其编译器版本可能较老。","link":"/post/riscv-toolchains.html"},{"title":"Trace-use-3","text":"快速构建 Trace 子系统“认知” 3 2.6 之前 1234include/ asm-arm/ asm-i386/ asm-ppc/ “It would end up something like the old include/2.6.24 起 123asm-* setup” arch/arm/include/asm/...arch/x86/include/asm/... XFS 文件系统的 trace 头就放在 fs/xfs/xfs_trace.hDRM 的 trace 头就放在 drivers/gpu/drm/drm_trace.h架构相关的 trace 头就放在 arch/mips/include/trace/… #define TRACE_EVENT(…)之后要展开为trace代码并编译进去。 模块代码引用 silly-trace.h(drivers/sillymod/silly-trace.h)，而 silly-trace.h 引用了 define_trace.h（include/trace/define_trace.h）。 silly-trace.h 1234567891011121314#undef TRACE_SYSTEM#define TRACE_SYSTEM sillymod#if !defined(_TRACE_SILLYMOD_H) || defined(TRACE_HEADER_MULTI_READ)#define _TRACE_SILLYMOD_H#include &lt;linux/tracepoint.h&gt;// 定义你的 trace eventTRACE_EVENT(...);#endif /* _TRACE_SILLYMOD_H */#include &lt;trace/define_trace.h&gt; // 关键！用于生成实现代码 define_trace.hLinux 内核自带的通用头文件，它负责将你通过 TRACE_EVENT() 宏声明的事件，生成实际的 tracepoint 声明函数/结构体/探测函数等代码实现。","link":"/post/Trace-use-3.html"},{"title":"start-kerneling","text":"前言介绍….","link":"/post/start-kerneling.html"},{"title":"xv6-riscv-ch1","text":"This chapter-1 introduces the basic Unix process, file, and I/O abstractions that applications use to interact with the OS. ch1: Operating system interfaces As Figure 1.1 shows, xv6 takes the traditional form of a kernel, a special program that providesservices to running programs. Each running program, called a process, has memory containinginstructions, data, and a stack. The instructions implement the program’s computation. The dataare the variables on which the computation acts. The stack organizes the program’s procedure calls.A given computer typically has many processes but only a single kernel. When a user program invokes a sys-tem call, the hardware raises the privilege level and starts executing a pre-arranged function in thekernel.The collection of system calls that a kernel provides is the interface that user programs see. Thexv6 kernel provides a subset of the services and system calls that Unix kernels traditionally offer.Figure 1.2 lists all of xv6’s system calls. The shell is an ordinary program that reads commands from the user and executes them. Thefact that the shell is a user program, and not part of the kernel, illustrates the power of the systemcall interface: there is nothing special about the shell. It also means that the shell is easy to replace;as a result, modern Unix systems have a variety of shells to choose from, each with its own userinterface and scripting features. The xv6 shell is a simple implementation of the essence of theUnix Bourne shell. Its implementation can be found at (user/sh.c:1).The xv6 shell uses the exec calls of blew to run programs on behalf of users. The main structure ofthe shell is simple; see main (user/sh.c:146). The main loop reads a line of input from the user withgetcmd. Then it calls fork, which creates a copy of the shell process. The parent calls wait,while the child runs the command. For example, if the user had typed “echo hello” to the shell,runcmd would have been called with “echo hello” as the argument. runcmd (user/sh.c:55) runsthe actual command. For “echo hello”, it would call exec (user/sh.c:79). If exec succeeds thenthe child will execute instructions from echo instead of runcmd. At some point echo will callexit, which will cause the parent to return from wait in main (user/sh.c:146). 1.1 Processes and memory An xv6 process consists of user-space memory (instructions, data, and stack) and per-process stateprivate to the kernel. Xv6 time-shares processes: it transparently switches the available CPUsamong the set of processes waiting to execute. When a process is not executing, xv6 saves theprocess’s CPU registers, restoring them when it next runs the process. The kernel associates aprocess identifier, or PID, with each process. the following program fragment written in the C programming lan-guage 1234567891011int pid = fork();if(pid &gt; 0){printf(&quot;parent: child=%d\\n&quot;, pid);pid = wait((int *) 0);printf(&quot;child %d is done\\n&quot;, pid);} else if(pid == 0){printf(&quot;child: exiting\\n&quot;);exit(0);} else {printf(&quot;fork error\\n&quot;);} In the example, the output linesparent: child=1234child: exitingmight come out in either order (or even intermixed), depending on whether the parent or child getsto its printf call first. After the child exits, the parent’s wait returns, causing the parent to printparent: child 1234 is doneAlthough the child has the same memory contents as the parent initially, the parent and child areexecuting with separate memory and separate registers: changing a variable in one does not affectthe other. For example, when the return value of wait is stored into pid in the parent process, itdoesn’t change the variable pid in the child. The value of pid in the child will still be zero. The exec system call replaces the calling process’s memory with a new memory image loadedfrom a file stored in the file system. The file must have a particular format, which specifies whichpart of the file holds instructions, which part is data, at which instruction to start, etc. Xv6 uses theELF format, which Chapter 3 discusses in more detail. Usually the file is the result of compilinga program’s source code. When exec succeeds, it does not return to the calling program; instead,the instructions loaded from the file start executing at the entry point declared in the ELF header.exec takes two arguments: the name of the file containing the executable and an array of stringarguments. For example 123456char *argv[3];argv[0] = &quot;echo&quot;;argv[1] = &quot;hello&quot;;argv[2] = 0;exec(&quot;/bin/echo&quot;, argv);printf(&quot;exec error\\n&quot;); This fragment replaces the calling program with an instance of the program /bin/echo runningwith the argument list echo hello. Most programs ignore the first element of the argument array,which is conventionally the name of the program. why fork and exec are not combined in a single callwe will see later thatthe shell exploits the separation in its implementation of I/O redirection.Xv6 allocates most user-space memory implicitly: fork allocates the memory required for thechild’s copy of the parent’s memory, and exec allocates enough memory to hold the executablefile. A process that needs more memory at run-time (perhaps for malloc) can call sbrk(n) togrow its data memory by n zero bytes; sbrk returns the location of the new memory. 1.2 I/O and File descriptors A file descriptor is a small integer representing a kernel-managed object that a process may readfrom or write to. A process may obtain a file descriptor by opening a file, directory, or device,or by creating a pipe, or by duplicating an existing descriptor. For simplicity we’ll often referto the object a file descriptor refers to as a “file”; the file descriptor interface abstracts away thedifferences between files, pipes, and devices, making them all look like streams of bytes. We’llrefer to input and output as I/O. Internally, the xv6 kernel uses the file descriptor as an index into a per-process table, so thatevery process has a private space of file descriptors starting at zero. By convention, a process readsfrom file descriptor 0 (standard input), writes output to file descriptor 1 (standard output), andwrites error messages to file descriptor 2 (standard error). As we will see, the shell exploits theconvention to implement I/O redirection and pipelines. The shell ensures that it always has threefile descriptors open (user/sh.c:152), which are by default file descriptors for the console. The following program fragment (which forms the essence of the program cat) copies datafrom its standard input to its standard output. If an error occurs, it writes a message to the standarderror. 123456789101112131415char buf[512];int n;for(;;){n = read(0, buf, sizeof buf);if(n == 0)break;if(n &lt; 0){fprintf(2, &quot;read error\\n&quot;);exit(1);}if(write(1, buf, n) != n){fprintf(2, &quot;write error\\n&quot;);exit(1);}} The important thing to note in the code fragment is that cat doesn’t know whether it is readingfrom a file, console, or a pipe. Similarly cat doesn’t know whether it is printing to a console, afile, or whatever. The use of file descriptors and the convention that file descriptor 0 is input andfile descriptor 1 is output allows a simple implementation of cat. The close system call releases a file descriptor, making it free for reuse by a future open,pipe, or dup system call (see below). A newly allocated file descriptor is always the lowest-numbered unused descriptor of the current process. File descriptors and fork interact to make I/O redirection easy to implement. 12345678char *argv[2];argv[0] = &quot;cat&quot;;argv[1] = 0;if(fork() == 0) {close(0);open(&quot;input.txt&quot;, O_RDONLY);exec(&quot;cat&quot;, argv);} After the child closes file descriptor 0, open is guaranteed to use that file descriptor for the newlyopened input.txt: 0 will be the smallest available file descriptor. cat then executes with filedescriptor 0 (standard input) referring to input.txt. The parent process’s file descriptors are notchanged by this sequence Two file descriptors share an offset if they were derived from the same original file descriptorby a sequence of fork and dup calls. Otherwise file descriptors do not share offsets, even if theyresulted from open calls for the same file. dup allows shells to implement commands like this: ls existing-file non-existing-file &gt; tmp1 2&gt;&amp;1. The 2&gt;&amp;1 tells the shell to give the command a file descriptor 2 that is a duplicate of descriptor 1. Both the name of the existing file and the error message for the non-existing file will show up in the file tmp1. The xv6 shell doesn’t support I/O redirection for the error file descriptor, but now you know how to implement it. 1.3 Pipes A pipe is a small kernel buffer exposed to processes as a pair of file descriptors, one for readingand one for writing. Writing data to one end of the pipe makes that data available for reading fromthe other end of the pipe. Pipes provide a way for processes to communicate. The following example code runs the program wc with standard input connected to the readend of a pipe. 12345678910111213141516int p[2];char *argv[2];argv[0] = &quot;wc&quot;;argv[1] = 0;pipe(p);if(fork() == 0) {close(0);dup(p[0]);close(p[0]);close(p[1]);exec(&quot;/bin/wc&quot;, argv);} else {close(p[0]);write(p[1], &quot;hello world\\n&quot;, 12);close(p[1]);} 12345678910 pipe [p[1]] -------&gt; [p[0]] (write) (read)parent: write(p[1], ...)child: dup(p[0]) -&gt; fd 0 exec(&quot;wc&quot;) -&gt; wc reads from stdin (=read of pipe) The fact that read blocks until it is impossible for new data to arriveis one reason that it’s important for the child to close the write end of the pipe before executingwc above: if one of wc ’s file descriptors referred to the write end of the pipe and not close, wc would never seeend-of-file. The xv6 shell implements pipelines such as grep fork sh.c | wc -l in a manner similarto the above code (user/sh.c:101). The child process creates a pipe to connect the left end of thepipeline with the right end. Then it calls fork and runcmd for the left end of the pipeline andfork and runcmd for the right end, and waits for both to finish. The right end of the pipelinemay be a command that itself includes a pipe (e.g., a | b | c), which itself forks two new childprocesses (one for b and one for c). Thus, the shell may create a tree of processes. The leaves16of this tree are commands and the interior nodes are processes that wait until the left and rightchildren complete. 12345 sh / \\a sh / \\ b c echo hello world &gt;/tmp/xyz; wc &lt;/tmp/xyzPipes have at least three advantages over temporary files in this situation. First, pipes automatically clean themselves up; with the file redirection, a shell would have to be careful to remove /tmp/xyz when done. Second, pipes can pass arbitrarily long streams of data, while file redirection requires enough free space on disk to store all the data. Third, pipes allow for parallel execution of pipeline stages, while the file approach requires the first program to finish before the second starts. 1.4 File system The xv6 file system provides data files, which contain uninterpreted byte arrays, and directories,which contain named references to data files and other directories. There are system calls to create new files and directories: mkdir creates a new directory, openwith the O_CREATE flag creates a new data file, and mknod creates a new device file. This exampleillustrates all three: 1234mkdir(&quot;/dir&quot;);fd = open(&quot;/dir/file&quot;, O_CREATE|O_WRONLY);close(fd);mknod(&quot;/console&quot;, 1, 1); mknod creates a special file that refers to a device. Associated with a device file are the major andminor device numbers (the two arguments to mknod), which uniquely identify a kernel device.When a process later opens a device file, the kernel diverts read and write system calls to thekernel device implementation instead of passing them to the file system. A file’s name is distinct from the file itself; the same underlying file, called an inode, can havemultiple names, called links. Each link consists of an entry in a directory; the entry contains a filename and a reference to an inode. An inode holds metadata about a file, including its type (file ordirectory or device), its length, the location of the file’s content on disk, and the number of links toa file. The fstat system call retrieves information from the inode that a file descriptor refers to. Itfills in a struct stat, defined in stat.h (kernel/stat.h) as: 1234567891011#define T_DIR 1// Directory#define T_FILE 2// File#define T_DEVICE 3// Devicestruct stat { int dev; // File system’s disk device uint ino; // Inode number short type; // Type of file short nlink; // Number of links to file uint64 size; // Size of file in bytes}; The link system call creates another file system name referring to the same inode as an exist-ing file. This fragment creates a new file named both a and b. 12open(&quot;a&quot;, O_CREATE|O_WRONLY);link(&quot;a&quot;, &quot;b&quot;); Reading from or writing to a is the same as reading from or writing to b. Each inode is identifiedby a unique inode number. After the code sequence above, it is possible to determine that a and brefer to the same underlying contents by inspecting the result of fstat: both will return the sameinode number (ino), and the nlink count will be set to 2.The unlink system call removes a name from the file The unlink system call removes a name from the file system. The file’s inode and the diskspace holding its content are only freed when the file’s link count is zero and no file descriptorsrefer to it. Thus adding 1unlink(&quot;a&quot;); to the last code sequence leaves the inode and file content accessible as b. Furthermore, 12fd = open(&quot;/tmp/xyz&quot;, O_CREATE|O_RDWR);unlink(&quot;/tmp/xyz&quot;); is an idiomatic way to create a temporary inode with no name that will be cleaned up when theprocess closes fd or exits. Unix provides file utilities callable from the shell as user-level programs, for example mkdir,ln, and rm. This design allows anyone to extend the command-line interface by adding new user-level programs. In hindsight this plan seems obvious, but other systems designed at the time ofUnix often built such commands into the shell (and built the shell into the kernel).One exception is cd, which is built into the shell (user/sh.c:161). cd must change the currentworking directory of the shell itself. If cd were run as a regular command, then the shell would18fork a child process, the child process would run cd, and cd would change the child ’s workingdirectory. The parent’s (i.e., the shell’s) working directory would not change. 1.5 Real world the shell was the first so-called “scripting language.” The Unix system call interface persists today insystems like BSD, Linux, and macOS. The Unix system call interface has been standardized through the Portable Operating SystemInterface (POSIX) standard. Xv6 is not POSIX compliant: it is missing many system calls (in-cluding basic ones such as lseek), and many of the system calls it does provide differ from thestandard. Our main goals for xv6 are simplicity and clarity while providing a simple UNIX-likesystem-call interface. Several people have extended xv6 with a few more system calls and a sim-ple C library in order to run basic Unix programs. Modern kernels, however, provide many moresystem calls, and many more kinds of kernel services, than xv6. For example, they support net-working, windowing systems, user-level threads, drivers for many devices, and so on. Modernkernels evolve continuously and rapidly, and offer many features beyond POSIX. Xv6 does not provide a notion of users or of protecting one user from another; in Unix terms,all xv6 processes run as root. comments： Linux tries to adhere to POSIX (glibc provides most of the POSIX interfaces), but has its own extensions (e.g., epoll). Programmers who write POSIX interfaces can compile and run them on macOS, BSD, and Linux (as long as they don’t use platform-specific extensions). Think of the xv6 system call interface as a “subset implementation of POSIX.” POSIX文档","link":"/post/xv6-riscv-ch1.html"},{"title":"xv6-riscv-ch2","text":"This chapter2 explains how the OS is structured internally to manage hardware resources, run processes, and enforce protection. ch2: Operating system organization A key requirement for an operating system is to support several activities at once. an operating system must fulfill three requirements: multiplexing, isolation, andinteraction. Xv6 runs on a multi-core1 RISC-V microprocessor, and much of its low-level functionality(for example, its process implementation) is specific to RISC-V. RISC-V is a 64-bit CPU, and xv6is written in “LP64” C, which means long (L) and pointers (P) in the C programming languageare 64 bits, but an int is 32 bits. RISCV Technical Specifications 2.1 Abstracting physical resources The Unix interface is not the only way to abstractresources, but it has proved to be a good one. 2.2 User mode, supervisor mode, and system calls CPUs provide hardware support for strong isolation. For example, RISC-V has three modes inwhich the CPU can execute instructions: machine mode, supervisor mode, and user mode. Instruc-tions executing in machine mode have full privilege; a CPU starts in machine mode. Machine modeis mostly intended for setting up the computer during boot. Xv6 executes a few lines in machinemode and then changes to supervisor mode. In supervisor mode the CPU is allowed to execute privileged instructions: for example, en-abling and disabling interrupts, reading and writing the register that holds the address of a pagetable, etc. An application can execute only user-mode instructions (e.g., addingnumbers, etc.) and is said to be running in user space, while the software in supervisor mode canalso execute privileged instructions and is said to be running in kernel space. The software runningin kernel space (or in supervisor mode) is called the kernel. CPUs provide aspecial instruction that switches the CPU from user mode to supervisor mode and enters the kernelat an entry point specified by the kernel. (RISC-V provides the ecall instruction for this purpose.)Once the CPU has switched to supervisor mode, the kernel can then validate the arguments of thesystem call (e.g., check if the address passed to the system call is part of the application’s memory),decide whether the application is allowed to perform the requested operation (e.g., check if theapplication is allowed to write the specified file), and then deny it or execute it. It is important thatthe kernel control the entry point for transitions to supervisor mode; if the application could decidethe kernel entry point, a malicious application could, for example, enter the kernel at a point wherethe validation of arguments is skipped. 2.3 Kernel organization A key design question is what part of the operating system should run in supervisor mode. Onepossibility is that the entire operating system resides in the kernel, so that the implementations ofall system calls run in supervisor mode. This organization is called a monolithic kernel. A downside of the monolithic organization is that the interactions among different parts ofthe operating system are often complex (as we will see in the rest of this text), and therefore it is easy for an operating system developer to make a mistake. In a monolithic kernel, a mistake isfatal, because an error in supervisor mode will often cause the kernel to fail. If the kernel fails,the computer stops working, and thus all applications fail too. The computer must reboot to startagain. To reduce the risk of mistakes in the kernel, OS designers can minimize the amount of operatingsystem code that runs in supervisor mode, and execute the bulk of the operating system in usermode. This kernel organization is called a microkernel. Figure 2.1 illustrates this microkernel design. In the figure, the file system runs as a user-levelprocess. OS services running as processes are called servers. To allow applications to interact withthe file server, the kernel provides an inter-process communication mechanism to send messagesfrom one user-mode process to another. Xv6 is implemented as a monolithic kernel, like most Unix operating systems. Thus, the xv6kernel interface corresponds to the operating system interface, and the kernel implements the com-plete operating system. Since xv6 doesn’t provide many services, its kernel is smaller than somemicrokernels, but conceptually xv6 is monolithic. 2.4 Code: xv6 organization The xv6 kernel source is in the kernel/ sub-directory. The source is divided into files, followinga rough notion of modularity; Figure 2.2 lists the files.The inter-module interfaces are defined in defs.h (kernel/defs.h). 2.5 Process overview The unit of isolation in xv6 (as in other Unix operating systems) is a process. The process ab-straction prevents one process from wrecking or spying on another process’s memory, CPU, filedescriptors, etc. It also prevents a process from wrecking the kernel itself, so that a process can’tsubvert the kernel’s isolation mechanisms. To help enforce isolation, the process abstraction provides the illusion to a program that it hasits own private machine. A process provides a program with what appears to be a private memorysystem, or address space, which other processes cannot read or write. A process also provides theprogram with what appears to be its own CPU to execute the program’s instructions. Xv6 uses page tables (which are implemented by hardware) to give each process its own ad-dress space. The RISC-V page table translates (or “maps”) a virtual address (the address that anRISC-V instruction manipulates) to a physical address (an address that the CPU sends to mainmemory). 12345678910111213| 9 bits | 9 bits | 9 bits | 12 bits || VPN[2] | VPN[1] | VPN[0] | Page Offset ||-------- 页表索引 --------| 页内偏移 |example:虚拟地址：0x0000004001234567 ↓VPN[2] = 0x01 → level-2 页表（根）中偏移 8，得到 PTE1 → 指向 0x2000_0000VPN[1] = 0x01 → level-1 页表中偏移 8，得到 PTE2 → 指向 0x3000_0000VPN[0] = 0x46 → level-0 页表中偏移 0x230，得到 PTE3 → 页帧 0x4000offset = 0x4567→ 最终物理地址 = 0x4000_0000 + 0x4567 = **0x4000_4567** Xv6 maintains a separate page table for each process that defines that process’s address space.As illustrated in Figure 2.3, an address space includes the process’s user memory starting at virtualaddress zero. Instructions come first, followed by global variables, then the stack, and finally a “heap” area (for malloc) that the process can expand as needed. There are a number of factors that limit the maximum size of a process’s address space: pointers on the RISC-V are 64 bits wide; the hardware uses only the low 39 bits when looking up virtual addresses in page tables; and xv6 uses only 38 of those 39 bits. Thus, the maximum address is 238 − 1 = 0x3fffffffff, which is MAXVA (kernel/riscv.h:378). At the top of the address space xv6 places a trampoline page (4096 bytes) and a trapframe page. Xv6 uses these two pages to transition into the kernel and back; the trampoline page contains the code to transition in and out of the kernel, and the trapframe is where the kernel saves the process’s user registers, as Chapter 4 explains. The xv6 kernel maintains many pieces of state for each process, which it gathers into a struct proc (kernel/proc.h:85). A process’s most important pieces of kernel state are its page table, its kernelstack, and its run state. We’ll use the notation p-&gt;xxx to refer to elements of the proc structure; for example, p-&gt;pagetable is a pointer to the process’s page table. Each process has a thread of control (or thread for short) that holds the state needed to ex-ecute the process.might be executing on a CPU, or suspended (notexecuting, but capable of resuming executing in the future). Each process has two stacks: user stack: When the process is executing user instructions,only its user stack is in use, and its kernel stack is empty. kernel stack: When the process enters the kernel (for a system call or interrupt), the kernel code executes on the process’s kernel stack; while a process is in the kernel, its user stack still contains saved data, but isn’t actively used. A process’s thread alternates between actively using its user stack and its kernel stack. The kernel stack is separate (and protected from user code) so that the kernel can execute even if a process has wrecked its user stack. A process can make a system call by executing the RISC-V ecall instruction. This instruction raises the hardware privilege level and changes the program counter to a kernel-defined entry point. The code at the entry point switches to the process’s kernel stack and executes the kernel instructions that implement the system call. When the system call completes, the kernel switches back to the user stack and returns to user space by calling the sret instruction, which lowers the hardware privilege level and resumes executing user instructions just after the system call instruction. A process’s thread can “block” in the kernel to wait for I/O, and resume where it left off when the I/O has finished. p-&gt;state indicates whether the process is allocated, ready to run, currently running on a CPU, waiting for I/O, or exiting. p-&gt;pagetable holds the process’s page table, in the format that the RISC-V hardware ex- pects. Xv6 causes the paging hardware to use a process’s p-&gt;pagetable when executing that process in user space. A process’s page table also serve In summary, a process bundles two design ideas: an address space to give a process the illusion of its own memory, and a thread to give the process the illusion of its own CPU. In xv6, a process consists of one address space and one thread. In real operating systems a process may have more than one thread to take advantage of multiple CPUs. 2.6 Code: starting xv6, the first process and system callTo make xv6 more concrete, we’ll outline how the kernel starts and runs the first process.The subsequent chapters will describe the mechanisms that show up in this overview in more detail. When the RISC-V computer powers on, it initializes itself and runs a boot loader which is stored in read-only memory.The boot loader loads the xv6 kernel into memory.Then, in machine mode, the CPU executes xv6 starting at _entry (kernel/entry.S:7).The RISC-V starts with paging hardware disabled: virtual addresses map directly to physical addresses. The loader loads the xv6 kernel into memory at physical address 0x80000000.The reason it places the kernel at 0x80000000 rather than 0x0 is because the address range 0x0:0x80000000 contains I/O devices. The instructions at _entry set up a stack so that xv6 can run C code.Xv6 declares space for an initial stack, stack0, in the file start.c (kernel/start.c:11).The code at _entry loads the stack pointer register sp with the address stack0 + 4096, the top of the stack, because the stack on RISC-V grows down.Now that the kernel has a stack, _entry calls into C code at start (kernel/start.c:15). The function start performs some configuration that is only allowed in machine mode, and then switches to supervisor mode.To enter supervisor mode, RISC-V provides the instruction mret.This instruction is most often used to return from a previous call from supervisor mode to machine mode.start isn’t returning from such a call, but sets things up as if it were: it sets the previous privilege mode to supervisor in the register mstatus, it sets the return address to main by writing main’s address into the register mepc, disables virtual address translation in supervisor mode by writing 0 into the page-table register satp, and delegates all interrupts and exceptions to supervisor mode. Before jumping into supervisor mode, start performs one more task:it programs the clock chip to generate timer interrupts.With this housekeeping out of the way, start “returns” to supervisor mode by calling mret.This causes the program counter to change to main (kernel/main.c:11), the address previously stored in mepc. After main (kernel/main.c:11) initializes several devices and subsystems, it creates the first process by calling userinit (kernel/proc.c:233).The first process executes a small program written in RISC-V assembly, which makes the first system call in xv6.initcode.S (user/initcode.S:3) loads the number for the exec system call, SYS_EXEC (kernel/syscall.h:8), into register a7,and then calls ecall to re-enter the kernel. The kernel uses the number in register a7 in syscall (kernel/syscall.c:132) to call the desired system call.The system call table (kernel/syscall.c:107) maps SYS_EXEC to the function sys_exec, which the kernel invokes.As we saw in Chapter 1, exec replaces the memory and registers of the current process with a new program (in this case, /init). Once the kernel has completed exec, it returns to user space in the /init process.init (user/init.c:15) creates a new console device file if needed and then opens it as file descriptors 0, 1, and 2. Then it starts a shell on the console. The system is up. 2.7 Security Model The operating system must assume that a process’s user-level code will do its best to wreck the kernel or other processes.User code may try to dereference pointers outside its allowed address space; it may attempt to execute any RISC-V instructions, even those not intended for user code; it may try to read and write any RISC-V control register; it may try to directly access device hardware; and it may pass clever values to system calls in an attempt to trick the kernel into crashing or doing something stupid. The kernel’s goal is to restrict each user process so that all it can do is: read/write/execute its own user memory, use the 32 general-purpose RISC-V registers, and affect the kernel and other processes only in the ways that system calls are intended to allow. The expectations for the kernel’s own code are quite different.Kernel code is assumed to be written by well-meaning and careful programmers.Kernel code is expected to be bug-free, and certainly to contain nothing malicious.This assumption affects how we analyze kernel code.For example, there are many internal kernel functions (e.g., the spin locks) that would cause serious problems if kernel code used them incorrectly.When examining any specific piece of kernel code, we’ll want to convince ourselves that it behaves correctly.We assume, however, that kernel code in general is correctly written, and follows all the rules about use of the kernel’s own functions and data structures. At the hardware level, the RISC-V CPU, RAM, disk, etc. are assumed to operate as advertised in the documentation, with no hardware bugs. 2.8 Real worldMost operating systems have adopted the process concept, and most processes look similar to xv6’s. Modern operating systems, however, support several threads within a process, to allow a single process to exploit multiple CPUs. Supporting multiple threads in a process involves quite a bit of machinery that xv6 doesn’t have, often including interface changes (e.g., Linux’s clone, a variant of fork), to control which aspects of a process threads share.","link":"/post/xv6-riscv-ch2.html"},{"title":"IIC","text":"EEPROM(IIC) ROM(Read Only Memory): 制造和升级不便 PROM(Programmable ROM): 但是只能写入一次，后续无法修改 EPROM(Erasable Programmable ROM): 紫外线透过玻璃窗口照射内部芯片就可以擦除其内部的数据 EEPROM: 带电可擦除可编程只读存储器，以电子信号来修改其内容，它属于双电压芯片。借助于EEPROM芯片的双电压特性，可以使BIOS具有良好的防毒功能，在升级时，把跳线开关打至“on”的位置，即给芯片加上相应的编程电压，就可以方便地升级；平时使用时，则把跳线开关打至“off”的位置，防止CIH类的病毒对BIOS芯片的非法修改。 读取IIC设备： 控制器 设备地址：根据芯片手册和模块原理图查询查找IIC设备：i2cdetect -y 0/1/2… Platform 总线 (Platform Bus): 特点: 这是一种虚拟总线，用于连接那些没有硬件总线（如PCI、USB等）但又需要与CPU直接通信的设备。这些设备通常是SoC（System on Chip）内部的各种控制器，例如GPIO控制器、UART、SPI、I2C控制器本身等等。 作用: 它提供了一种统一的机制来管理和抽象这些片上设备，使得驱动开发者不需要关心具体的硬件地址和中断号，而是通过 Platform 总线提供的接口来注册和操作设备。 原理: Platform 设备和 Platform 驱动通过 platform_device 和 platform_driver 结构体进行描述。当设备和驱动的名称匹配时，内核就会将它们关联起来。 匹配过程: 通常是基于 name 字段的字符串匹配。platform_device 中的 name 字段与 platform_driver 中 driver.name 字段进行比较。如果匹配成功，就会调用驱动的 probe 函数。 I2C 总线 (I2C Bus): 特点: I2C 是一种串行通信协议，用于连接低速外设，如传感器、EEPROM、实时时钟 (RTC) 等。在 Linux 中，I2C 总线管理着I2C控制器和I2C从设备。 作用: 它为I2C设备提供了一套标准的API，使得驱动开发者可以方便地读写I2C设备寄存器，而无需关心I2C协议的底层细节。 原理: I2C 总线包含I2C适配器（I2C Adapter，即I2C控制器）和I2C客户端（I2C Client，即I2C从设备）。适配器提供I2C通信能力，客户端则代表具体的I2C设备。 匹配过程: I2C设备的匹配通常有两种方式： 基于名称匹配: i2c_client 中的 name 字段与 i2c_driver 中的 id_table 里的 name 字段进行匹配。 基于compatible字符串匹配 (更常用和推荐): i2c_client 中的 of_node-&gt;compatible 属性与 i2c_driver 中的 of_match_table 里的 compatible 字符串进行匹配。这种方式常用于设备树 (Device Tree) 中。 除了这两者，还有： PCI 总线 (PCI Bus): 用于连接高性能外设，如显卡、网卡、声卡等。 USB 总线 (USB Bus): 用于连接各种USB设备，如U盘、键盘、鼠标、摄像头等。 SPI 总线 (SPI Bus): 另一种串行通信协议，常用于连接传感器、FLASH存储器等。 MMC/SD 总线 (MMC/SD Bus): 用于连接SD卡、eMMC存储器等。 Input 总线 (Input Bus): 用于管理各种输入设备，如键盘、鼠标、触摸屏等。 等等… 2. 它们之间有什么区别和联系？都有什么用？原理是什么？区别： 物理特性: I2C: 是一种串行通信协议，有SDA（数据线）和SCL（时钟线）两根线。 Platform: 是一种虚拟总线，没有对应的物理连接线，它抽象的是CPU内部或直接连接到CPU的设备。 其他总线 (PCI, USB等): 都有各自的物理连接方式和通信协议。 设备类型: I2C: 专注于低速外设。 Platform: 专注于SoC内部或直接连接的片上设备。 PCI: 专注于高性能、高带宽设备。 USB: 专注于即插即用、通用性强的设备。 联系： 统一的设备模型: 尽管有各种不同的总线，但 Linux 内核提供了一个统一的设备模型 (Device Model)。这个模型的目标是将设备和驱动进行分离，实现通用化和可移植性。 Bus-Device-Driver 架构: 所有的总线都遵循 Bus-Device-Driver 架构。 Bus (总线): 负责管理其上的设备和驱动，提供匹配机制。 Device (设备): 代表具体的硬件设备，包含设备的各种信息（地址、中断号、名称、ID等）。 Driver (驱动): 负责与特定类型的设备进行交互，实现设备的各种功能。 Client-Driver 适配: 你说的没错！无论是 I2C、Platform 还是其他总线，它们的核心都是 Client (设备) 和 Driver (驱动) 进行适配。这里的 “Client” 对应于 “Device”。 都有什么用？ 抽象硬件差异: 不同的总线屏蔽了底层硬件的复杂性，提供统一的编程接口。 代码复用: 驱动可以独立于具体的硬件平台开发，只要设备模型支持，就可以在不同的平台上运行。 模块化管理: 允许设备和驱动作为独立的模块加载和卸载，提高了系统的灵活性。 即插即用 (Plug-and-Play): 对于PCI、USB等支持热插拔的总线，设备模型可以实现设备的动态识别和加载驱动。 原理： Linux 设备模型的核心思想是 “将设备和驱动分离”。它通过 struct device 和 struct device_driver 这两个核心结构体来表示设备和驱动。 struct bus_type: 定义了总线的属性和操作，包括设备的注册、驱动的注册、设备的遍历、驱动的匹配函数等。 struct device: 代表一个具体的硬件设备。它包含设备的通用属性（如名称、父设备、设备树节点等），以及特定总线的私有数据。 struct device_driver: 代表一个设备驱动。它包含驱动的通用属性（如名称、驱动所支持的设备ID表），以及驱动的操作函数（如 probe、remove 等）。 当一个设备被注册到总线上时，总线会遍历所有已注册的驱动，尝试找到能够与该设备匹配的驱动。一旦匹配成功，就会调用驱动的 probe 函数来初始化设备。 3. 驱动不就是一份代码吗？为什么还有driver的代码和client的代码，两份？这是一个很好的问题，也是很多初学者容易混淆的地方。 驱动 (Driver) 确实是“一份代码”，但这份代码是为了管理一类特定功能的硬件设备。 为了实现驱动的通用性和可移植性，Linux 设备模型将驱动分成了两个逻辑部分： 设备 (Device) 的描述代码 (Client/Platform Device): 这部分代码主要负责 描述硬件设备的信息，而不是实现设备的功能。它告诉内核：“这里有一个设备，它的类型是什么，它连接在哪个总线上，它的地址是多少，它需要哪些资源（如中断、内存区域）”。 这部分代码通常位于 板级文件 (board-specific file) 或 设备树 (Device Tree) 中。 例如： 对于 Platform 设备，你会看到 struct platform_device 的定义，里面包含了设备的名称、资源等。 对于 I2C 设备，你会看到 struct i2c_client 的定义，或者在设备树中描述I2C设备的节点。 驱动 (Driver) 的功能实现代码 (Platform Driver/I2C Driver): 这部分代码才是真正意义上的 “驱动”。它负责 实现与特定类型设备进行交互的逻辑。 它包含了设备初始化（probe 函数）、数据传输、中断处理、电源管理等核心功能。 这部分代码通常位于 独立的驱动文件 中（例如：drivers/char/xxx.c, drivers/i2c/chips/yyy.c, drivers/platform/zzz.c）。 例如： 对于 Platform 驱动，你会看到 struct platform_driver 的定义，其中包含了 probe、remove 等函数指针。 对于 I2C 驱动，你会看到 struct i2c_driver 的定义，同样包含 probe、remove 等函数指针。 为什么会有两份？ 分离关注点 (Separation of Concerns): 将设备描述和驱动功能分离，使得驱动代码更加通用。同一个驱动可以在不同的硬件平台上使用，只要这些平台能够正确描述出该设备。 板级厂商只需要描述其硬件设备的特性，而驱动开发者可以专注于实现设备功能。 可移植性 (Portability): 驱动代码可以独立于具体的硬件平台编译和加载。 当硬件平台发生变化时，可能只需要修改设备描述部分（如设备树），而无需修改驱动代码本身。 模块化 (Modularity): 设备和驱动可以作为独立的模块动态加载和卸载，方便开发和调试。 即插即用 (Plug and Play): 当设备被发现时（例如插入USB设备），内核可以根据设备的描述信息自动寻找并加载对应的驱动。 4. Match 的过程是什么样子的呢？匹配 (Matching) 是设备模型中最核心的机制之一。当一个设备被注册到总线上时，内核会触发匹配过程，寻找能够驱动该设备的驱动。 通用匹配流程 (以 Platform 总线为例)： 设备注册: 当内核或某个模块发现并注册一个 platform_device 到 platform_bus 上时，匹配过程开始。 这通常发生在内核启动时，或者通过设备树动态解析设备。 platform_device_register() 函数会被调用。 遍历驱动: platform_bus 会遍历所有已经注册到它上面的 platform_driver。 调用匹配函数: 对于每一个 platform_driver，总线会调用其内部的 match 函数（对于 Platform 总线，通常是 platform_match()）。 匹配逻辑: match 函数会根据预定的规则（通常是名称匹配或 compatible 字符串匹配）来判断当前设备是否与当前驱动兼容。 名称匹配: platform_device-&gt;name == platform_driver-&gt;driver.name。 设备树 compatible 匹配 (更常用): platform_device-&gt;of_node-&gt;compatible 属性与 platform_driver-&gt;of_match_table 中的 compatible 字符串进行比较。 成功匹配: 如果 match 函数返回成功（表示设备和驱动匹配），那么总线就会将该设备与该驱动关联起来。 调用 probe 函数: 接着，内核会调用匹配成功的 platform_driver 的 probe 函数。 probe 函数是驱动的核心，它负责初始化设备、请求资源、注册中断、创建设备节点等等。 如果 probe 函数成功返回，表示设备驱动加载成功；如果返回错误码，则表示加载失败。 I2C 总线匹配流程： I2C 总线的匹配与 Platform 类似，但它有自己的 i2c_bus_type 和匹配函数。 I2C 适配器注册: I2C 控制器作为 Platform 设备注册后，其 Platform 驱动会注册 i2c_adapter，这代表了一个可用的 I2C 总线。 I2C 客户端注册: I2C 设备的描述（通常在设备树中）被解析后，会创建一个 i2c_client 结构体并将其注册到对应的 i2c_adapter 上。 遍历 I2C 驱动: I2C 总线会遍历所有已经注册的 i2c_driver。 调用匹配函数: I2C 总线会调用其内部的匹配函数 (i2c_device_match())。 匹配逻辑: ID 表匹配: i2c_driver-&gt;id_table 字段中的 name 成员与 i2c_client-&gt;name 进行匹配。 设备树 compatible 匹配 (更常用): i2c_client-&gt;dev.of_node-&gt;compatible 属性与 i2c_driver-&gt;driver.of_match_table 中的 compatible 字符串进行比较。 成功匹配: 匹配成功后，调用 i2c_driver 的 probe 函数。 总结： Linux 设备模型的核心是 Bus-Device-Driver 架构，旨在将设备和驱动分离，实现代码的通用性和可移植性。 总线 (Bus) 负责管理设备和驱动的注册与匹配。 设备 (Device/Client) 描述了硬件的特性和资源。 驱动 (Driver) 实现了与设备交互的逻辑。 匹配过程是总线根据预定的规则（名称或 compatible 字符串）将设备和驱动关联起来的过程，成功后会调用驱动的 probe 函数。 希望这些解释能帮助你更好地理解 Linux 设备驱动的 Bus-Device-Driver 架构！这是一个值得深入学习的领域，一旦掌握，你会对操作系统如何与硬件交互有更深刻的理解。","link":"/post/wds-IIC.html"},{"title":"xv6-riscv_struct","text":"File structure of xv6-riscv 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475.├── kernel│ ├── bio.c│ ├── buf.h│ ├── console.c│ ├── defs.h│ ├── elf.h│ ├── entry.S│ ├── exec.c│ ├── fcntl.h│ ├── file.c│ ├── file.h│ ├── fs.c│ ├── fs.h│ ├── kalloc.c│ ├── kernel.ld│ ├── kernelvec.S│ ├── log.c│ ├── main.c│ ├── memlayout.h│ ├── param.h│ ├── pipe.c│ ├── plic.c│ ├── printf.c│ ├── proc.c│ ├── proc.h│ ├── riscv.h│ ├── sleeplock.c│ ├── sleeplock.h│ ├── spinlock.c│ ├── spinlock.h│ ├── start.c│ ├── stat.h│ ├── string.c│ ├── swtch.S│ ├── syscall.c│ ├── syscall.h│ ├── sysfile.c│ ├── sysproc.c│ ├── trampoline.S│ ├── trap.c│ ├── types.h│ ├── uart.c│ ├── virtio_disk.c│ ├── virtio.h│ └── vm.c├── LICENSE├── Makefile├── mkfs│ └── mkfs.c├── README└── user ├── cat.c ├── echo.c ├── forktest.c ├── grep.c ├── grind.c ├── init.c ├── initcode.S ├── kill.c ├── ln.c ├── ls.c ├── mkdir.c ├── printf.c ├── rm.c ├── sh.c ├── stressfs.c ├── ulib.c ├── umalloc.c ├── user.h ├── user.ld ├── usertests.c ├── usys.pl ├── wc.c └── zombie.c 逐文件详细解读它们的作用、重要性： 🧠 整体结构 目录/文件 作用 建议重点关注 kernel/ xv6 核心实现，内核源码 ✅ 必看 user/ 用户态程序、测试程序、库 ✅ 推荐 mkfs/ 构建文件系统镜像工具 ❗可选 顶层构建文件 Makefile, README, LICENSE 等 🧩 熟悉结构用 📁 kernel/ — xv6 的心脏（重点模块）xv6 主要模块可分为： 内核启动与内存管理：entry.S、start.c、vm.c、kalloc.c 中断、异常、系统调用：trap.c、syscall.c、kernelvec.S、trampoline.S 进程调度与管理：proc.c、swtch.S 文件系统与文件接口：fs.c、file.c、bio.c、log.c 设备驱动：uart.c、console.c、virtio_disk.c、plic.c 内核工具代码：spinlock.c、sleeplock.c、printf.c、string.c 🧾 文件级详细解释（按子系统分类） 🟩 一、内核启动与内存管理 文件 作用 说明 entry.S 核心启动汇编入口 设置页表、跳转到 start() start.c C 语言入口 main() 前做环境初始化（页表、trapframe） main.c 内核主函数 启动内核模块，如 userinit() kernel.ld 内核链接脚本 指定内核段地址、符号导出顺序 vm.c 虚拟内存系统 页表分配、拷贝、映射（基于 Sv39） kalloc.c 物理页分配器 管理空闲页链表，用于 allocuvm memlayout.h 内存布局宏 定义 KERNBASE、PHYSTOP 等常量 📌 建议从 entry.S → start.c → main.c → userinit() 跑一遍流程。 🟦 二、进程管理与调度 文件 作用 说明 proc.c 管理进程表、fork/wait scheduler() 是调度器核心 proc.h 进程结构体定义 结构体 struct proc 包含页表、状态、trapframe swtch.S 上下文切换汇编 保存/恢复寄存器，调度必用 trap.c trap/中断入口 用户 → 内核的总入口；异常调度、syscall 都走它 sysproc.c 与进程相关的系统调用 sys_exit、sys_fork、sys_wait 📌 强烈建议：给 fork()、scheduler()、yield() 加打印观察运行。 🟨 三、系统调用机制 文件 作用 说明 syscall.c syscall 分发器 根据 syscall num 分发到 sys_* syscall.h syscall 编号 用 #define SYS_write 1 等映射 sysfile.c 文件相关 syscall open/close/read/write 的内核实现 usys.pl → usys.S 用户态 syscall 包装 生成用户代码 mov a7, id; ecall 📌 syscall 流程 = 用户态 ecall → trap → syscall.c → sys_*() 🟧 四、文件系统与 I/O 接口 文件 作用 说明 fs.c inode 层 ialloc, readi, writei，文件核心结构 fs.h inode 定义 struct inode，块地址信息等 file.c 文件描述符层 struct file，管理 open/close 等 file.h 文件描述符定义 支持 pipe/dev/inode 等类型 bio.c 缓存块读写 实现 block 级读写缓存 log.c 日志机制 crash-safe 写操作事务（write-ahead logging） pipe.c 管道实现 内存中双向 FIFO fcntl.h, stat.h POSIX 相关头文件 用于 open flag、stat 结构体 📌 建议调试 fs.c 的 namei()、dirlookup()，看路径如何被解析。 🟥 五、设备驱动与中断控制 文件 作用 说明 uart.c 串口驱动 初始化串口，写入字符给终端 console.c 控制台 I/O 与 UART 配合实现 shell 输入输出 plic.c 中断控制器 Platform-Level Interrupt Controller virtio_disk.c 虚拟磁盘驱动 QEMU 虚拟磁盘硬件访问层 virtio.h virtio 设备定义 配套数据结构 📌 virtio_disk.c 调试方法：观察 virtio_rw() 实现的读写逻辑。 🟫 六、工具类 / 内核库函数 文件 作用 说明 defs.h 内核函数声明 extern 所有模块函数，供全局使用 riscv.h RISC-V CSR 宏、寄存器定义 包含 rdtime, csrr, sstatus 等 spinlock.c/.h 自旋锁实现 核心互斥机制，需关中断 sleeplock.c/.h 睡眠锁实现 用于文件系统，sleep/wakeup 管理 string.c libc 实现 memcpy, strlen 等内核自带函数 printf.c 内核级 printf 用于调试打印，无缓冲版 param.h 系统参数宏 定义 NPROC, MAXPATH 等全局参数 types.h 常用类型定义 uchar, uint, sint 等简写 📌 常用 grep 命令：grep -rn &quot;spin_lock&quot; kernel/ 追踪并发点 📁 user/ — 用户态程序与测试 文件 作用 说明 *.c 命令程序 shell 命令如 ls, cat, echo, sh 等 init.c 首个用户进程 userinit() 启动的程序，运行 /init initcode.S 最原始的用户态代码 由 userinit() 载入的程序（汇编） ulib.c libc 函数 用户态的 malloc, printf 等 umalloc.c malloc 实现 用户态堆分配 usertests.c 用户态测试集 测试 syscall、进程、文件功能 user.ld 用户态链接脚本 控制用户程序的段分布 user.h 函数声明 供用户程序引用 printf, fork 等接口 📌 强烈推荐你从 init.c 开始 debug，第一个用户进程的运行关键路径！ 📁 mkfs/ — 构建文件系统镜像工具 文件 说明 mkfs.c 构建 xv6 文件系统镜像（user/init 等文件压入） 📌 不看也无妨，用于 make 阶段构建 fs.img 📄 顶层文件 文件 说明 Makefile 编译入口，构建 kernel, fs.img, qemu 等 README 简要说明文档，讲解如何使用 LICENSE 授权条款（MIT） ✅ 总结 用 模块化思维 分阶段学，比如 “先把 trap 理清楚”，再看 syscall。 推荐搭配如下工具： tmux zsh grep…","link":"/post/xv6-riscv-struct.html"},{"title":"xv6-riscv_ch4","text":"How traps and system calls work on RISC-V. It introduces the trap mechanism, how user programs invoke system calls, how the kernel handles those traps, and how arguments are passed. It also covers kernel-mode traps, page faults, and real-world implications like protection and isolation. ch4: Traps and system calls There are three kinds of event which cause the CPU to set aside ordinary execution of instructions and force a transfer of control to special code that handles the event. and we uses trap as a generic term for these situations. One situation is a systemcall, when a user program executes the ecall instruction to ask the kernel to do something for it. Another situation is an exception: an instruction (user or kernel) does something illegal, such as divide by zero or use an invalid virtual address. The third situation is a device interrupt, when a device signals that it needs attention, for example when the disk hardware finishes a read or write request. Xv6 handles all traps in the kernel; traps are not delivered to user code. Handling traps in the kernel is natural for system calls. It makes sense for interrupts since isolation demands that only the kernel be allowed to use devices, and because the kernel is a convenient mechanism with which to share devices among multiple processes. It also makes sense for exceptions since xv6 responds to all exceptions from user space by killing the offending program. Xv6 trap handling proceeds in four stages: hardware actions taken by the RISC-V CPU, some assembly instructions that prepare the way for kernel C code a C function that decides what to do with the trap and the system call or device-driver service routine. While commonality among the three trap types suggests that a kernel could handle all traps with a single code path, it turns out to be convenient to have separate code for two distinct cases: traps from user space, and traps from kernel space. Kernel code (assembler or C) that processes a trap is often called a handler; the first handler instructions are usually written in assembler (rather than C) and are sometimes called a vector. 4.1 RISC-V trap machinery机制 Each RISC-V CPU has a set of control registers that the kernel writes to tell the CPU how to handle traps, and that the kernel can read to find out about a trap that has occurred. The RISC-V documents contain the full story [3]. riscv.h (kernel/riscv.h:1) contains definitions that xv6 uses. Here’s an outline of the most important registers: stvec: The kernel writes the address of its trap handler here; the RISC-V jumps to the address in stvec to handle a trap. sepc: When a trap occurs, RISC-V saves the program counter here (since the pc is then overwritten with the value in stvec). The sret (return from trap) instruction copies sepc to the pc. The kernel can write sepc to control where sret goes. scause: RISC-V puts a number here that describes the reason for the trap. sscratch: The trap handler code uses sscratch to help it avoid overwriting user registers before saving them. sstatus: The SIE bit in sstatus controls whether device interrupts are enabled. If the kernel clears SIE, the RISC-V will defer device interrupts until the kernel sets SIE. The SPP bit indicates whether a trap came from user mode or supervisor mode, and controls to what mode sret returns. The above registers relate to traps handled in supervisor mode, and they cannot be read or written in user mode. Each CPU on a multi-core chip has its own set of these registers, and more than one CPU may be handling a trap at any given time. When it needs to force a trap, the RISC-V hardware does the following for all trap types: If the trap is a device interrupt, and the sstatus SIE bit is clear, don’t do any of the following. Disable interrupts by clearing the SIE bit in sstatus. Copy the pc to sepc. Save the current mode (user or supervisor) in the SPP bit in sstatus. Set scause to reflect the trap’s cause. Set the mode to supervisor. Copy stvec to the pc. Start executing at the new pc. Note that the CPU doesn’t switch to the kernel page table, doesn’t switch to a stack in the kernel, and doesn’t save any registers other than the pc. Kernel software must perform these tasks. One reason that the CPU does minimal work during a traps is to provide flexibility to software; for example, some operating systems omit a page table switch in some situations to increase trap performance.It’s worth thinking about whether any of the steps listed above could be omitted, perhaps in search of faster traps. Though there are situations in which a simpler sequence can work, many of the steps would be dangerous to omit in general. For example, suppose that the CPU didn’t switch program counters. Then a trap from user space could switch to supervisor mode while still running user instructions. Those user instructions could break user/kernel isolation, for example by modifying the satp register to point to a page table that allowed accessing all of physical memory. It is thus important that the CPU switch to a kernel-specified instruction address, namely stvec. 4.2 Traps from user spaceXv6 handles traps differently depending on whether the trap occurs while executing in the kernelor in user code. Here is the story for traps from user code; Section 4.5 describes traps from kernelcode.A trap may occur while executing in user space if the user program makes a system call (ecallinstruction), or does something illegal, or if a device interrupts. The high-level path of a trap fromuser space is uservec (kernel/trampoline.S:22), then usertrap (kernel/trap.c:37); and when re-turning, usertrapret (kernel/trap.c:90) and then userret (kernel/trampoline.S:101).A major constraint on the design of xv6’s trap handling is the fact that the RISC-V hardwaredoes not switch page tables when it forces a trap. This means that the trap handler address instvec must have a valid mapping in the user page table, since that’s the page table in force whenthe trap handling code starts executing. Furthermore, xv6’s trap handling code needs to switch tothe kernel page table; in order to be able to continue executing after that switch, the kernel pagetable must also have a mapping for the handler pointed to by stvec.Xv6 satisfies these requirements using a trampoline page. The trampoline page contains uservec,the xv6 trap handling code that stvec points to. The trampoline page is mapped in every process’spage table at address TRAMPOLINE, which is at the top of the virtual address space so that it will beabove memory that programs use for themselves. The trampoline page is also mapped at addressTRAMPOLINE in the kernel page table. See Figure 2.3 and Figure 3.3. Because the trampolinepage is mapped in the user page table, traps can start executing there in supervisor mode. Becausethe trampoline page is mapped at the same address in the kernel address space, the trap handlercan continue to execute after it switches to the kernel page table.The code for the uservec trap handler is in trampoline.S (kernel/trampoline.S:22). Whenuservec starts, all 32 registers contain values owned by the interrupted user code. These 32values need to be saved somewhere in memory, so that later on the kernel can restore them beforereturning to user space. Storing to memory requires use of a register to hold the address, but at thispoint there are no general-purpose registers available! Luckily RISC-V provides a helping hand inthe form of the sscratch register. The csrw instruction at the start of uservec saves a0 insscratch. Now uservec has one register (a0) to play with.uservec’s next task is to save the 32 user registers. The kernel allocates, for each process, apage of memory for a trapframe structure that (among other things) has space to save the 32user registers (kernel/proc.h:43). Because satp still refers to the user page table, uservec needsthe trapframe to be mapped in the user address space. Xv6 maps each process’s trapframe at virtualaddress TRAPFRAME in that process’s user page table; TRAPFRAME is just below TRAMPOLINE.The process’s p-&gt;trapframe also points to the trapframe, though at its physical address so thekernel can use it through the kernel page table.Thus uservec loads address TRAPFRAME into a0 and saves all the user registers there,including the user’s a0, read back from sscratch.The trapframe contains the address of the current process’s kernel stack, the current CPU’shartid, the address of the usertrap function, and the address of the kernel page table. uservecretrieves these values, switches satp to the kernel page table, and jumps to usertrap.The job of usertrap is to determine the cause of the trap, process it, and return (kernel/-trap.c:37). It first changes stvec so that a trap while in the kernel will be handled by kernelvecrather than uservec. It saves the sepc register (the saved user program counter), becauseusertrap might call yield to switch to another process’s kernel thread, and that process mightreturn to user space, in the process of which it will modify sepc. If the trap is a system call,usertrap calls syscall to handle it; if a device interrupt, devintr; otherwise it’s an ex-ception, and the kernel kills the faulting process. The system call path adds four to the saved userprogram counter because RISC-V, in the case of a system call, leaves the program pointer pointingto the ecall instruction but user code needs to resume executing at the subsequent instruction.On the way out, usertrap checks if the process has been killed or should yield the CPU (if thistrap is a timer interrupt).The first step in returning to user space is the call to usertrapret (kernel/trap.c:90). Thisfunction sets up the RISC-V control registers to prepare for a future trap from user space: settingstvec to uservec and preparing the trapframe fields that uservec relies on. usertrapretsets sepc to the previously saved user program counter. At the end, usertrapret calls userreton the trampoline page that is mapped in both user and kernel page tables; the reason is that as-sembly code in userret will switch page tables.usertrapret’s call to userret passes a pointer to the process’s user page table in a0(kernel/trampoline.S:101). userret switches satp to the process’s user page table. Recall that theuser page table maps both the trampoline page and TRAPFRAME, but nothing else from the kernel.The trampoline page mapping at the same virtual address in user and kernel page tables allowsuserret to keep executing after changing satp. From this point on, the only data userretcan use is the register contents and the content of the trapframe. userret loads the TRAPFRAMEaddress into a0, restores saved user registers from the trapframe via a0, restores the saved usera0, and executes sret to return to user space. 4.3 Code: Calling system callsChapter 2 ended with initcode.S invoking the exec system call (user/initcode.S:11). Let’s lookat how the user call makes its way to the exec system call’s implementation in the kernel.initcode.S places the arguments for exec in registers a0 and a1, and puts the system callnumber in a7. System call numbers match the entries in the syscalls array, a table of functionpointers (kernel/syscall.c:107). The ecall instruction traps into the kernel and causes uservec,usertrap, and then syscall to execute, as we saw above.syscall (kernel/syscall.c:132) retrieves the system call number from the saved a7 in the trapframeand uses it to index into syscalls. For the first system call, a7 contains SYS_exec (ker-nel/syscall.h:8), resulting in a call to the system call implementation function sys_exec.When sys_exec returns, syscall records its return value in p-&gt;trapframe-&gt;a0. This willcause the original user-space call to exec() to return that value, since the C calling conventionon RISC-V places return values in a0. System calls conventionally return negative numbers toindicate errors, and zero or positive numbers for success. If the system call number is invalid,syscall prints an error and returns −1. 4.4 Code: System call arguments System call implementations in the kernel need to find the arguments passed by user code. Becauseuser code calls system call wrapper functions, the arguments are initially where the RISC-V Ccalling convention places them: in registers. The kernel trap code saves user registers to the currentprocess’s trap frame, where kernel code can find them. The kernel functions argint, argaddr,and argfd retrieve the n ’th system call argument from the trap frame as an integer, pointer, or a filedescriptor. They all call argraw to retrieve the appropriate saved user register (kernel/syscall.c:34).Some system calls pass pointers as arguments, and the kernel must use those pointers to reador write user memory. The exec system call, for example, passes the kernel an array of pointersreferring to string arguments in user space. These pointers pose two challenges. First, the user pro-gram may be buggy or malicious, and may pass the kernel an invalid pointer or a pointer intendedto trick the kernel into accessing kernel memory instead of user memory. Second, the xv6 kernelpage table mappings are not the same as the user page table mappings, so the kernel cannot useordinary instructions to load or store from user-supplied addresses.The kernel implements functions that safely transfer data to and from user-supplied addresses.fetchstr is an example (kernel/syscall.c:25). File system calls such as exec use fetchstr toretrieve string file-name arguments from user space. fetchstr calls copyinstr to do the hardwork.copyinstr (kernel/vm.c:415) copies up to max bytes to dst from virtual address srcva inthe user page table pagetable. Since pagetable is not the current page table, copyinstruses walkaddr (which calls walk) to look up srcva in pagetable, yielding physical addresspa0. The kernel’s page table maps all of physical RAM at virtual addresses that are equal to theRAM’s physical address. This allows copyinstr to directly copy string bytes from pa0 to dst.walkaddr (kernel/vm.c:109) checks that the user-supplied virtual address is part of the process’s user address space, so programs cannot trick the kernel into reading other memory. A similarfunction, copyout, copies data from the kernel to a user-supplied address. 4.5 Traps from kernel spaceXv6 handles traps from kernel code in a different way than traps from user code. When enteringthe kernel, usertrap points stvec to the assembly code at kernelvec (kernel/kernelvec.S:12).Since kernelvec only executes if xv6 was already in the kernel, kernelvec can rely onsatp being set to the kernel page table, and on the stack pointer referring to a valid kernel stack.kernelvec pushes all 32 registers onto the stack, from which it will later restore them so thatthe interrupted kernel code can resume without disturbance.kernelvec saves the registers on the stack of the interrupted kernel thread, which makessense because the register values belong to that thread. This is particularly important if the trapcauses a switch to a different thread – in that case the trap will actually return from the stack of thenew thread, leaving the interrupted thread’s saved registers safely on its stack.kernelvec jumps to kerneltrap (kernel/trap.c:135) after saving registers. kerneltrapis prepared for two types of traps: device interrupts and exceptions. It calls devintr (kernel/-trap.c:185) to check for and handle the former. If the trap isn’t a device interrupt, it must be anexception, and that is always a fatal error if it occurs in the xv6 kernel; the kernel calls panic andstops executing.If kerneltrap was called due to a timer interrupt, and a process’s kernel thread is running(as opposed to a scheduler thread), kerneltrap calls yield to give other threads a chance torun. At some point one of those threads will yield, and let our thread and its kerneltrap resumeagain. Chapter 7 explains what happens in yield.When kerneltrap’s work is done, it needs to return to whatever code was interruptedby the trap. Because a yield may have disturbed sepc and the previous mode in sstatus,kerneltrap saves them when it starts. It now restores those control registers and returns tokernelvec (kernel/kernelvec.S:38). kernelvec pops the saved registers from the stack and ex-ecutes sret, which copies sepc to pc and resumes the interrupted kernel code.It’s worth thinking through how the trap return happens if kerneltrap called yield due toa timer interrupt.Xv6 sets a CPU’s stvec to kernelvec when that CPU enters the kernel from user space;you can see this in usertrap (kernel/trap.c:29). There’s a window of time when the kernel hasstarted executing but stvec is still set to uservec, and it’s crucial that no device interrupt occurduring that window. Luckily the RISC-V always disables interrupts when 4.6 Page-fault exceptions4.7 Real world4.8 Exercises","link":"/post/xv6-riscv-ch4.html"},{"title":"xv6-riscv_ch3","text":"This chapter covers the fundamental concepts of paging hardware, memory allocation, and process address space management, including practical code implementations like creating address spaces, physical memory allocation, and process management functions such as sbrk and exec. ch3: Page tables Page tables are the most popular mechanism through which the operating system provides each process with its own private address space and memory. Xv6 performs a few tricks: mapping the same memory (a trampoline page) in several address spaces, and guarding kernel and user stacks with an unmapped page. The rest of this chapter explains the page tables that the RISC-V hardware provides and how xv6 uses them. 3.1 Paging hardware As a reminder, RISC-V instructions (both user and kernel) manipulate virtual addresses. The machine’s RAM, or physical memory, is indexed with physical addresses. The RISC-V page table hardware connects these two kinds of addresses, by mapping each virtual address to a physical address. Xv6 runs on Sv39 RISC-V, which means that only the bottom 39 bits of a 64-bit virtual address are used; the top 25 bits are not used. In this Sv39 configuration, a RISC-V page table is logically an array of 227 (134,217,728) page table entries (PTEs). Each PTE contains a 44-bit physical page number (PPN) and some flags. The paging hardware translates a virtual address by using the top 27 bits of the 39 bits to index into the page table to find a PTE, and making a 56-bit physical address whose top 44 bits come from the PPN in the PTE and whose bottom 12 bits are copied from the original virtual address. Figure 3.1 shows this process with a logical view of the page table as a simple array of PTEs (see Figure 3.2 for a fuller story). A page table gives the operating system control over virtual-to-physical address translations at the granularity of aligned chunks of 4096 (212 ) bytes. Such a chunk is called a page. In Sv39 RISC-V, the top 25 bits of a virtual address are not used for translation. The physicaladdress also has room for growth: there is room in the PTE format for the physical page numberto grow by another 10 bits. The designers of RISC-V chose these numbers based on technologypredictions. 239 bytes is 512 GB, which should be enough address space for applications running on RISC-V computers. 256 is enough physical memory space for the near future to fit many I/Odevices and RAM chips. If more is needed, the RISC-V designers have defined Sv48 with 48-bitvirtual addresses [3]. As Figure 3.2 shows, a RISC-V CPU translates a virtual address into a physical in three steps.A page table is stored in physical memory as a three-level tree. The root of the tree is a 4096-bytepage-table page that contains 512 PTEs, which contain the physical addresses for page-table pagesin the next level of the tree. Each of those pages contains 512 PTEs for the final level in the tree.The paging hardware uses the top 9 bits of the 27 bits to select a PTE in the root page-table page,the middle 9 bits to select a PTE in a page-table page in the next level of the tree, and the bottom9 bits to select the final PTE. (In Sv48 RISC-V a page table has four levels, and bits 39 through 47of a virtual address index into the top-level.)If any of the three PTEs required to translate an address is not present, the paging hardwareraises a page-fault exception, leaving it up to the kernel to handle the exception (see Chapter 4).The three-level structure of Figure 3.2 allows a memory-efficient way of recording PTEs, com-pared to the single-level design of Figure 3.1. In the common case in which large ranges of virtualaddresses have no mappings, the three-level structure can omit entire page directories. For exam-ple, if an application uses only a few pages starting at address zero, then the entries 1 through 511of the top-level page directory are invalid, and the kernel doesn’t have to allocate pages those for511 intermediate page directories. Furthermore, the kernel also doesn’t have to allocate pages forthe bottom-level page directories for those 511 intermediate page directories. So, in this example,the three-level design saves 511 pages for intermediate page directories and 511 × 512 pages forbottom-level page directories.Although a CPU walks the three-level structure in hardware as part of executing a load or storeinstruction, a potential downside of three levels is that the CPU must load three PTEs from memoryto perform the translation of the virtual address in the load/store instruction to a physical address.To avoid the cost of loading PTEs from physical memory, a RISC-V CPU caches page table entriesin a Translation Look-aside Buffer (TLB).allowed to be used. PTE_V indicates whether the PTE is present: if it is not set, a reference to thepage causes an exception (i.e., is not allowed). PTE_R controls whether instructions are allowedto read to the page. PTE_W controls whether instructions are allowed to write to the page. PTE_Xcontrols whether the CPU may interpret the content of the page as instructions and execute them.PTE_U controls whether instructions in user mode are allowed to access the page; if PTE_U is notset, the PTE can be used only in supervisor mode. Figure 3.2 shows how it all works. The flags andall other page hardware-related structures are defined in (kernel/riscv.h) To tell a CPU to use a page table, the kernel must write the physical address of the root page-table page into the satp register. A CPU will translate all addresses generated by subsequentinstructions using the page table pointed to by its own satp. Each CPU has its own satp so thatdifferent CPUs can run different processes, each with a private address space described by its ownpage table. notice:A few notes about terms used in this book. Physical memory refers to storage cells in RAM.A byte of physical memory has an address, called a physical address. Instructions that dereferenceaddresses (such as loads, stores, jumps, and function calls) use only virtual addresses, which thepaging hardware translates to physical addresses, and then sends to the RAM hardware to read orwrite storage. An address space is the set of virtual addresses that are valid in a given page table; each xv6 process has a separate user address space, and the xv6 kernel has its own address space aswell. User memory refers to a process’s user address space plus the physical memory that the pagetable allows the process to access. Virtual memory refers to the ideas and techniques associatedwith managing page tables and using them to achieve goals such as isolation. 3.2 Kernel address space Xv6 maintains one page table per process, describing each process’s user address space, plus a sin-gle page table that describes the kernel’s address space. The kernel configures the layout of its ad-dress space to give itself access to physical memory and various hardware resources at predictable virtual addresses. Figure 3.3 shows how this layout maps kernel virtual addresses to physical addresses. The file (kernel/memlayout.h) declares the constants for xv6’s kernel memory layout. The kernel gets at RAM and memory-mapped device registers using “direct mapping;” thatis, mapping the resources at virtual addresses that are equal to the physical address. For example,the kernel itself is located at KERNBASE=0x80000000 in both the virtual address space and inphysical memory. Direct mapping simplifies kernel code that reads or writes physical memory. There are a couple of kernel virtual addresses that aren’t direct-mapped: The trampoline page. It is mapped at the top of the virtual address space; user page tables have this same mapping. Chapter 4 discusses the role of the trampoline page, but we see here an interesting use case of page tables; a physical page (holding the trampoline code) is mapped twice in the virtual address space of the kernel: once at top of the virtual address space and once with a direct mapping. The kernel stack pages. Each process has its own kernel stack, which is mapped high so that below it xv6 can leave an unmapped guard page. The guard page’s PTE is invalid (i.e., PTE_V is not set), so that if the kernel overflows a kernel stack, it will likely cause an exception and the kernel will panic. Without a guard page an overflowing stack would overwrite other kernel memory, resulting in incorrect operation. A panic crash is preferable. 3.3 Code: creating an address space Most of the xv6 code for manipulating address spaces and page tables resides in vm.c (kernel/vm.c:1). The central data structure is pagetable_t, which is really a pointer to a RISC-V root page-table page; a pagetable_t may be either the kernel page table, or one of the per-process page tables. The central functions are walk, which finds the PTE for a virtual address,and mappages, which installs PTEs for new mappings. Functions starting with kvm manipulate the kernel page table; functions starting with uvm manipulate a user page table; other functions are used for both. copyout and copyin copy data to and from user virtual addresses provided as system call arguments; they are in vm.c because they need to explicitly translate those addresses in order to find the corresponding physical memory. Early in the boot sequence, main calls kvminit (kernel/vm.c:54) to create the kernel’s page ta-ble using kvmmake (kernel/vm.c:20). This call occurs before xv6 has enabled paging on the RISC-V,so addresses refer directly to physical memory. kvmmake first allocates a page of physical mem-ory to hold the root page-table page. Then it calls kvmmap to install the translations that the kernelneeds. The translations include the kernel’s instructions and data, physical memory up to PHYSTOP,and memory ranges which are actually devices. proc_mapstacks (kernel/proc.c:33) allocates akernel stack for each process. It calls kvmmap to map each stack at the virtual address generatedby KSTACK, which leaves room for the invalid stack-guard pages. kvmmap (kernel/vm.c:132) calls mappages (kernel/vm.c:144), which installs mappings into apage table for a range of virtual addresses to a corresponding range of physical addresses. It doesthis separately for each virtual address in the range, at page intervals. For each virtual address tobe mapped, mappages calls walk to find the address of the PTE for that address. It then initializesthe PTE to hold the relevant physical page number, the desired permissions (PTE_W, PTE_X, and/orPTE_R), and PTE_V to mark the PTE as valid (kernel/vm.c:165). walk (kernel/vm.c:86) mimics the RISC-V paging hardware as it looks up the PTE for a virtualaddress (see Figure 3.2). walk descends the page table one level at a time, using each level’s 9bits of virtual address to index into the relevant page directory page. At each level it finds eitherthe PTE of the next level’s page directory page, or the PTE of final page (kernel/vm.c:92). If a PTEin a first or second level page directory page isn’t valid, then the required directory page hasn’tyet been allocated; if the alloc argument is set, walk allocates a new page-table page and putsits physical address in the PTE. It returns the address of the PTE in the lowest layer in the tree(kernel/vm.c:102). main calls kvminithart (kernel/vm.c:62) to install the kernel page table. It writes the physicaladdress of the root page-table page into the register satp. After this the CPU will translate ad-dresses using the kernel page table. Since the kernel uses a direct mapping, the now virtual addressof the next instruction will map to the right physical memory address. Each RISC-V CPU caches page table entries in a Translation Look-aside Buffer (TLB), andwhen xv6 changes a page table, it must tell the CPU to invalidate corresponding cached TLBentries. If it didn’t, then at some point later the TLB might use an old cached mapping, point-ing to a physical page that in the meantime has been allocated to another process, and as a re-sult, a process might be able to scribble on some other process’s memory. The RISC-V has an instruction sfence.vma that flushes the current CPU’s TLB. Xv6 executes sfence.vma inkvminithart after reloading the satp register, and in the trampoline code that switches to auser page table before returning to user space (kernel/trampoline.S:89).It is also necessary to issue sfence.vma before changing satp, in order to wait for comple-tion of all outstanding loads and stores. This wait ensures that preceding updates to the page tablehave completed, and ensures that preceding loads and stores use the old page table, not the newone.To avoid flushing the complete TLB, RISC-V CPUs may support address space identifiers(ASIDs) [3]. The kernel can then flush just the TLB entries for a particular address space. Xv6does not use this feature. 3.4 Physical memory allocation The kernel must allocate and free physical memory at run-time for page tables, user memory,kernel stacks, and pipe buffers. Xv6 uses the physical memory between the end of the kernel and PHYSTOP for run-time alloca-tion. It allocates and frees whole 4096-byte pages at a time. It keeps track of which pages are freeby threading a linked list through the pages themselves. Allocation consists of removing a pagefrom the linked list; freeing consists of adding the freed page to the list. 3.5 Code: Physical memory allocator The allocator resides in kalloc.c (kernel/kalloc.c:1). The allocator’s data structure is a free listof physical memory pages that are available for allocation. Each free page’s list element is astruct run (kernel/kalloc.c:17). Where does the allocator get the memory to hold that data struc-ture? It store each free page’s run structure in the free page itself, since there’s nothing else storedthere. The free list is protected by a spin lock (kernel/kalloc.c:21-24). The list and the lock arewrapped in a struct to make clear that the lock protects the fields in the struct. For now, ignore thelock and the calls to acquire and release; Chapter 6 will examine locking in detail.The function main calls kinit to initialize the allocator (kernel/kalloc.c:27). kinit initializesthe free list to hold every page between the end of the kernel and PHYSTOP. Xv6 ought to de-termine how much physical memory is available by parsing configuration information providedby the hardware. Instead xv6 assumes that the machine has 128 megabytes of RAM. kinit callsfreerange to add memory to the free list via per-page calls to kfree. A PTE can only refer toa physical address that is aligned on a 4096-byte boundary (is a multiple of 4096), so freerangeuses PGROUNDUP to ensure that it frees only aligned physical addresses. The allocator starts withno memory; these calls to kfree give it some to manage.The allocator sometimes treats addresses as integers in order to perform arithmetic on them(e.g., traversing all pages in freerange), and sometimes uses addresses as pointers to read andwrite memory (e.g., manipulating the run structure stored in each page); this dual use of addressesis the main reason that the allocator code is full of C type casts.The function kfree (kernel/kalloc.c:47) begins by setting every byte in the memory being freedto the value 1. This will cause code that uses memory after freeing it (uses “dangling references”)to read garbage instead of the old valid contents; hopefully that will cause such code to break faster.Then kfree prepends the page to the free list: it casts pa to a pointer to struct run, records theold start of the free list in r-&gt;next, and sets the free list equal to r. kalloc removes and returnsthe first element in the free list. 3.6 Process address space Each process has its own page table, and when xv6 switches between processes, it also changespage tables. Figure 3.4 shows a process’s address space in more detail than Figure 2.3. A process’suser memory starts at virtual address zero and can grow up to MAXVA (kernel/riscv.h:375), allowinga process to address in principle 256 Gigabytes of memory.A process’s address space consists of pages that contain the text of the program (which xv6maps with the permissions PTE_R, PTE_X, and PTE_U), pages that contain the pre-initialized dataof the program, a page for the stack, and pages for the heap. Xv6 maps the data, stack, and heapwith the permissions PTE_R, PTE_W, and PTE_U.Using permissions within a user address space is a common technique to harden a user process.If the text were mapped with PTE_W, then a process could accidentally modify its own program;for example, a programming error may cause the program to write to a null pointer, modifyinginstructions at address 0, and then continue running, perhaps creating more havoc. To detect sucherrors immediately, xv6 maps the text without PTE_W; if a program accidentally attempts to storeto address 0, the hardware will refuse to execute the store and raises a page fault (see Section 4.6).The kernel then kills the process and prints out an informative message so that the developer cantrack down the problem.Similarly, by mapping data without PTE_X, a user program cannot accidentally jump to anaddress in the program’s data and start executing at that address.In the real world, hardening a process by setting permissions carefully also aids in defendingagainst security attacks. An adversary may feed carefully-constructed input to a program (e.g., aWeb server) that triggers a bug in the program in the hope of turning that bug into an exploit [14].Setting permissions carefully and other techniques, such as randomizing of the layout of the useraddress space, make such attacks harder.The stack is a single page, and is shown with the initial contents as created by exec. Stringscontaining the command-line arguments, as well as an array of pointers to them, are at the verytop of the stack. Just under that are values that allow a program to start at main as if the functionmain(argc, argv) had just been called.To detect a user stack overflowing the allocated stack memory, xv6 places an inaccessible guardpage right below the stack by clearing the PTE_U flag. If the user stack overflows and the processtries to use an address below the stack, the hardware will generate a page-fault exception becausethe guard page is inaccessible to a program running in user mode. A real-world operating systemmight instead automatically allocate more memory for the user stack when it overflows.When a process asks xv6 for more user memory, xv6 grows the process’s heap. Xv6 first uses kalloc to allocate physical pages. It then adds PTEs to the process’s page table that point to thenew physical pages. Xv6 sets the PTE_W, PTE_R, PTE_U, and PTE_V flags in these PTEs. Mostprocesses do not use the entire user address space; xv6 leaves PTE_V clear in unused PTEs.We see here a few nice examples of use of page tables. First, different processes’ page tablestranslate user addresses to different pages of physical memory, so that each process has private usermemory. Second, each process sees its memory as having contiguous virtual addresses starting atzero, while the process’s physical memory can be non-contiguous. Third, the kernel maps a pagewith trampoline code at the top of the user address space (without PTE_U), thus a single page ofphysical memory shows up in all address spaces, but can be used only by the kernel. 3.7 Code: sbrk sbrk is the system call for a process to shrink or grow its memory. The system call is implementedby the function growproc (kernel/proc.c:260). growproc calls uvmalloc or uvmdealloc, de-pending on whether n is positive or negative. uvmalloc (kernel/vm.c:233) allocates physical mem-ory with kalloc, zeros the allocated memory, and adds PTEs to the user page table with mappages.uvmdealloc calls uvmunmap (kernel/vm.c:178), which uses walk to find PTEs and kfree tofree the physical memory they refer to.Xv6 uses a process’s page table not just to tell the hardware how to map user virtual addresses, but also as the only record of which physical memory pages are allocated to that process. That is the reason why freeing user memory (in uvmunmap) requires examination of the user page table. 3.8 Code: exec A binary is typically the output of the compiler and linker, and holdsmachine instructions and program data. exec (kernel/exec.c:23) opens the named binary path usingnamei (kernel/exec.c:36), which is explained in Chapter 8. Then, it reads the ELF header. Xv6binaries are formatted in the widely-used ELF format, defined in (kernel/elf.h). An ELF binaryconsists of an ELF header, struct elfhdr (kernel/elf.h:6), followed by a sequence of programsection headers, struct proghdr (kernel/elf.h:25). Each progvhdr describes a section of theapplication that must be loaded into memory; xv6 programs have two program section headers:one for instructions and one for data.The first step is a quick check that the file probably contains an ELF binary. An ELF binarystarts with the four-byte “magic number” 0x7F, ‘E’, ‘L’, ‘F’, or ELF_MAGIC (kernel/elf.h:3). Ifthe ELF header has the right magic number, exec assumes that the binary is well-formed.exec allocates a new page table with no user mappings with proc_pagetable (kernel/exec.c:49),allocates memory for each ELF segment with uvmalloc (kernel/exec.c:65), and loads each segmentinto memory with loadseg (kernel/exec.c:10). loadseg uses walkaddr to find the physical ad-dress of the allocated memory at which to write each page of the ELF segment, and readi to readfrom the file. The program section header for /init, the first user program created with exec, looks like this: 12345678910111213141516# objdump -p user/_init# 告诉操作系统如何将文件的各个段（segment）加载到内存中去执行user/_init: file format elf64-littleProgram Header:0x70000003 off 0x0000000000006bb0 vaddr 0x0000000000000000 paddr 0x0000000000000000 align 2**0 filesz 0x000000000000004a memsz 0x0000000000000000 flags r--LOAD off 0x0000000000001000 vaddr 0x0000000000000000 paddr 0x0000000000000000 align 2**12 filesz 0x0000000000001000 memsz 0x0000000000001000 flags r-xLOAD off 0x0000000000002000 vaddr 0x0000000000001000 paddr 0x0000000000001000 align 2**12 filesz 0x0000000000000010 memsz 0x0000000000000030 flags rw-STACK off 0x0000000000000000 vaddr 0x0000000000000000 paddr 0x0000000000000000 align 2**4 filesz 0x0000000000000000 memsz 0x0000000000000000 flags rw- The output of objdump -p shows the Program Header Table of an ELF (Executable and Linkable Format) file. This is a crucial part of the ELF file, telling the operating system how to load each segment into memory for execution. 🌟 Explanation of ELF Program Header fields — each segment corresponds to the following fields: Field Meaning off Byte offset of the segment in the file, counted from the beginning. vaddr Virtual address of the segment in memory. paddr Physical address — usually ignored by modern OSes. align Alignment requirement; the segment must be aligned to this (often a page size). filesz Size of the segment in the file (in bytes). loadseg reads this many bytes. memsz Total size the segment occupies in memory after loading (may be larger than in the file, e.g., for bss). flags Permission flags: r (read), w (write), x (execute). type Segment type (e.g., LOAD, STACK, NOTE, DYNAMIC, etc.). align Memory alignment requirement. 4 Program Headers (Segments):1. First Segment: Type 0x70000003 (non-standard) 1230x70000003 off 0x0000000000006bb0 vaddr 0x0000000000000000 paddr 0x0000000000000000 align 2**0 filesz 0x000000000000004a memsz 0x0000000000000000 flags r-- type=0x70000003 is processor-specific and non-standard — generally ignored by developers. off=0x6bb0 — the offset within the file. filesz=0x4a — 74 bytes of data. memsz=0x0 — although there is data in the file, nothing is loaded into memory (e.g., debug info, notes). This is typically metadata, not code or data. 2. Second Segment: The actual code segment (text segment) 123LOAD off 0x0000000000001000 vaddr 0x0000000000000000 paddr 0x0000000000000000 align 2**12 filesz 0x0000000000001000 memsz 0x0000000000001000 flags r-x Type: LOAD — to be loaded into memory. off=0x1000 — loading starts from offset 0x1000 in the file. vaddr=0x0 — to be loaded at virtual address 0x0 (important). filesz = memsz = 0x1000 — both file and memory size are 4KB. flags=r-x — readable and executable. Indicates this is the code segment. 🔑 This segment contains program instructions (text segment), and exec/loadseg will load it into memory at 0x0. 3. Third Segment: Data segment 123LOAD off 0x0000000000002000 vaddr 0x0000000000001000 paddr 0x0000000000001000 align 2**12 filesz 0x0000000000000010 memsz 0x0000000000000030 flags rw- Type: LOAD — also needs to be loaded. File offset: 0x2000 Virtual address: 0x1000 filesz = 0x10, memsz = 0x30 — 16 bytes from file, the rest is BSS (needs to be zero-initialized). flags = rw- — read/write permission, indicates this is a data segment. 🔑 This segment holds global variables — part comes from the file, the rest is zeroed out by memset or page initialization. 4. Fourth Segment: Stack segment 123STACK off 0x0000000000000000 vaddr 0x0000000000000000 paddr 0x0000000000000000 align 2**4 filesz 0x0000000000000000 memsz 0x0000000000000000 flags rw- Type: STACK — indicates a stack segment. Usually has no actual content; the stack is manually allocated by the kernel (not loaded from ELF). This is just a marker indicating that the ELF expects a stack. 🔧 Main points in how exec loads ELF in xv6 ELF segment loading Each segment (like .text, .data) specifies a vaddr (virtual address). The kernel reads filesz bytes from the file offset and loads them into memory at vaddr. If memsz &gt; filesz, the remaining space (like .bss) must be zero-initialized. User stack initialization One page of memory is allocated for the user stack; the top stores the argument strings. A “guard page” is placed below the stack and made inaccessible. Arguments are passed to main() via a0 (argc) and a1 (argv). Security checks A malicious ELF file could attempt to set dangerous vaddr values or trigger integer overflows. xv6 performs checks, such as preventing vaddr + memsz overflows. On RISC-V, xv6 uses separate page tables for user and kernel space to prevent interference. ELF loading steps (in exec) The kernel reads the ELF header and locates the Program Headers. For each LOAD-type segment: It calls uvmalloc to allocate memory from vaddr to vaddr + memsz. It uses loadseg to read filesz bytes from the file into that memory. If memsz &gt; filesz, the remaining bytes are cleared to zero. Finally, it creates the user stack, sets up the trap frame and arguments, and jumps to user mode for execution. 3.9 Real world Xv6 is simplified by the kernel’s use of a direct map between virtual and physical addresses, andby its assumption that there is physical RAM at address 0x80000000, where the kernel expects tobe loaded. This works with QEMU, but on real hardware it turns out to be a bad idea; real hardwareplaces RAM and devices at unpredictable physical addresses, so that (for example) there might beno RAM at 0x80000000, where xv6 expect to be able to store the kernel. More serious kerneldesigns exploit the page table to turn arbitrary hardware physical memory layouts into predictablekernel virtual address layouts. RISC-V supports protection at the level of physical addresses, but xv6 doesn’t use that feature. On machines with lots of memory it might make sense to use RISC-V’s support for “superpages.” Small pages make sense when physical memory is small, to allow allocation and page-outto disk with fine granularity. For example, if a program uses only 8 kilobytes of memory, givingit a whole 4-megabyte super-page of physical memory is wasteful. Larger pages make sense onmachines with lots of RAM, and may reduce overhead for page-table manipulation. The xv6 kernel’s lack of a malloc-like allocator that can provide memory for small objectsprevents the kernel from using sophisticated data structures that would require dynamic allocation.A more elaborate kernel would likely allocate many different sizes of small blocks, rather than (asin xv6) just 4096-byte blocks; a real kernel allocator would need to handle small allocations aswell as large ones.","link":"/post/xv6-riscv-ch3.html"}],"tags":[{"name":"Trace","slug":"Trace","link":"/tags/Trace/"},{"name":"interview","slug":"interview","link":"/tags/interview/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"riscv","slug":"riscv","link":"/tags/riscv/"},{"name":"riscv-toolchain","slug":"riscv-toolchain","link":"/tags/riscv-toolchain/"},{"name":"kernel-start","slug":"kernel-start","link":"/tags/kernel-start/"},{"name":"xv6-riscv","slug":"xv6-riscv","link":"/tags/xv6-riscv/"},{"name":"linux-dirvers","slug":"linux-dirvers","link":"/tags/linux-dirvers/"}],"categories":[],"pages":[{"title":"","text":"Troy's Blog >>> 欢迎交换友链~ 请通过邮件联系我。","link":"/friend/index.html"}]}