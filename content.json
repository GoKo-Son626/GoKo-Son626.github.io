{"posts":[{"title":"常见的编译库和编译工具","text":"常见的编译库与编译工具的介绍以及使用场景 1. 编译器 (Compiler) 代码的“翻译官”。将 C/C++ 代码翻译成机器能懂的汇编代码或目标文件 (.o)。 典型代表：GCC, Clang。 使用场景：写任何代码并想让它运行时，第一步就是用编译器进行翻译。 2. 工具链 (Toolchain) 一个完整的“工具箱”。编译器是工具链的核心组件之一**。工具链还包含了链接器 (ld)、汇编器 (as) 等，它们协同工作，将代码和库文件打包成最终的可执行文件。 使用场景： 本地工具链：编译在本机运行的程序 (e.g., gcc)。 交叉工具链：在一种架构（如x86电脑）上，编译给另一种架构（如RISC-V开发板）运行的程序 (e.g., riscv64-linux-gnu-gcc)。 3. libc (C标准库) 一个API标准或规范，不是一个具体的软件。它定义了像 printf, malloc 等基础函数。它是所有C程序的基础依赖。你需要一个具体的实现来使用它。 使用场景：这是一个抽象概念，你写的每一行C代码，只要调用了标准函数，都在与这个“标准”打交道。 4. glibc (GNU C Library) libc 的一种强大、功能全面的实现**。它是为 Linux 操作系统设计的 libc。它不仅包含标准C函数，还包含大量与Linux内核交互的接口（如进程、网络功能）。 使用场景：当你需要开发一个运行在标准Linux系统（如Ubuntu/Debian/CentOS）上的应用程序时，你的程序会链接 glibc。对应的工具链通常叫 ...-linux-gnu-gcc。 5. newlib libc 的一种轻量级、精简的实现**。它是为没有操作系统的环境设计的。因此，它没有 fork 等需要OS支持的复杂功能。 使用场景：开发裸机 (Bare-metal) 程序、固件 (Firmware)、Bootloader，或者在简单的实时操作系统 (RTOS) 上开发。对应的工具链通常叫 ...-elf-gcc。 6. GNU 一个庞大的自由软件生态系统。上面讨论的大部分经典工具都来自GNU项目，包括 GCC (编译器), glibc (C库), GDB (调试器), Make (构建工具)。”GNU Toolchain” 指的就是这一整套工具。 使用场景：Linux 和嵌入式开发的事实标准。 8. ELF (Executable and Linkable Format) 一种文件格式，像 .doc 或 .pdf 一样。它是工具链最终生成的产品。无论是裸机程序还是Linux程序，最终都可以打包成 ELF 格式。 使用场景： 裸机ELF：内部不依赖 glibc，直接在硬件上跑。 Linux ELF：内部依赖 glibc 和Linux内核，必须在Linux系统上跑。 关键：决定它在哪跑的，是它内部链接了什么库，而不是 ELF 这个格式本身。 JIT:(即时编译，Just-In-Time compilation): 动态编译技术，它介于解释执行和提前编译（AOT, Ahead-Of-Time）之间。 解释执行：代码一行行解释运行（如 Python、早期 JavaScript），启动快，但运行慢。 提前编译（AOT）：代码在运行前全部编译成目标机器码（如 C/C++），运行快，但灵活性差。 JIT：在运行时，把中间表示（IR, bytecode）翻译成机器码，并缓存起来，下次直接运行机器码。既能接近原生性能，又保留了灵活性。 JIT 的本质过程：程序一开始以 字节码（或中间表示）形式运行。运行过程中，JIT 编译器发现“这段代码执行得很频繁”（热点代码），于是触发编译。把字节码即时翻译成当前 CPU 架构的机器码（x86、ARM、RISC-V 等）。后续直接执行机器码（免去解释器逐条解释的开销）。 Clang/LLVM: Clang：C/C++ 前端（把 C/C++/Objective-C 源转成通用的标准化的中间语言LLVM IR）。 词法分析：把代码拆成一个个单词（token），比如 int, main, (, ), {, }。 语法分析：检查这些单词组合起来是否符合 C/C++ 的语法规则。如果写了 int main{) 这种错误，Clang 就在这一步报错。 生成中间表示 (IR)：如果语法正确，Clang 会把代码转换成一种通用的、与具体计算机架构无关的格式，这就是 LLVM Intermediate Representation (LLVM IR)。 LLVM：编译器后台/工具链，接收Clang生成的LLVM IR, 进行一系列加工，生成可执行的机器码。 优化 (Optimization)：LLVM 会对 IR 进行大量的优化。比如删除无用的代码、合并重复的计算、展开循环等等，让最终的程序跑得更快、体积更小。这是 LLVM 的核心优势之一。 代码生成 (Code Generation)：这是最关键的一步。LLVM 会根据你指定的 “目标平台 (Target)”，将优化后的 IR 翻译成该平台专属的机器指令。 可移植性：Clang/LLVM 支持很多后端/目标（x86/arm/bpf 等）。当你用 -target bpf 时，Clang 输出的是 eBPF 字节码（虚拟指令），不是 x86 或 arm 机器码。这个字节码理论上可以在任何支持 eBPF 的内核上运行（但内核版本/ABI 细节会影响，CO-RE/BTF 出现就是为了解决这类兼容性问题）。最终在内核里，JIT 会把字节码翻译成当前 CPU 的本地机器码。","link":"/post/Compilation-libraries-and-tools.html"},{"title":"平台总线的结构及框架分析","text":"平台总线是linux系统虚拟出来的一种总线,是一个内核子系统，负责管理 platform_device（硬件描述）和 platform_driver（驱动代码）,使它们先分离.后搭档 平台总线(Platform Bus)(总线控制器信息和控制器驱动之间) 平台总线（Platform Bus）是内核的一条“虚拟”总线。它不像 PCI、USB 那样是物理上存在的总线，而是为了解决一类特殊设备的驱动问题而设计的 软件机制。这类设备通常是 SoC (System on Chip) 芯片内部集成的、不可被自动识别的外设，比如 I2C 控制器、SPI 控制器、GPIO 控制器、LCD 控制器等。 原理：设备与驱动的分离与匹配 (Separation and Matching) 问题： 对于 PCI 或 USB 设备，设备自身带有 ID 信息（Vendor ID, Product ID）。驱动可以根据这些 ID “认领” 设备。但 SoC 上的那些外设，它们的寄存器地址、中断号都是固定的，写死在芯片里了，没法自动发现。 解决： 把 设备信息 和 驱动代码 分开！” 平台设备 (platform_device)： 这是一块纯粹的“数据”，用来描述硬件资源。它告诉内核：“在物理地址 0x12345678 有个设备，它使用中断号 5，它的名字叫 my-i2c-controller”。这些信息通常写在 设备树 (Device Tree, .dts 文件) 中，或者早期的板级配置文件 (board-xxx.c)里。 平台驱动 (platform_driver)： 真正的驱动代码。注册时告诉内核：是一个驱动，我能处理名字叫 my-i2c-controller 的设备。 匹配 (Match)： 当一个 platform_device 和一个 platform_driver 被注册到内核时，平台总线核心会进行匹配。最常见的匹配方式就是看 名字 是否一样。 探测 (Probe)： 一旦匹配成功，总线核心就会调用平台驱动的 .probe 函数。在这个函数里，驱动程序会通过相关的API函数从 platform_device 结构体中获取到设备的硬件资源（如内存地址、中断号），然后用这些信息去初始化硬件，完成驱动的加载。 流程： 系统启动，内核解析设备树。 内核在设备树里读到一段描述 I2C 控制器硬件的节点（包含了寄存器地址、中断号，以及最重要的 compatible = “vendor,i2c-controller-v1”;）。 内核根据这个节点，创建并注册一个 platform_device 到平台总线。 I2C 控制器驱动（platform_driver）在加载时，会告诉平台总线：“我能处理 compatible 是 “vendor,i2c-controller-v1” 的设备”。 平台总线看到两者匹配，于是调用 I2C 控制器驱动的 .probe 函数。 在 I2C 控制器驱动的 .probe 函数中，驱动程序会执行一系列初始化操作，其中最重要的一步是调用 i2c_add_adapter() 或 i2c_add_numbered_adapter()。这个函数调用，才是在内核中“建立”或“注册”了一条 I2C 总线（即一个 i2c_adapter）。这条逻辑上的总线就代表了那条物理的 I2C 总线。内核里的 i2c_adapter 就是物理 I2C 总线在软件层面的抽象。 设备间交互： “其他设备驱动”（比如 I2C 温度传感器驱动）不直接调用 I2C 控制器驱动里的 ops。这是一个分层概念。 正确的交互方式： I2C 控制器驱动把它实现底层 I/O 操作的 ops（struct i2c_algorithm）注册给了 I2C 总线核心。 I2C 温度传感器驱动想通信时，它调用的是 I2C 总线核心提供的标准、统一的 API，如 i2c_master_send() 和 i2c_master_recv()。 I2C 总线核心在收到这些 API 调用后，会找到对应的 i2c_adapter，然后去调用这个 adapter 在注册时提供的 ops 里的具体函数，最终由 I2C 控制器驱动的代码来操作硬件。 platform bus 设备和驱动1. platform_device结构体1234567891011121314151617181920212223struct platform_device { const char *name; // 显示在/sys/bus/platform/devices/name.id(.auto) int id; // 用来区分不同设备：name.id, id = -1: 没有后缀 bool id_auto; // 自动设置id：name.id.auto struct device dev; // 设备的通用属性部分 u64 platform_dma_mask; struct device_dma_parameters dma_parms; u32 num_resources; // 存储的资源的个数 struct resource *resource; // 存储资源 const struct platform_device_id *id_entry; /* * Driver name to force a match. Do not set directly, because core * frees it. Use driver_set_override() to set or clear it. */ const char *driver_override; /* MFD cell pointer */ struct mfd_cell *mfd_cell; /* arch specific additions */ struct pdev_archdata archdata;}; 成员结构体: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123struct device { struct kobject kobj; struct device *parent; struct device_private *p; const char *init_name; /* initial name of the device */ const struct device_type *type; const struct bus_type *bus; /* type of bus device is on */ struct device_driver *driver; /* which driver has allocated this device */ void *platform_data; /* Platform specific data, device core doesn't touch it */ void *driver_data; /* Driver data, set and get with dev_set_drvdata/dev_get_drvdata */ struct mutex mutex; /* mutex to synchronize calls to * its driver. */ struct dev_links_info links; struct dev_pm_info power; struct dev_pm_domain *pm_domain;#ifdef CONFIG_ENERGY_MODEL struct em_perf_domain *em_pd;#endif#ifdef CONFIG_PINCTRL#endif struct dev_pin_info *pins; struct dev_msi_info msi;#ifdef CONFIG_ARCH_HAS_DMA_OPS const struct dma_map_ops *dma_ops;#endif u64 *dma_mask; /* dma mask (if dma'able device) */ u64 coherent_dma_mask;/* Like dma_mask, but for alloc_coherent mappings as not all hardware supports 64 bit addresses for consistent allocations such descriptors. */ u64 bus_dma_limit; /* upstream dma constraint */ const struct bus_dma_region *dma_range_map; struct device_dma_parameters *dma_parms; struct list_head dma_pools; /* dma pools (if dma'ble) */#ifdef CONFIG_DMA_DECLARE_COHERENT struct dma_coherent_mem *dma_mem; /* internal for coherent mem override */#endif#ifdef CONFIG_DMA_CMA struct cma *cma_area; /* contiguous memory area for dma allocations */#endif#ifdef CONFIG_SWIOTLB struct io_tlb_mem *dma_io_tlb_mem;#endif#ifdef CONFIG_SWIOTLB_DYNAMIC struct list_head dma_io_tlb_pools; spinlock_t dma_io_tlb_lock; bool dma_uses_io_tlb;#endif /* arch specific additions */ struct dev_archdata archdata; struct device_node *of_node; /* associated device tree node */ struct fwnode_handle *fwnode; /* firmware device node */#ifdef CONFIG_NUMA int numa_node; /* NUMA node this device is close to */#endif dev_t devt; /* dev_t, creates the sysfs &quot;dev&quot; */ u32 id; /* device instance */ spinlock_t devres_lock; struct list_head devres_head; const struct class *class; const struct attribute_group **groups; /* optional groups */ void (*release)(struct device *dev); // 必须编写，不然驱动编译不过去 struct iommu_group *iommu_group; struct dev_iommu *iommu; struct device_physical_location *physical_location; enum device_removable removable; bool offline_disabled:1; bool offline:1; bool of_node_reused:1; bool state_synced:1; bool can_match:1;#if defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_DEVICE) || \\ defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU) || \\ defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU_ALL) bool dma_coherent:1;#endif#ifdef CONFIG_DMA_OPS_BYPASS bool dma_ops_bypass : 1;#endif#ifdef CONFIG_DMA_NEED_SYNC bool dma_skip_sync:1;#endif#ifdef CONFIG_IOMMU_DMA bool dma_iommu:1;#endif};/* * Resources are tree-like, allowing * nesting etc.. */struct resource { resource_size_t start; // 资源的起始信息和终止信息 resource_size_t end; // etc：中断的起始地址和终止地址 const char *name; // 存储信息名称：etc：中断-irq unsigned long flags; // 存储资源类型：etc: IORESOURCE_IO/MEM/REG/IRQ/DMA/BUS... unsigned long desc; // 描述信息 struct resource *parent, *sibling, *child; // 节点相关}; 2. 简单的platform_device insmod注册成功：ls /sys/bus/platform/devices/mydevice 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#include &lt;linux/module.h&gt;#include &lt;linux/init.h&gt;#include &lt;linux/platform_device.h&gt;static struct resource mydevice_resource[] = { [0] = { .start = 0xFDD60000, .end = 0xFDD50004, .flags = IORESOURCE_IO, }, [1] = { .start = 13, .end = 13, .flags = IORESOURCE_IRQ, },}void mydevice_release(struct device *dev){ printk(&quot;This is mydevice_release\\n&quot;);}static platform_device platform_device_test = { .name = &quot;mydevice&quot;, .id = -1, .resource = mydevice_resource, .num_resources = ARRAY_SIZE(mydevice_resource), .dev = { .release = mydevice_release, },};static int platform_device_init(void){ platform_device_register(&amp;platform_device_test); printk(&quot;platform_device init\\n&quot;); return 0;}static int platform_device_exit(void){ platform_device_unregister(&amp;platform_device_test); printk(&quot;platform_device exit\\n&quot;); return 0;}module init(platform_device_init);module exit(platform_device_exit);MODULE_LICENSE(&quot;GPL&quot;);MODULE_AUTHOR(&quot;goko&quot;);MODULE_VERSION(&quot;V1.0&quot;); 3. platform_driver结构体123456789101112131415161718struct platform_driver { int (*probe)(struct platform_device *); // 匹配成功之后执行 void (*remove)(struct platform_device *); // 设备移除时执行 void (*shutdown)(struct platform_device *); // 设备关闭时执行dy int (*suspend)(struct platform_device *, pm_message_t state); // 设备挂起时执行dy int (*resume)(struct platform_device *); // 设备恢复时执行dy struct device_driver driver; // 设备公用的一些属性 const struct platform_device_id *id_table; // 设备id表 bool prevent_deferred_probe; /* * For most device drivers, no need to care about this flag as long as * all DMAs are handled through the kernel DMA API. For some special * ones, for example VFIO drivers, they know how to manage the DMA * themselves and set this flag so that the IOMMU layer will allow them * to setup and manage their own I/O address space. */ bool driver_managed_dma;}; 4. 简单的platform_driver insmod注册成功：ls /sys/bus/platform/drivers/mydevice 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287#include &lt;linux/module.h&gt; // 所有模块都需要#include &lt;linux/init.h&gt; // __init 和 __exit 宏#include &lt;linux/fs.h&gt; // file_operations 结构体和文件系统相关函数#include &lt;linux/cdev.h&gt; // cdev 结构体和相关函数#include &lt;linux/uaccess.h&gt; // copy_to_user, copy_from_user#include &lt;linux/device.h&gt; // class_create, device_create#include &lt;linux/io.h&gt; // ioremap, iounmap#include &lt;linux/platform_device.h&gt; // platform_driver 和 platform_device#include &lt;linux/of.h&gt; // of_match_ptr, 设备树相关（如果使用设备树匹配）#include &lt;linux/slab.h&gt; // kzalloc, kfree#define DRIVER_NAME &quot;my_platform_device&quot; // 定义驱动名称#define DEVICE_COUNT 1 // 定义设备数量// 驱动的私有数据结构体，用于存储设备相关的所有信息// 这个结构体整合了你截图中`struct device_test`的所有成员struct mydevice_dev { dev_t dev_num; // 设备号 (主设备号 + 次设备号) struct cdev cdev_test; // 字符设备结构体 struct class *class; // 设备类，用于在/sys/class/下创建条目 struct device *device; // 设备实例，用于在/dev/下创建设备文件 char kbuf[32]; // 内核缓冲区，用于与用户空间交换数据 void __iomem *vir_gpio_dr; // 经过ioremap映射后的虚拟地址};// 全局指针，指向我们的私有数据结构体// 在probe中分配，在remove中释放// 注意：更好的做法是通过 platform_set_drvdata/platform_get_drvdata 来管理，这里为了清晰展示，先用一个全局指针// 稍后会展示更标准的做法struct mydevice_dev *global_mydev; // --- 文件操作函数集 (file_operations) ---static int mydevice_open(struct inode *inode, struct file *file){ struct mydevice_dev *dev; printk(KERN_INFO &quot;mydevice: device opened\\n&quot;); // 通过 inode 中的 cdev 指针，找到包含它的父结构体 mydevice_dev // 这是内核中非常常见和重要的技巧 dev = container_of(inode-&gt;i_cdev, struct mydevice_dev, cdev_test); // 将设备私有结构体的指针存放在 file-&gt;private_data 中 // 这样，在后续的 read/write/release 操作中，就可以直接从 file 中获取，无需再次查找 file-&gt;private_data = dev; return 0;}static int mydevice_release(struct inode *inode, struct file *file){ printk(KERN_INFO &quot;mydevice: device closed\\n&quot;); // 这里不需要释放 file-&gt;private_data，因为它指向的是在 probe 中分配的内存 // 该内存的生命周期与驱动绑定，而不是与文件的打开/关闭绑定 return 0;}static ssize_t mydevice_read(struct file *file, char __user *buf, size_t size, loff_t *off){ // 从 file-&gt;private_data 中获取设备私有结构体指针 struct mydevice_dev *dev = file-&gt;private_data; size_t len = strlen(dev-&gt;kbuf); int ret; printk(KERN_INFO &quot;mydevice: reading data: %s\\n&quot;, dev-&gt;kbuf); if (size &gt; len) { size = len; } // 将内核空间的数据 (dev-&gt;kbuf) 拷贝到用户空间 (buf) ret = copy_to_user(buf, dev-&gt;kbuf, size); if (ret != 0) { printk(KERN_ERR &quot;mydevice: copy_to_user failed\\n&quot;); return -EFAULT; // 返回一个标准的错误码 } // 在这里，一个简单的实现是每次读取后返回已读取的字节数 // 一个更完整的实现需要处理 *off，以支持多次读取文件的不同部分 return size;}static ssize_t mydevice_write(struct file *file, const char __user *buf, size_t size, loff_t *off){ // 从 file-&gt;private_data 中获取设备私有结构体指针 struct mydevice_dev *dev = file-&gt;private_data; int ret; if (size &gt;= sizeof(dev-&gt;kbuf)) { printk(KERN_WARNING &quot;mydevice: write size is too large\\n&quot;); // 截断写入的数据，防止缓冲区溢出 size = sizeof(dev-&gt;kbuf) - 1; } // 将用户空间的数据 (buf) 拷贝到内核空间 (dev-&gt;kbuf) ret = copy_from_user(dev-&gt;kbuf, buf, size); if (ret != 0) { printk(KERN_ERR &quot;mydevice: copy_from_user failed\\n&quot;); return -EFAULT; } // 给内核缓冲区加上字符串结束符 dev-&gt;kbuf[size] = '\\0'; printk(KERN_INFO &quot;mydevice: written data: %s\\n&quot;, dev-&gt;kbuf); // 在一个真实的GPIO驱动中，这里会解析 kbuf 中的命令（如&quot;on&quot;或&quot;off&quot;） // 然后通过 dev-&gt;vir_gpio_dr 指针向硬件寄存器写入值 // 例如：iowrite32(1, dev-&gt;vir_gpio_dr); return size; // 返回成功写入的字节数}// 定义 file_operations 结构体，并将我们的函数与之关联static const struct file_operations mydevice_fops = { .owner = THIS_MODULE, .open = mydevice_open, .release = mydevice_release, .read = mydevice_read, .write = mydevice_write,};// --- Platform 驱动核心函数 ---// 当内核匹配到同名的 platform_device 时，会调用此 probe 函数static int mydriver_probe(struct platform_device *pdev){ int ret; struct resource *mem_res; struct mydevice_dev *dev; printk(KERN_INFO &quot;mydriver_probe: device probed!\\n&quot;); // 1. 分配私有数据结构体内存 // 使用 devm_kzalloc, &quot;devm_&quot; 开头的函数是受设备管理的，当设备卸载时会自动释放资源，非常方便 dev = devm_kzalloc(&amp;pdev-&gt;dev, sizeof(struct mydevice_dev), GFP_KERNEL); if (!dev) { return -ENOMEM; } global_mydev = dev; // 赋值给全局指针（仅为示例） // 2. 从 platform_device 获取资源 (这里以内存资源为例) // 参数: platform_device指针, 资源类型, 索引(第0个内存资源) mem_res = platform_get_resource(pdev, IORESOURCE_MEM, 0); if (!mem_res) { printk(KERN_ERR &quot;mydriver: failed to get memory resource\\n&quot;); return -EINVAL; } printk(KERN_INFO &quot;mydriver: mem resource start: 0x%pa, size: %lld\\n&quot;, &amp;mem_res-&gt;start, resource_size(mem_res)); // 3. 将物理地址映射到内核虚拟地址空间 // devm_ioremap_resource 会自动处理 ioremap 和 iounmap，非常推荐使用 dev-&gt;vir_gpio_dr = devm_ioremap_resource(&amp;pdev-&gt;dev, mem_res); if (IS_ERR(dev-&gt;vir_gpio_dr)) { printk(KERN_ERR &quot;mydriver: failed to ioremap memory resource\\n&quot;); return PTR_ERR(dev-&gt;vir_gpio_dr); } // ======== 以下是字符设备创建的标准流程 (来自你的截图逻辑) ======== // 4. 动态申请设备号 ret = alloc_chrdev_region(&amp;dev-&gt;dev_num, 0, DEVICE_COUNT, DRIVER_NAME); if (ret &lt; 0) { printk(KERN_ERR &quot;mydriver: failed to allocate chrdev region\\n&quot;); return ret; } printk(KERN_INFO &quot;mydriver: allocated major=%d, minor=%d\\n&quot;, MAJOR(dev-&gt;dev_num), MINOR(dev-&gt;dev_num)); // 5. 初始化 cdev 结构体，并绑定 file_operations cdev_init(&amp;dev-&gt;cdev_test, &amp;mydevice_fops); dev-&gt;cdev_test.owner = THIS_MODULE; // 6. 将 cdev 添加到内核中 ret = cdev_add(&amp;dev-&gt;cdev_test, dev-&gt;dev_num, DEVICE_COUNT); if (ret &lt; 0) { printk(KERN_ERR &quot;mydriver: failed to add cdev\\n&quot;); goto err_unregister_chrdev; } // 7. 创建设备类 /sys/class/my_platform_device dev-&gt;class = class_create(THIS_MODULE, DRIVER_NAME); if (IS_ERR(dev-&gt;class)) { ret = PTR_ERR(dev-&gt;class); printk(KERN_ERR &quot;mydriver: failed to create class\\n&quot;); goto err_cdev_del; } // 8. 创建设备文件 /dev/my_platform_device dev-&gt;device = device_create(dev-&gt;class, NULL, dev-&gt;dev_num, NULL, DRIVER_NAME); if (IS_ERR(dev-&gt;device)) { ret = PTR_ERR(dev-&gt;device); printk(KERN_ERR &quot;mydriver: failed to create device\\n&quot;); goto err_class_destroy; } // 9. 将私有数据结构体指针与 platform_device 关联 // 这样在 remove 函数中就可以通过 platform_get_drvdata 获取它 platform_set_drvdata(pdev, dev); // 初始化内核缓冲区 strcpy(dev-&gt;kbuf, &quot;Hello from kernel!&quot;); printk(KERN_INFO &quot;mydriver: probe successful, device created at /dev/%s\\n&quot;, DRIVER_NAME); return 0;// 错误处理：按相反的顺序释放已申请的资源err_class_destroy: class_destroy(dev-&gt;class);err_cdev_del: cdev_del(&amp;dev-&gt;cdev_test);err_unregister_chrdev: unregister_chrdev_region(dev-&gt;dev_num, DEVICE_COUNT); // devm_kzalloc 和 devm_ioremap_resource 分配的资源会自动释放，无需手动处理 return ret;}// 当驱动被卸载或设备被移除时，调用此 remove 函数static int mydriver_remove(struct platform_device *pdev){ // 通过 platform_get_drvdata 获取在 probe 中设置的私有数据 struct mydevice_dev *dev = platform_get_drvdata(pdev); printk(KERN_INFO &quot;mydriver_remove: removing device\\n&quot;); // 按照与 probe 相反的顺序销毁和释放资源 // 注意：devm_ 家族函数管理的资源（内存、ioremap）不需要在这里手动释放！ // 驱动核心会在这个函数返回后自动清理它们。 // 销毁设备文件 /dev/my_platform_device device_destroy(dev-&gt;class, dev-&gt;dev_num); // 销毁设备类 /sys/class/my_platform_device class_destroy(dev-&gt;class); // 从内核中删除 cdev cdev_del(&amp;dev-&gt;cdev_test); // 注销设备号 unregister_chrdev_region(dev-&gt;dev_num, DEVICE_COUNT); printk(KERN_INFO &quot;mydriver_remove: remove successful\\n&quot;); return 0;}// ID 表，用于匹配 platform_device// 当一个 platform_device 的 .name 字段与这里的 .name 匹配时，probe 就会被调用static const struct platform_device_id mydriver_id_table[] = { { .name = &quot;my-platform-device-example&quot; }, // 这个名字需要与 platform_device 注册时使用的名字完全一致 { /* sentinel */ }, // 结尾的空条目，表示列表结束};MODULE_DEVICE_TABLE(platform, mydriver_id_table); // 将id_table导出，让内核和用户空间知道// 定义 platform_driver 结构体static struct platform_driver my_platform_driver = { .probe = mydriver_probe, .remove = mydriver_remove, .driver = { .name = &quot;my-platform-device-example&quot;, // 驱动的名字 .owner = THIS_MODULE, }, .id_table = mydriver_id_table, // 关联ID匹配表};// 模块加载函数static int __init my_driver_init(void){ printk(KERN_INFO &quot;my_driver_init: Registering platform driver\\n&quot;); // 注册 platform_driver 到内核 return platform_driver_register(&amp;my_platform_driver);}// 模块卸载函数static void __exit my_driver_exit(void){ printk(KERN_INFO &quot;my_driver_exit: Unregistering platform driver\\n&quot;); // 从内核中注销 platform_driver platform_driver_unregister(&amp;my_platform_driver);}// 注册模块加载和卸载函数module_init(my_driver_init);module_exit(my_driver_exit);// 模块许可和信息MODULE_LICENSE(&quot;GPL&quot;);MODULE_AUTHOR(&quot;Your Name&quot;);MODULE_DESCRIPTION(&quot;A complete platform device driver example&quot;);MODULE_VERSION(&quot;1.0&quot;); 平台总线和设备驱动 启动与准备 (第 0 步): 内核启动，解析设备树。 它看到一个描述 I2C 控制器的节点，于是创建了一个 platform_device。 它看到 I2C 控制器节点下还有一个描述温度传感器的子节点，于是为它创建了一个 i2c_client 的描述信息（但此时还未注册，因为 I2C 总线还不存在）。 第一层匹配 (平台总线): 你加载了 I2C 控制器驱动 (platform_driver)。 平台总线发现这个驱动和之前创建的 platform_device 匹配。 平台总线调用 I2C 控制器驱动 的 .probe() 函数。 桥梁搭建 (控制器驱动的工作): 在 I2C 控制器驱动 的 .probe() 函数中，驱动初始化了硬件，然后调用 i2c_add_adapter()。 这个调用是关键！ 它在内核里创建并注册了一条功能完备的 I2C 总线。 第二层匹配 (I2C 总线): 新的 I2C 总线被注册后，内核的 I2C 核心会把之前为温度传感器准备的 i2c_client 描述信息，正式注册到这条新的 I2C 总线上。 现在，你加载了温度传感器驱动 (i2c_driver)。 I2C 总线发现这个驱动和刚刚注册的 i2c_client 匹配。 I2C 总线调用温度传感器驱动的 .probe() 函数。 最终通信 (设备驱动的工作): 在温度传感器驱动的 .probe() 或其他函数里，它想读取温度。 它调用一个标准的、与硬件无关的函数 i2c_master_recv()。 I2C 核心收到调用，查找该设备挂在哪条 I2C 总线上。 它找到了由I2C 控制器驱动注册的那条总线，然后调用了该控制器驱动提供的底层传输函数。 I2C 控制器驱动的代码开始执行，通过操作寄存器来命令物理 I2C 控制器去和物理温度传感器通信，并取回数据。","link":"/post/Platform-bus.html"},{"title":"利用 hexo 搭建博客","text":"使用hexo和GitHub Pagtes部署一个自己的博客 1. 安装并初始化Hexo 安装 Hexo CLI 1npm install -g hexo-cli 初始化博客项目目录 123mkdir my-blog &amp;&amp; cd my-bloghexo initnpm install 本地预览 1hexo server 启动本地服务：在浏览器访问 http://localhost:4000 查看效果 2. 配置 GitHub Pages 部署 创建GitHub仓库 创建一个仓库，名字叫 你的GitHub用户名.github.io 比如你是 goko，就叫 goko.github.io 安装部署插件 1npm install hexo-deployer-git --save 修改 _config.yml（根目录下）添加部署配置： 12345deploy: type: git # repo建议使用SSH, SSH免密 repo: https://github.com/你的GitHub用户名/你的GitHub用户名.github.io.git branch: main # 或者 master，看你的默认分支 生成并部署博客 123hexo cleanhexo generatehexo deploy 3. 域名(.com)绑定 添加域名(在my-blog下) 123echo &quot;&lt;xxxx&gt;.com&quot; &gt; source/CNAME# 或者可以：echo &quot;www.&lt;xxxx&gt;.com&quot; &gt; source/CNAME# 只能添加一个，而且两个需要添加不同的域名解析（如下） 重新部署 1hexo clean &amp;&amp; hexo g &amp;&amp; hexo d 设置 DNS 解析指向 GitHub Pages A. 使用裸域名（apex 域名）goku72.com 记录类型 主机记录 记录值 说明 A @ 185.199.108.153 GitHub Pages IP A @ 185.199.109.153 GitHub Pages IP A @ 185.199.110.153 GitHub Pages IP A @ 185.199.111.153 GitHub Pages IP example aliyun: 选择业务需求: 将网站域名解析到服务器IPv4地址 选择网站域名(主机记录): .com（对应设置“@”主机记录） 填写 IP（记录值）： 在输入框里粘贴以下四行（每一行一个 IP）： &gt; 185.199.109.153 &gt; 185.199.108.153 &gt; 185.199.110.153 &gt; 185.199.111.153 B. 使用 www.goku72.com 作为主域名 记录类型 主机记录 记录值 说明 CNAME www &lt;github用户名&gt;.github.io. 指向你的 GitHub 用户页仓库 example aliyun: 选择业务需求: 将网站域名解析到另外的目标域名 选择网站域名(主机记录): www..com（对应设置“www”主机记录） 填写 IP（记录值）：&lt;github用户名&gt;.github.io. (最有有一个符号”.”) 4. 设置主题 cd my-blog/themes git clone -b master https://github.com/jerryc127/hexo-theme-butterfly.git butterfly 修改_config.yml: theme: butterfly hexo clean &amp;&amp; hexo g &amp;&amp; hexo d 更多主题：https://hexo.io/themes/ 注： 如果AB两个方式都添加了，只需要在 Hexo 项目的 source/CNAME 文件中写 www..com，GitHub Pages 就会自动把 goku72.com 重定向过去，无需额外设置！ 后续换域名只需要：阿里云重新解析 + 修改 source/CNAME + 重新部署 Hexo，就能完成域名迁移。 有些主题可能需要下载插件","link":"/post/hexo-blog.html"},{"title":"Linux bpf技术解析及libbpf的使用(基于riscv-k1)","text":"了解ebpf并在riscv平台上支持ebpf, 最后理解并使用libbpf库中的示例 BPF 和 eBPF BPF （extened Berkeley Packet Filter）: BPF提供了一种在各种内核事件和应用程序事件发生时运行一小段程序的机制。该技术将内核变成完全可编程，允许用户定制和控制它们的系统，以解决现实问题。提供了一套内核接口/bpf() 系统调用集合。 BPF是一项灵活而高效的技术，由指令集、存储对象和辅助函数等几部分组成。由于它采用了虚拟指令集规范，因此也可将它视作一种虚拟机实现。这些指令由Linux内核的BPF运行时模块执行。 eBPF（extened Berkeley Packet Filter）: 扩展版 BPF（64-bit regs、更多指令、复杂验证、maps、helper 函数、更广泛用途）。现在说 BPF/ eBPF 基本都指 eBPF。 原理 eBPF 的工作原理主要分为三个步骤：加载、编译和执行。 eBPF 需要在内核中运行。这通常是由用户态的应用程序完成的，它会通过系统调用来加载 eBPF 程序。在加载过程中，内核会将 eBPF 程序的代码复制到内核空间。 eBPF 程序需要经过编译和执行。这通常是由Clang/LLVM的编译器完成，然后形成字节码后，将用户态的字节码装载进内核，Verifier会对要注入内核的程序进行一些内核安全机制的检查,这是为了确保 eBPF 程序不会破坏内核的稳定性和安全性。在检查过程中，内核会对 eBPF 程序的代码进行分析，以确保它不会进行恶意操作，如系统调用、内存访问等。如果 eBPF 程序通过了内核安全机制的检查，它就可以在内核中正常运行了，其会通过通过一个JIT编译步骤将程序的通用字节码转换为机器特定指令集，以优化程序的执行速度。 JIT:(即时编译，Just-In-Time compilation): 动态编译技术，它介于解释执行和提前编译（AOT, Ahead-Of-Time）之间。 解释执行：代码一行行解释运行（如 Python、早期 JavaScript），启动快，但运行慢。 提前编译（AOT）：代码在运行前全部编译成目标机器码（如 C/C++），运行快，但灵活性差。 JIT：在运行时，把中间表示（IR, bytecode）翻译成机器码，并缓存起来，下次直接运行机器码。既能接近原生性能，又保留了灵活性。 JIT 的本质过程：程序一开始以 字节码（或中间表示）形式运行。运行过程中，JIT 编译器发现“这段代码执行得很频繁”（热点代码），于是触发编译。把字节码即时翻译成当前 CPU 架构的机器码（x86、ARM、RISC-V 等）。后续直接执行机器码（免去解释器逐条解释的开销）。 Clang/LLVM: Clang：C/C++ 前端（把 C/C++/Objective-C 源转成通用的标准化的中间语言LLVM IR）。 词法分析：把代码拆成一个个单词（token），比如 int, main, (, ), {, }。 语法分析：检查这些单词组合起来是否符合 C/C++ 的语法规则。如果写了 int main{) 这种错误，Clang 就在这一步报错。 生成中间表示 (IR)：如果语法正确，Clang 会把代码转换成一种通用的、与具体计算机架构无关的格式，这就是 LLVM Intermediate Representation (LLVM IR)。 LLVM：编译器后台/工具链，接收Clang生成的LLVM IR, 进行一系列加工，生成可执行的机器码。 优化 (Optimization)：LLVM 会对 IR 进行大量的优化。比如删除无用的代码、合并重复的计算、展开循环等等，让最终的程序跑得更快、体积更小。这是 LLVM 的核心优势之一。 代码生成 (Code Generation)：这是最关键的一步。LLVM 会根据你指定的 “目标平台 (Target)”，将优化后的 IR 翻译成该平台专属的机器指令。 可移植性：Clang/LLVM 支持很多后端/目标（x86/arm/bpf 等）。当你用 -target bpf 时，Clang 输出的是 eBPF 字节码（虚拟指令），不是 x86 或 arm 机器码。这个字节码理论上可以在任何支持 eBPF 的内核上运行（但内核版本/ABI 细节会影响，CO-RE/BTF 出现就是为了解决这类兼容性问题）。最终在内核里，JIT 会把字节码翻译成当前 CPU 的本地机器码。 所以，Clang/LLVM 实现了第一层可移植性（源码 -&gt; eBPF 字节码），而内核的 JIT 编译器实现了第二层可移植性（eBPF 字节码 -&gt; 具体 CPU 机器码）。 ebpf结构图 用户空间程序与内核中的 BPF 字节码交互的流程主要如下： 我们可以使用 LLVM 或者 GCC 工具将编写的 BPF 代码程序编译成 BPF 字节码； 然后使用加载程序 Loader 将字节码加载至内核；内核使用验证器（verfier） 组件保证执行字节码的安全性，以避免对内核造成灾难，在确认字节码安全后将其加载对应的内核模块执行；BPF 观测技术相关的程序程序类型可能是 kprobes/uprobes/tracepoint/perf_events 中的一个或多个，其中：kprobes：实现内核中动态跟踪。kprobes 可以跟踪到 Linux 内核中的导出函数入口或返回点，但是不是稳定 ABI 接口，可能会因为内核版本变化导致，导致跟踪失效。uprobes：用户级别的动态跟踪。与 kprobes 类似，只是跟踪用户程序中的函数。tracepoints：内核中静态跟踪。tracepoints 是内核开发人员维护的跟踪点，能够提供稳定的 ABI 接口，但是由于是研发人员维护，数量和场景可能受限。perf_events：定时采样和 PMC。 内核中运行的 BPF 字节码程序可以使用两种方式将测量数据回传至用户空间 maps 方式可用于将内核中实现的统计摘要信息（比如测量延迟、堆栈信息）等回传至用户空间； perf-event 用于将内核采集的事件实时发送至用户空间，用户空间程序实时读取分析； 用途和优势 用途 网络监控：eBPF 可以用于捕获网络数据包，并执行特定的逻辑来分析网络流量。例如，可以使用 eBPF 程序来监控网络流量，并在发现异常流量时进行警报。 安全过滤：eBPF 可以用于对网络数据包进行安全过滤。例如，可以使用 eBPF 程序来阻止恶意流量的传播，或者在发现恶意流量时对其进行拦截。 性能分析：eBPF 可以用于对内核的性能进行分析。例如，可以使用 eBPF 程序来收集内核的性能指标，并通过特定的接口将其可视化。这样，可以更好地了解内核的性能瓶颈，并进行优化。 虚拟化：eBPF 可以用于虚拟化技术。例如，可以使用 eBPF 程序来收集虚拟机的性能指标，并进行负载均衡。这样，可以更好地利用虚拟化环境的资源，提高系统的性能和稳定性。 优势 安全：验证器确保了任何 eBPF 程序都不会搞垮内核。 可移植：同一份 eBPF 字节码，理论上可以在任何架构（x86, ARM, RISC-V）的 Linux 内核上运行，因为 JIT(见下文) 会为它们翻译出对应的原生机器码。 高性能：因为 JIT 的存在，最终运行的是原生机器码，速度接近于原生内核代码。 在k1上安装并配置ebpf的环境和工具 烧录官方提供的bianbu镜像v3.0 但是启动后发现：1234567891011121314151617root@k1:/sys/kernel# cat /boot/config-6.1.15 | grep BPFCONFIG_BPF=yCONFIG_HAVE_EBPF_JIT=yBPF subsystemCONFIG_BPF_SYSCALL=yCONFIG_BPF_JIT is not set # JIT 编译器被关闭 (JIT Compiler is Disabled)CONFIG_BPF_UNPRIV_DEFAULT_OFF=yend of BPF subsystemCONFIG_CGROUP_BPF=yCONFIG_NETFILTER_XT_MATCH_BPF=yCONFIG_BPFILTER is not setCONFIG_NET_CLS_BPF is not set # 缺少网络核心功能CONFIG_BPF_STREAM_PARSER is not setCONFIG_NBPFAXI_DMA is not setCONFIG_BPF_EVENTS=yroot@k1:/sys/kernel# grep BTF /boot/config-x.x.xCONFIG_DEBUG_INFO_BTF=y # 内核没有编译进 BTF (BPF Type Format) 信息 编译源码替换内核镜像 备份旧文件 123cd /bootcp vmlinuz-6.6.63 vmlinuz-6.6.63.bakcp spacemit/6.6.63/k1-x_evb.dtb spacemit/6.6.63/k1-x_evb.dtb.bak vmlinuz只有一个，但是dtb有多个，需要确定是哪一个 这里查看dtb和model输出 1234567891011root@k1:~# ls /boot/spacemit/6.6.63/k1-x_baton-camera.dtb k1-x_fpga.dtb k1-x_MUSE-Card.dtb k1-x_som.dtbk1-x_bit-brick.dtb k1-x_FusionOne.dtb k1-x_MUSE-N1.dtb k1-x_uav.dtbk1-x_deb1.dtb k1-x_InnoBoard-Pi.dtb k1-x_MUSE-Paper2.dtb k1-x_ZT001H.dtbk1-x_deb2.dtb k1-x_lpi3a.dtb k1-x_MUSE-Paper-mini-4g.dtb k1-x_ZT_RVOH007.dtbk1-x_evb.dtb k1-x_LX-V10.dtb k1-x_MUSE-Pi.dtb m1-x_milkv-jupiter.dtbk1-x_evb.dtb.bak k1-x_milkv-jupiter.dtb k1-x_MUSE-Pi-Pro.dtbk1-x_fpga_1x4.dtb k1-x_MINI-PC.dtb k1-x_NetBridge-C1.dtbk1-x_fpga_2x2.dtb k1-x_MUSE-Book.dtb k1-x_RV4B.dtbroot@k1:~# cat /proc/device-tree/modelspacemit k1-x deb1 boardroot 然后就感觉可能是其中的k1-x_deb1.dtb 但是不放心就又去查看Uboot启动时加载的dtb 123456=&gt; printenv bootboot_default boot_device boot_devnum bootcmd bootdelay bootfs_devnamebootfs_part bootmenu_0 bootmenu_1 bootmenu_2 bootmenu_3 bootmenu_4bootmenu_5 bootmenu_6 bootmenu_7 bootmenu_8 bootmenu_9 bootmenu_delay=&gt; printenv bootcmdbootcmd=run autoboot; echo &quot;run autoboot&quot; 再查看autoboot: 显示如果从 eMMC 启动，那么就执行 mmc_boot 这个脚本。 12printenv autobootautoboot=if test ${boot_device} = nand; then run nand_boot; elif test ${boot_device} = nor; then run nor_boot; elif test ${boot_device} = mmc; then run mmc_boot; fi; 查看mmc_boot: 执行 detect_dtb 脚本。 1234=&gt; printenv mmc_boot mmc_boot=echo &quot;Try to boot from ${bootfs_devname}${boot_devnum} ...&quot;; run commonargs; run set_mmc_root; run set_mmc_args; run detect_dtb; run loadknl; run loaddtb; run loadramdisk; bootm ${kernel_addr_r} ${ramdisk_combo} ${dtb_addr}; echo &quot;########### boot kernel failed by d=&gt; printenv detect_dtb detect_dtb=echo &quot;product_name: ${product_name}&quot;; run dtb_env; echo &quot;select ${dtb_name} to load&quot;; 继续查看dev_env 12345=&gt; printenv dtb_envdtb_env=if test -n &quot;${product_name}&quot;; then if test &quot;${product_name}&quot; = k1_evb; then setenv dtb_name ${dtb_dir}/k1-x_evb.dtb; elif test &quot;${product_name}&quot; = k1_deb1; then setenv dtb_name ${dtb_dir}/k1-x_deb1.dtb; elif test &quot;${product_name}&quot; = k1_deb2; then setenv dtb_name ${dt=&gt; printenv product_name product_name=k1-x_deb1=&gt; printenv dtb_namedtb_name=k1-x_evb.dtb 最后发现加载的dtb为k1-x_exb.dtb 最后备份并且替换的是k1-x_exb.dtb文件 clone linux6.6内核源码 git clone https://gitee.com/bianbu-linux/linux-6.6.git –depth = 1 在原有基础上检查并打开配置并编译123456789101112131415161718192021make k1_defconfigmake menuconfig # 建议手动开启，可以打开对应更高级配置# eBPF 核心与性能 (必须开启)./scripts/config --enable BPF_JIT./scripts/config --enable BPF_JIT_ALWAYS_ON./scripts/config --enable DEBUG_INFO_BTF# eBPF 网络功能 (推荐开启)./scripts/config --enable NET_CLS_BPF./scripts/config --enable NET_ACT_BPF./scripts/config --enable XDP_SOCKETS# eBPF 追踪与调试功能 (必须开启)./scripts/config --enable BPF_EVENTS./scripts/config --enable KPROBES./scripts/config --enable UPROBES./scripts/config --enable TRACEPOINTS./scripts/config --enable FTRACE./scripts/config --enable KPROBE_EVENTS./scripts/config --enable UPROBE_EVENTS./scripts/config --enable FTRACE_SYSCALLS 这里编译时报错，发现工具链不支持zicond扩展，应该是当时编译时没有开启，重新编译12345CC scripts/mod/empty.oHOSTCC scripts/mod/mk_elfconfigCC scripts/mod/devicetable-offsets.sAssembler messages:错误： rv64imac_zicond_zihintpause_zba_zbc_zbs: unknown prefixed ISA extension `zicond' 编译 1234# 假设你的安装目录还是 /opt/riscv./configure --prefix=/opt/riscv --with-arch=rv64gc_zba_zbb_zbc_zbs_zicond# 这个过程会比较长，请耐心等待make -j$(nproc) linux 重新编译工具链后内核可完成编译，替换镜像和设备树 12mv /home/share/Image /boot/vmlinuz-6.6.63mv /home/share/k1-x_evb.dtb /boot/spacemit/6.6.63/k1-x_evb.dtb 确保写入磁盘并重启 12sync reboot 配置libbpf并使用libbpf-bootstrap库中示例 clone 仓库123git clone https://github.com/libbpf/libbpf-bootstrap.git --depth=1git submodule update --init --recursivecd libbpf-bootstrap 编译12cd examples/cmake 刚开始打算在pc上交叉编译，但是尝试了好久都没成功编译出来，于是在开发板上直接本地编译了 1234- sudo apt update- sudo apt install -y build-essential clang llvm libelf-dev libz-dev git- cd libbpf-bootstrap/examples/c- make 编译成功 123456789101112131415161718192021222324252627#编译成功root@k1:~# ls repo/libbpf-bootstrap/examples/c/bootstrap kprobe lsm.c minimal_ns sockfilter.c tc.cbootstrap.bpf.c kprobe.bpf.c Makefile minimal_ns.bpf.c sockfilter.h uprobebootstrap.c kprobe.c minimal minimal_ns.c task_iter uprobe.bpf.cbootstrap.h ksyscall minimal.bpf.c profile.bpf.c task_iter.bpf.c uprobe.cCMakeLists.txt ksyscall.bpf.c minimal.c profile.c task_iter.c usdtfentry ksyscall.c minimal_legacy profile.h task_iter.h usdt.bpf.cfentry.bpf.c lsm minimal_legacy.bpf.c sockfilter tc usdt.cfentry.c lsm.bpf.c minimal_legacy.c sockfilter.bpf.c tc.bpf.c xmake.lua# 成功运行 root@k1:~# ./repo/libbpf-bootstrap/examples/c/bootstrap TIME EVENT COMM PID PPID FILENAME/EXIT CODE15:11:17 EXEC ls 692 682 /usr/bin/ls15:11:17 EXIT ls 692 682 [0] (3ms)15:11:23 EXEC cat 693 682 /usr/bin/cat15:11:26 EXIT cat 693 682 [0] (3732ms)15:11:33 EXEC touch 694 682 /usr/bin/touch15:11:33 EXIT touch 694 682 [0] (2ms)15:11:33 EXEC ls 695 682 /usr/bin/ls15:11:33 EXIT ls 695 682 [0] (3ms)15:11:36 EXEC rm 696 682 /usr/bin/rm15:11:36 EXIT rm 696 682 [0] (1ms)15:11:37 EXIT bash 682 681 [0]15:11:38 EXIT sshd-session 681 671 [255]15:11:38 EXIT sshd-session 671 669 [255]15:11:38 EXIT (sd-close) 697 1 [0] BPF helper functions 函数 功能 用法示例 bpf_get_current_pid_tgid() 获取当前进程 PID 和 TID u64 id = bpf_get_current_pid_tgid(); pid = id &gt;&gt; 32; tid = id &amp; 0xFFFFFFFF; bpf_get_current_uid_gid() 当前进程的 UID/GID 用户态安全权限判断 bpf_get_current_comm(char *buf, int size) 获取当前进程名 用于日志输出或过滤 bpf_ktime_get_ns() 返回内核时间戳（ns） 用于计算函数耗时 bpf_map_lookup_elem() 访问 BPF map 中的元素 统计计数/缓存数据 bpf_map_update_elem() 更新 map 中的值 更新计数/状态 bpf_map_delete_elem() 删除 map 元素 清理数据 bpf_probe_read() / bpf_probe_read_str() 从内核空间安全读取数据 读取 struct task_struct 或驱动数据 bpf_perf_event_output() 发送数据到用户态 perf buffer 用于实时上报事件 bpf_trace_printk() 打印日志到 trace_pipe 调试 / 简单监控 第一个demo 早期无CO-RE模式，见此博客demo bpf_load.c + libelf + 手动链接 libbpf, 需要手动配置头文件，依赖内核版本源码，使用辅助脚本等 现代CO-RE模式 libbpf 库 + BPF Skeleton (.skel.h), libbpf 库和 bpftool 工具为你处理所有底层的繁琐工作（如ELF解析、map创建、程序加载、挂载）。 CO-RE (一次编译，到处运行)：通过 vmlinux.h 和 BTF，代码不强依赖特定内核版本，可移植性极高。 目的: 让一个 eBPF 程序只监控加载它自身的那个进程的 write 系统调用。 minimal_ns.bpf.c: 这是运行在内核中的 eBPF 代码，它会被附加到 write 系统调用的跟踪点上。 12345678910111213141516171819202122232425262728293031323334353637383940// SPDX-License-Identifier: GPL-2.0 OR BSD-3-Clause/* Copyright (c) 2023 Hosein Bakhtiari */#include &lt;linux/bpf.h&gt;#include &lt;bpf/bpf_helpers.h&gt; // BPF 辅助函数库#include &lt;linux/sched.h&gt; // 包含任务相关的结构定义char LICENSE[] SEC(&quot;license&quot;) = &quot;Dual BSD/GPL&quot;;// --- 全局变量 ---// 这些变量位于 .bss 段，意味着它们是未初始化的全局变量。// libbpf 允许用户态程序在加载 BPF 对象前，修改这些变量的值。int my_pid = 0;unsigned long long dev;unsigned long long ino;// --- BPF 程序逻辑 ---// SEC(&quot;tp/syscalls/sys_enter_write&quot;) 表示将此程序附加到// tracepoint 'syscalls:sys_enter_write'，即任何进程进入 write 系统调用时触发。SEC(&quot;tp/syscalls/sys_enter_write&quot;)int handle_tp(void *ctx){ // 定义一个结构体来接收 PID 命名空间信息 struct bpf_pidns_info ns; // --- 身份识别与过滤 --- // 调用 bpf_get_ns_current_pid_tgid 辅助函数。 // 它获取当前进程的 PID，但这个 PID 是相对于 dev 和 ino 所指定的 PID 命名空间而言的。 // dev 和 ino 是我们从用户态传递进来的，用于唯一标识一个命名空间。 bpf_get_ns_current_pid_tgid(dev, ino, &amp;ns, sizeof(ns)); // 过滤逻辑：如果当前触发 write 系统调用的进程的 PID // 不等于我们从用户态传递进来的 my_pid，就直接返回，什么也不做。 if (ns.pid != my_pid) return 0; // 如果 PID 匹配，就打印一条日志到跟踪管道。 bpf_printk(&quot;BPF triggered from PID %d.\\n&quot;, ns.pid); return 0;} 关键点 作用 全局变量 (my_pid, dev, ino) 这不是普通的全局变量。在 eBPF 中，定义在顶层的变量会被编译器放入 ELF 文件的特定段（如 .data, .bss, .rodata）。libbpf 在加载 BPF 程序时，能够识别这些变量，并允许用户态代码在 bpf_object__load() 之前对它们进行修改。这正是用户态向内核态传递配置的桥梁。 bpf_get_ns_current_pid_tgid() 这是一个非常重要的辅助函数。在复杂的容器环境中，一个进程的 PID 在容器内和在宿主机上是不同的。这个函数允许我们查询一个进程在特定PID命名空间中的 PID。它通过设备号（dev）和 inode 号（ino）来唯一确定一个命名空间。 过滤逻辑 这是整个 BPF 程序的核心。它确保了只有“目标”进程（即加载它自己的那个用户态程序）的 write 调用才能触发 bpf_printk。所有其他进程的 write 调用都会在 if 判断那里被提前过滤掉。 minimal_ns.c: 这是运行在用户空间的普通 C 程序，负责加载、设置、附加和销毁 eBPF 程序。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172// SPDX-License-Identifier: (LGPL-2.1 OR BSD-2-Clause)/* Copyright (c) 2023 Hosein Bakhtiari */#include &lt;stdio.h&gt;#include &lt;sys/stat.h&gt; // stat()#include &lt;unistd.h&gt; // getpid(), sleep()#include &lt;bpf/libbpf.h&gt; // libbpf 核心库#include &quot;minimal_ns.skel.h&quot; // 自动生成的 BPF 骨架文件// libbpf 的日志打印回调函数，方便调试static int libbpf_print_fn(enum libbpf_print_level level, const char *format, va_list args){ return vfprintf(stderr, format, args);}int main(int argc, char **argv){ struct minimal_ns_bpf *skel; // BPF 骨架对象指针 int err; struct stat sb; // 用于接收 stat() 系统调用的结果 libbpf_set_print(libbpf_print_fn); // 设置 libbpf 的日志回调 // --- BPF 程序生命周期：打开 --- skel = minimal_ns_bpf__open(); if (!skel) { fprintf(stderr, &quot;Failed to open BPF skeleton\\n&quot;); return 1; } // --- 关键部分 4: 获取自身身份信息并传递给 BPF 程序 --- // /proc/self/ns/pid 是一个特殊的文件，它的 dev 和 ino 号唯一标识了当前进程所在的 PID 命名空间。 if (stat(&quot;/proc/self/ns/pid&quot;, &amp;sb) == -1) { fprintf(stderr, &quot;Failed to acquire namespace information&quot;); return 1; } // 通过 skeleton 对象，访问 BPF 程序中的 .bss 全局变量并赋值。 skel-&gt;bss-&gt;dev = sb.st_dev; // 传递命名空间设备号 skel-&gt;bss-&gt;ino = sb.st_ino; // 传递命名空间 inode 号 skel-&gt;bss-&gt;my_pid = getpid(); // 传递当前进程的 PID // --- BPF 程序生命周期：加载 --- // 此刻，携带了正确配置（PID和命名空间ID）的 BPF 程序被加载到内核中。 err = minimal_ns_bpf__load(skel); if (err) { fprintf(stderr, &quot;Failed to load and verify BPF skeleton\\n&quot;); goto cleanup; } // --- BPF 程序生命周期：附加 --- // 将加载到内核的 BPF 程序附加到它指定的 tracepoint 上。 err = minimal_ns_bpf__attach(skel); if (err) { fprintf(stderr, &quot;Failed to attach BPF skeleton\\n&quot;); goto cleanup; } printf(&quot;Successfully started! Please run `sudo cat /sys/kernel/debug/tracing/trace_pipe` to see output...\\n&quot;); // --- 关键部分 5: 触发 BPF 程序 --- for (;;) { // 通过向 stderr 打印一个字符，来调用 write() 系统调用。 // 这个动作会触发我们刚刚附加的 BPF 程序。 fprintf(stderr, &quot;.&quot;); sleep(1); }cleanup: // --- BPF 程序生命周期：销毁 --- // 程序退出时，清理并卸载 BPF 程序。 minimal_ns_bpf__destroy(skel); return -err;} 关键点 作用 minimal_ns.skel.h 这是 libbpf-bootstrap 的魔法核心。构建系统会使用 bpftool 工具根据你的 .bpf.c 文件自动生成这个头文件。它包含了 struct minimal_ns_bpf 的定义，以及 __open, __load, __attach, __destroy 等生命周期管理函数。 skel-&gt;bss-&gt;… 这就是访问 BPF 全局变量的方式。skel 指向整个 BPF 对象的句柄，skel-&gt;bss 是一个指向 BPF 程序 .bss 段变量的结构体。你可以像访问普通结构体成员一样读写它们。 stat(“/proc/self/ns/pid”, &amp;sb): 这是获取当前进程 PID 命名空间标识符的标准方法。sb.st_dev 和 sb.st_ino 的组合在整个系统中是唯一的。 fprintf(stderr, “.”) 这行代码不仅仅是为了在终端上显示进度。它的本质是调用 write 系统调用。因为 BPF 程序监控的就是 write，并且只对自己这个 PID 感兴趣，所以这个调用就是触发 BPF 程序执行的“扳机”。 执行流程： 用户态程序启动。 它通过 stat 系统调用获取自己所在的 PID 命名空间标识（dev 和 ino），并通过 getpid() 获取自己的 PID。 它通过 BPF skeleton (skel-&gt;bss-&gt;…) 将这三个值写入 BPF 程序的全局变量中。 它调用 __load() 将配置好的 BPF 程序加载进内核。 它调用 __attach() 将 BPF 程序挂载到 write 系统调用的入口。 用户态程序进入无限循环，每秒调用一次 fprintf，这会触发 write 系统调用。 内核中的 BPF 程序被触发，它检查发现是目标进程，于是打印一条日志。 我们在另一个终端通过 cat /sys/kernel/debug/tracing/trace_pipe 就能看到这条日志。 运行结果: 相关仓库 libbpf-bootstrap (仓库) 定位: 一个由 libbpf 官方维护的最佳实践模板和入门脚手架。 价值: 它展示了如何正确地构建一个现代、基于 libbpf + CO-RE 的 eBPF 程序。它的 Makefile 和示例代码封装了所有复杂的构建细节： 如何调用 Clang 将 .bpf.c 编译成 .bpf.o。 如何使用 bpftool 基于 .o 文件生成骨架头文件 (.skel.h)。 如何将用户态的 C 程序与 libbpf 库链接起来。 对于初学者来说，克隆这个仓库是学习 libbpf 开发最直接、最标准的方式。 awesome-ebpf (仓库) 定位: 一个精心策划的 eBPF 资源聚合清单。 价值: 如果你想了解 eBPF 生态的全貌，这里是你的起点。它系统地收集和分类了互联网上几乎所有与 eBPF 相关的优秀资源，包括但不限于： 入门教程和文档 知名开源项目 (如 Cilium, Falco) 实用的开发工具 深度技术博客和文章 会议演讲和视频","link":"/post/ebpf-and-use-libbpf.html"},{"title":"linux内核启动流程分析2","text":"简单分析启动过程中初始化等操作流程 继续上文的start_kernle() 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061asmlinkage void __init start_kernel(void){ char *command_line; /* 最早期初始化（调试、安全、CPU信息） */ lockdep_init(); // 锁依赖检测 smp_setup_processor_id(); // 设置当前 CPU ID debug_objects_early_init(); // 调试对象初始化 boot_init_stack_canary(); // 栈溢出保护 cgroup_init_early(); // 控制组（早期阶段） /* 禁用中断，初始化时钟 &amp; 架构 */ local_irq_disable(); early_boot_irqs_disabled = true; tick_init(); // 初始化 tick timer boot_cpu_init(); // 标记 boot CPU page_address_init(); // 页地址映射表初始化 printk(KERN_NOTICE &quot;%s&quot;, linux_banner); // 打印内核 banner setup_arch(&amp;command_line); // **架构相关初始化**（内存、MMU、DTB） /* 初始化内存管理 */ mm_init_owner(&amp;init_mm, &amp;init_task); mm_init_cpumask(&amp;init_mm); setup_command_line(command_line); // 解析 boot 参数 setup_nr_cpu_ids(); setup_per_cpu_areas(); smp_prepare_boot_cpu(); // 准备 CPU 数据结构 build_all_zonelists(NULL); // 建立内存分配区域 page_alloc_init(); // 页分配器初始化 /* 参数解析 + 基础子系统初始化 */ parse_early_param(); // 解析 early 参数 parse_args(...); // 解析 boot args setup_log_buf(0); // 日志缓冲区 trap_init(); // 异常处理 mm_init(); // 完整内存初始化 /* 调度器 &amp; 同步机制 */ sched_init(); // 调度器初始化 preempt_disable(); // 禁止抢占 rcu_init(); // RCU radix_tree_init(); // radix 树 /* 中断和定时器 */ early_irq_init(); init_IRQ(); // 完整中断 init_timers(); // 普通 timer hrtimers_init(); // 高精度 timer softirq_init(); // 软件中断 time_init(); // 时间子系统 /* 启用中断 + 控制台 */ local_irq_enable(); console_init(); /* 剩余初始化 + 创建 init 进程 */ security_init(); vfs_caches_init(totalram_pages); // 文件系统缓存 signals_init(); rest_init(); // 创建 init/kthreadd} rest_init() -&gt; rest_init() 12345678910111213141516171819202122232425262728293031323334353637static noinline void __init_refok rest_init(void){ int pid; rcu_scheduler_starting(); // **RCU（读-复制-更新）调度器初始化** /* * 创建 init 进程（PID 1） * 调用 kernel_thread() → do_fork()，目标函数是 kernel_init() */ kernel_thread(kernel_init, NULL, CLONE_FS | CLONE_SIGHAND); numa_default_policy(); // **设置 NUMA 默认策略** /* * 创建 kthreadd 线程（内核线程管理器，负责所有内核线程） * 它的 PID 通常是 2 */ pid = kernel_thread(kthreadd, NULL, CLONE_FS | CLONE_FILES); /* * 通过 PID 查找 kthreadd 的 task_struct，并保存到全局变量 */ rcu_read_lock(); kthreadd_task = find_task_by_pid_ns(pid, &amp;init_pid_ns); rcu_read_unlock(); /* * 通知 kernel_init()（init 进程）可以继续执行 */ complete(&amp;kthreadd_done); /* * 当前任务是 idle 任务（PID 0） * 执行 schedule() 一次，启动调度 */ init_idle_bootup_task(current); schedule_preempt_disabled(); /* * 进入 idle 循环（CPU 空闲状态） * 此后 init 和 kthreadd 运行在用户态，PID 0 进入低功耗空转 */ cpu_idle();} kernel_init: Wait until kthreadd is all set-up: wait_for_completion(&amp;kthreadd_done); 12345678910111213141516171819202122232425262728293031323334353637383940414243static int __init kernel_init(void * unused){ wait_for_completion(&amp;kthreadd_done); // 等待 kthreadd 创建完成（PID 2） gfp_allowed_mask = __GFP_BITS_MASK; // 允许内存分配（调度器已准备好，可以阻塞分配） set_mems_allowed(node_states[N_HIGH_MEMORY]); // init 进程允许在任意内存节点分配 set_cpus_allowed_ptr(current, cpu_all_mask); // init 进程可以在所有 CPU 上运行 cad_pid = task_pid(current); // 保存 init 进程 PID smp_prepare_cpus(setup_max_cpus); // 准备多核 CPU 初始化 do_pre_smp_initcalls(); // 调用 SMP 初始化前的 initcall lockup_detector_init(); // 启动死锁检测机制 smp_init(); // 启动其他 CPU（多核） sched_init_smp(); // 初始化 SMP 调度器 do_basic_setup(); // 驱动子系统、文件系统等核心初始化 // 打开 /dev/console 作为标准输入输出 if (sys_open((const char __user *) &quot;/dev/console&quot;, O_RDWR, 0) &lt; 0) printk(KERN_WARNING &quot;Warning: unable to open an initial console.\\n&quot;); (void) sys_dup(0); // 复制文件描述符，stdin → stdout (void) sys_dup(0); // 复制文件描述符，stdin → stderr // 检查是否有 early userspace init，如果没有，则默认 /init if (!ramdisk_execute_command) ramdisk_execute_command = &quot;/init&quot;; // 如果 /init 不存在，准备挂载根文件系统 if (sys_access((const char __user *) ramdisk_execute_command, 0) != 0) { ramdisk_execute_command = NULL; prepare_namespace(); // 挂载根文件系统 } // 进入用户空间：释放 __init 段，执行 init 程序 init_post(); return 0;} kernel_init() -&gt; init_post() 12345678910111213141516171819202122232425262728293031323334static noinline int init_post(void){ async_synchronize_full(); // 等待所有异步 __init 任务完成 free_initmem(); // 释放 __init 段内存（只执行一次） mark_rodata_ro(); // 将只读数据段标记为只读，防止修改 system_state = SYSTEM_RUNNING; // 内核状态切换为运行态 numa_default_policy(); // 设置默认 NUMA 策略 current-&gt;signal-&gt;flags |= SIGNAL_UNKILLABLE; // init 进程不可被 kill // 如果 ramdisk_execute_command 存在（initrd/initramfs 提供的 init） if (ramdisk_execute_command) { run_init_process(ramdisk_execute_command); // 尝试执行 /init printk(KERN_WARNING &quot;Failed to execute %s\\n&quot;, ramdisk_execute_command); } // 如果uboot中内核参数 init=xxx 指定了 init 程序 if (execute_command) { run_init_process(execute_command); // 执行指定 init printk(KERN_WARNING &quot;Failed to execute %s. Attempting defaults...\\n&quot;, execute_command); } // 按优先级尝试启动用户空间的 init 进程 run_init_process(&quot;/sbin/init&quot;); run_init_process(&quot;/etc/init&quot;); run_init_process(&quot;/bin/init&quot;); run_init_process(&quot;/bin/sh&quot;); // 最后 fallback 到 shell // 如果所有都失败，内核 panic panic(&quot;No init found. Try passing init= option to kernel. &quot; &quot;See Linux Documentation/init.txt for guidance.&quot;);} 123456789static void run_init_process(const char *init_filename){ argv_init[0] = init_filename; // 设置 argv[0] 为要执行的 init 程序路径 kernel_execve(init_filename, argv_init, envp_init); // 调用 kernel_execve()，在内核态执行 execve 系统调用，加载用户态程序 // init_filename: 要执行的程序路径（/sbin/init 等） // argv_init: 参数数组（至少包含 init_filename） // envp_init: 环境变量（初始化时的默认环境变量）}","link":"/post/kernel-1.html"},{"title":"linux内核启动流程分析1","text":"*简单分析启动过程中的初始化内容 1. 寻找执行入口 和之前找uboot的执行入口一样，在编译时查看编译文件的顺序和链接文件发现入口为arch/armc/kernel/head.S “arch/arm/kernel/vmlinux.lds”…. = 0xC0000000 + 0x00008000.head.text : {text = .;*(.head.text)}… “arch/arm/kernel/head.S”….arm__HEADENTRY(stext)… “include/linux/init.h”…#define __HEAD .section “.head.text”, “ax”#define __INIT .section “.init.text”, “ax”#define __FINIT .previous… 2. head.S 确定 CPU 类型和启动模式（ARM/Thumb、SVC 模式） 关闭中断 检查处理器是否支持必要功能 初始化物理地址偏移 验证 ATAGs/DTB 参数 创建初始页表（identity mapping） 初始化 CPU 特定功能 开启 MMU（启用虚拟地址空间） 跳转到 __mmap_switched 进入内核下一阶段 段和入口定义 123.arm__HEAD // 定义在 .head.text 段ENTRY(stext) // 内核入口符号 stext .head.text 段：用于放置启动代码。 stext 是 ARM Linux 的 第一个执行入口（在物理地址 0xC0008000，即 PAGE_OFFSET + TEXT_OFFSET）。 检查 ARM/Thumb 模式并切换 1234THUMB( adr r9, BSYM(1f) )THUMB( bx r9 )THUMB( .thumb )THUMB(1: ) 如果内核是 Thumb-2 格式(一种ARM的嵌入式格式)，先跳转并切换到 Thumb 状态。 否则继续 ARM 模式。 设置 CPU 为 SVC 模式，关闭 IRQ/FIQ 1setmode PSR_F_BIT | PSR_I_BIT | SVC_MODE, r9 PSR_F_BIT：屏蔽 FIQ PSR_I_BIT：屏蔽 IRQ SVC_MODE：进入管理模式（Supervisor mode） 保证启动过程不会被中断打断。 获取 CPU ID 并匹配处理器类型 1234mrc p15, 0, r9, c0, c0 @ 获取 CP15 c0：主 ID 寄存器bl __lookup_processor_type @ 查找 CPU 类型表，返回 r5=procinfomovs r10, r5 @ 如果 r5=0，说明不支持该 CPUbeq __error_p @ 出错处理 读取 CPU ID（ARM 架构寄存器）。 检查是否在 内核支持的 CPU 列表里。 LPAE（ARMv7+）检查 1234mrc p15, 0, r3, c0, c1, 4 @ 读取 ID_MMFR0and r3, r3, #0xfcmp r3, #5 @ 检查是否支持长描述符页表blo __error_p 如果是 ARM LPAE 模式，检查是否支持 64-bit 页表。 计算 PHYS_OFFSET 1234adr r3, 2fldmia r3, {r4, r8}sub r4, r3, r4add r8, r8, r4 通过位置无关代码计算 物理偏移量，用于页表映射。 如果 CONFIG_XIP_KERNEL（执行在闪存），则直接 ldr r8, =PHYS_OFFSET。 验证传入参数 1234567bl __vet_atags @ 检查 ATAGs 或 Device Tree#ifdef CONFIG_SMP_ON_UPbl __fixup_smp @ 单核系统伪 SMP 修复#endif#ifdef CONFIG_ARM_PATCH_PHYS_VIRTbl __fixup_pv_table @ 修正物理/虚拟基址差异#endif 检查 启动参数有效性（machine ID, atags/dtb）。 如果是单核系统 + SMP 配置，做补丁。 修补物理/虚拟地址差异。 创建启动页表 1bl __create_page_tables 这是关键步骤，创建初始页表，映射： identity mapping：物理地址映射到相同的虚拟地址（用于开启 MMU） 内核虚拟基地址 (0xC0000000) 映射到物理地址 UART IO 区域（如果开启 DEBUG_LL） 进入 __create_page_tables 的逻辑： 分配页表内存 (pgtbl) 清空页表 identity map 启动代码区域（__turn_mmu_on） 映射内核代码区域 [KERNEL_START, KERNEL_END) 映射 boot 参数区（ATAGs 或 DTB） 如果 DEBUG_LL：映射 UART 寄存器地址 返回页表基地址（r4） 初始化 CPU 1234ldr r13, =__mmap_switched @ MMU 打开后跳转地址adr lr, BSYM(1f) @ 返回地址mov r8, r4 @ 设置页表基地址ARM( add pc, r10, #PROCINFO_INITFUNC ) 调用 CPU 特定初始化代码（在 arch/arm/mm/proc-*.S）。 初始化完成后返回，准备打开 MMU。 打开 MMU 1b __enable_mmu __enable_mmu： 配置 域访问控制寄存器（domain access control） 设置页表基地址（TTBR） 修改 CP15 控制寄存器（r0），打开 MMU（bit 0），可能还打开缓存。 然后执行： 1b __turn_mmu_on __turn_mmu_on： 写 CP15 控制寄存器，开启 MMU 切换 PC 到 __mmap_switched（位于高地址 0xC0000000 区域） 从此开始，虚拟地址生效。 __mmap_switched（MMU 打开后的第一步） 这个在 arch/arm/kernel/head-common.S，作用： 它完成从低级引导环境到C 语言内核环境的最后过渡，设置栈指针到内核栈, 清空 .bss, 确保内存布局、栈和关键变量正确，然后跳转到 start_kernel()。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172/* * __mmap_switched: * - MMU 已开启后执行的第一段代码 * - 作用：完成 .data 段拷贝、.bss 清零、设置栈、保存启动参数，然后跳到 start_kernel() */__mmap_switched: adr r3, __mmap_switched_data @ r3 &lt;- __mmap_switched_data 表虚拟地址 /* --------------------------------------------------------- * 1. 复制 .data 段（如果物理地址 != 虚拟地址） * 从 r3 中加载 4 个地址：__data_loc, _sdata, __bss_start, _end * --------------------------------------------------------- */ ldmia r3!, {r4, r5, r6, r7} @ r4=__data_loc, r5=_sdata, r6=__bss_start, r7=_end cmp r4, r5 @ 比较 __data_loc 和 _sdata1: cmpne r5, r6 @ 如果 .data 还没拷贝完并且不等于 .bss 开始 ldrne fp, [r4], #4 @ 从源地址(r4)加载一个字（递增4） strne fp, [r5], #4 @ 存到目标地址(r5)，并递增 bne 1b @ 如果还没结束，继续循环 /* --------------------------------------------------------- * 2. 清零 .bss 段（__bss_start 到 _end） * --------------------------------------------------------- */ mov fp, #0 @ fp 清零，用于写入1: cmp r6, r7 @ 比较当前位置和 _end strcc fp, [r6],#4 @ 如果 r6 &lt; r7，则写 0 并递增 bcc 1b @ 继续循环直到 r6 &gt;= r7 /* --------------------------------------------------------- * 3. 加载全局变量地址 + 设置栈 * 从表中再加载 5 个值：processor_id, machine_arch_type, atags_pointer, cr_alignment, sp * --------------------------------------------------------- */ARM( ldmia r3, {r4, r5, r6, r7, sp} ) @ ARM 模式直接加载5个寄存器THUMB( ldmia r3, {r4, r5, r6, r7} ) @ Thumb 模式先加载4个THUMB( ldr sp, [r3, #16] ) @ Thumb 再单独加载栈指针 /* --------------------------------------------------------- * 4. 保存启动参数到全局变量 * r9=CPU ID, r1=Machine ID, r2=atags/dtb 指针 * --------------------------------------------------------- */ str r9, [r4] @ 保存 processor_id str r1, [r5] @ 保存 machine_arch_type str r2, [r6] @ 保存 atags/dtb pointer /* --------------------------------------------------------- * 5. 保存 CP15 控制寄存器值 * r0 中存的是 CP15 控制寄存器（MMU 等） * CR_A 位是 Alignment fault enable，通常清掉 * --------------------------------------------------------- */ bic r4, r0, #CR_A @ r4 = r0 去掉 Alignment 位 stmia r7, {r0, r4} @ 保存 r0 和 r4 到 cr_alignment 区 /* --------------------------------------------------------- * 6. 跳转到 C 语言内核入口 * --------------------------------------------------------- */ b start_kernel @ 进入 start_kernel() （init/main.c）ENDPROC(__mmap_switched)/* * __mmap_switched_data 表： * 提供初始化所需的虚拟地址（因为 MMU 已开启） */__mmap_switched_data: .long __data_loc @ r4：物理 .data 起始地址 .long _sdata @ r5：虚拟 .data 起始地址 .long __bss_start @ r6：.bss 起始地址 .long _end @ r7：内核结束地址 .long processor_id @ 保存 CPU ID 的地址 .long __machine_arch_type @ 保存机器 ID 的地址 .long __atags_pointer @ 保存 atags/dtb 指针的地址 .long cr_alignment @ 保存 CP15 控制寄存器值的地址 .long init_thread_union + THREAD_START_SP @ 栈指针地址 .size __mmap_switched_data, . - __mmap_switched_data 进入 __mmap_switched 12__mmap_switched: adr r3, __mmap_switched_data adr r3, __mmap_switched_data：获取 __mmap_switched_data 表的虚拟地址（因为 MMU 已开启，现在是虚拟地址访问）。 复制 .data 段 123456ldmia r3!, {r4, r5, r6, r7} // 从表中读取4个值：__data_loc, _sdata, __bss_start, _endcmp r4, r5 // 比较 __data_loc 和 _sdata1: cmpne r5, r6 ldrne fp, [r4], #4 strne fp, [r5], #4 bne 1b 含义： r4 = __data_loc：物理内存中 .data 的位置 r5 = _sdata：虚拟地址 .data 段起始位置 r6 = __bss_start：.bss 段开始 r7 = _end：内核镜像结束位置 循环：如果 .data 的物理位置和虚拟位置不同（XIP 或 relocate），复制 .data 段内容到正确位置。 清零 BSS 1234mov fp, #0 @ 清零寄存器 fp1: cmp r6, r7 strcc fp, [r6],#4 bcc 1b 把 .bss 段清零（即 __bss_start 到 _end）。 原因：C 语言要求未初始化全局变量为 0。 加载保存关键变量的地址 123ARM( ldmia r3, {r4, r5, r6, r7, sp})THUMB( ldmia r3, {r4, r5, r6, r7} )THUMB( ldr sp, [r3, #16] ) 现在 r3 指向 __mmap_switched_data 的剩余部分： r4 = processor_id 地址 r5 = __machine_arch_type 地址 r6 = __atags_pointer 地址 r7 = cr_alignment 地址 sp = init_thread_union + THREAD_START_SP（设置栈指针） 保存启动参数 123str r9, [r4] @ processor_id = r9str r1, [r5] @ machine_arch_type = r1str r2, [r6] @ atags/dtb pointer = r2 把早期保存的 CPU ID（r9）、机器类型（r1）、atags/dtb 地址（r2）存到全局变量。 这样 start_kernel() 就可以用这些数据。 保存控制寄存器值 12bic r4, r0, #CR_Astmia r7, {r0, r4} @ 保存 CP15 control register 值 r0 是 CP15 控制寄存器值（MMU、Cache 状态）。清掉 CR_A（Alignment fault enable），保存两个版本：r0 原始值r4 去掉 Alignment bit 的值 跳转到 C 语言世界 1b start_kernel 直接跳到 start_kernel()（init/main.c），进入 Linux C 初始化。 __mmap_switched_data表 12345678910__mmap_switched_data: .long __data_loc @ r4 .long _sdata @ r5 .long __bss_start @ r6 .long _end @ r7 .long processor_id @ 保存 CPU ID 的地址 .long __machine_arch_type @ 保存机器 ID .long __atags_pointer @ 保存启动参数地址 .long cr_alignment @ 保存 CP15 控制寄存器值 .long init_thread_union + THREAD_START_SP @ sp 这个表提供了所有初始化需要的虚拟地址，因为现在 MMU 打开了，可以直接使用虚拟地址。 总结：__mmap_switched 主要干的事✔ 复制 .data 段（如果需要）✔ 清零 .bss 段✔ 设置栈指针（SP）✔ 保存 CPU ID、Machine ID、ATAGS/DTB 地址到全局变量✔ 保存 CP15 控制寄存器值✔ 跳转到 start_kernel() 进入 C 世界","link":"/post/kernel.html"},{"title":"libbpf, bcc和bpftrace的结构和关联分析","text":"libbpf bcc 和 bpftrace之间的结构以及和内核的关联 libbpf bcc 和 bpftrace之关系结构图(仅参考)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960+-----------------------------------------------------------------------------+| 用户空间 (User Space) ||-----------------------------------------------------------------------------|| [高层抽象/工具] || || +-----------------+ || | bpftrace(C++) | || | (诊断与排障语言)| || +-------+---------+ || | || 内部解析器解析用户脚本，翻译成C代码 || 将这个C代码实时编译成 eBPF 字节码(Clang/LLVM) || 使用 libbpf 的C API 来加载和管理eBPF程序 || | || | +----------------+ || | |BCC(内部实现C++)| python/Lua/c++高级语言封装|| | | (快速原型框架) | header file || |Clang/LLVM +-------------+--+ || | | || | 运行时把c代码编译成eBPF字段，API加载交互 || | 还是写C,只是编译和加载被Python框架封装了 || | | Clang/LLVM || +---------------------------+ ---------------------- || | libbpf(C) | | || | (是纯C开发库, 提供CO-RE) |------------------------------------ || | 封装bpf()和Maps API) | | || +-------------+-------------+ | || | || BPF Maps (e.g., RingBuf, Hash) | System Call || &lt;------------------------------------------------------&gt; (bpf()) || (内核与用户空间的数据通道) | (控制与加载) || | |+------------------------------------------------------------+----------------+| 内核空间 (Kernel Space) | ||-----------------------------------------------------------------------------|| | || BPF Maps (内核中的键值对存储) &lt;-----------------------------+ || | | || +---------------------------------------------------+&lt;---| | || | eBPF 子系统 | | || | | | || | +-----------+ +----------+ +-----------+ | | || | | Verifier | --&gt; | JIT | --&gt; | CPU | | | || | | (安全检查)| | (编译优化) | | (原生执行)| | | || | +-----------+ +----------+ +-----------+ | | || | | | || | ebpf 程序 (你的.bpf.c代码) | | || | | | || +-----------------------+---------------------------+ | || ^ | || | (事件触发) (读取/写入)| || | v || +-----------------------+-------------------------------------------+ || | 内核钩子 (kernel hooks) | || | | || | ftrace (kprobes, tracepoints), LSM, TC, XDP, Sockets ... | || | (BTF - 提供内核元数据, 增强钩子能力) | || +-------------------------------------------------------------------+ || |+-----------------------------------------------------------------------------+ BCC (BPF Compiler Collection) 构成: BCC 是一个强大的 eBPF 开发工具包和框架。其核心是一个 C++ 库，并提供了 Python、C++、Go 等多种语言的前端封装，其中 Python 前端最为流行和成熟。 核心机制: 运行时编译: BCC 的标志性特点是在程序运行时动态编译 eBPF C 代码。开发者在 Python 等脚本中嵌入 C 代码字符串，BCC 框架在后台调用 libclang (Clang 的库版本) 将其编译成 eBPF 字节码。 自有的加载器: BCC 拥有一套自研的加载器逻辑，负责处理字节码的加载、Map 创建以及与内核的交互。 与 libbpf 的关系: BCC 诞生早于 libbpf 的成熟期，因此其核心不依赖 libbpf。 然而，为了拥抱社区标准和利用 CO-RE 等现代特性，新版本的 BCC 已经开始逐步集成 libbpf，并提供基于 libbpf 的新工具和 API。 libbpf 构成: libbpf 是一个由内核社区维护的、用于开发 eBPF 应用的纯 C 语言核心库。它被认为是构建现代、高性能、可移植 eBPF 程序的事实标准。 核心机制: 预编译 (Ahead-of-Time): libbpf 的典型工作流是在开发阶段就将 eBPF C 代码（.bpf.c）编译成包含字节码的 ELF 对象文件（.bpf.o）。 智能加载: 用户态程序通过调用 libbpf 的 API，可以智能地解析 .o 文件，并将 eBPF 程序和 Maps 加载到内核。 CO-RE (一次编译，到处运行): 这是 libbpf 的王牌特性。它利用 BTF (BPF Type Format) 元数据，在加载时动态调整 eBPF 程序，以解决因内核版本不同导致的数据结构差异问题，极大地增强了程序的可移植性。 bpftrace 定位与构成: bpftrace 是一款专为 Linux 设计的高级动态追踪语言和命令行工具。它的语法简洁强大，类似 awk 和 DTrace，让使用者能用极少的代码快速排查系统性能问题。 核心机制: 高级语言到 C 的翻译: bpftrace 的核心是一个C++ 程序，它负责将用户编写的高级脚本实时翻译成 eBPF C 代码。 后端依赖: 它不直接与内核交互，而是依赖一个后端引擎来完成编译和加载。 与 libbpf/BCC 的关系: 历史与现在: 早期 bpftrace 依赖 BCC 作为其后端。为了追求更好的性能、更轻的依赖和 CO-RE 支持，现代版本的 bpftrace 已经默认切换到使用 libbpf 作为其核心后端。 对比表格：libbpf vs BCC vs bpftrace 特性 libbpf BCC bpftrace 定位 核心库 (Core Library) 开发框架 (Framework) 高级工具/语言 (High-level Tool) 主要用途 生产级应用、Agent、底层开发 快速原型、教学、脚本化开发 实时排障、命令行即时查询 编程接口 C/C++ API Python/C++ API 专用脚本语言 (类 awk) 编译时机 预编译 (开发时) 运行时 运行时 CO-RE 可移植性 原生支持 (核心优势) 支持有限/较弱 通过 libbpf 后端获得支持 运行时依赖 极轻量 重量级 中量级 … 需要 libbpf.so? 是 否 (但正在集成) 是 (现代版本) … 需要 Clang/LLVM? 否 是 是 … 需要内核头文件? 否 是 (传统方式) 否 … 需要 Python? 否 是 否 部署产物 单个二进制文件 Python脚本 + 运行时环境 bpftrace 工具 + 运行时环境 性能 最高 (启动快，无运行时编译开销) 中等 (有运行时编译开销) 中高 (比BCC快，但仍有运行时开销) 灵活性 最高 (完全控制) 高 (动态修改 C 代码方便) 中等 (受限于语言特性) 易用性 低 (最复杂，需手写 C) 中等 (Python 封装，较友好) 最高 (最简单，一行命令) 最适合场景 需要嵌入到其他程序中的长期监控Agent，如Cilium、Datadog Agent。 编写一次性的调试脚本，探索内核行为，快速验证想法。 系统管理员在服务器上快速定位一个具体问题，如”哪个进程在大量读写磁盘？”","link":"/post/libbpf-bcc-and-bpftrace.html"},{"title":"linux内核中qspinlock锁的优缺点分析","text":"qspinlock 是一种为现代多核系统设计的先进混合自旋锁。它巧妙地融合了两种经典锁的优点：既继承了票据锁（ticket lock）的公平性，又借鉴了 MCS 锁优异的可扩展性。 1. 传统spinlock： 多个等待的 CPU 核心中，谁先获得锁并无保证，存在公平性问题，同时缓存一致性开销大（如MESI），CPU核心越大，cache需求越厉害，缺乏可扩展性 2. Ticket spinlock1234567891011121314151617#define TICKET_NEXT 16typedef struct { union { u32 lock; struct __raw_tickets { /* little endian */ u16 owner; u16 next; } tickets; };} arch_spinlock_t;my_ticket = atomic_fetch_inc(&amp;lock-&gt;tickets.next);while (lock-&gt;tickets.owner != my_ticket) cpu_relax(); 解决了公平问题，防止某些 CPU 永远得不到锁，但所有核都轮询同一个owner变量，read cache line成热点，限制扩展性 3. MCS lock 本质上是一种基于链表结构的自旋锁，每个CPU有一个对应的节点(锁的副本)，基于各自不同的副本变量进行等待，锁本身是共享的，但队列节点是线程自己维护的，每个CPU只需要查询自己对应的本地cache line，仅在这个变量发生变化的时候，才需要读取内存和刷新这条cache line, 不像 classic/ticket对共享变量进行spin 123456789101112131415161718192021222324struct mcs_spinlock { struct mcs_spinlock *next; int locked; /* 1 if lock acquired */ int count; /* nesting count, see qspinlock.c */};static inlinevoid mcs_spin_lock(struct mcs_spinlock **lock, struct mcs_spinlock *node){ struct mcs_spinlock *prev; /* Init node */ node-&gt;locked = 0; node-&gt;next = NULL; prev = xchg(lock, node); if (likely(prev == NULL)) { return; } WRITE_ONCE(prev-&gt;next, node); /* Wait until the lock holder passes the lock down. */ arch_mcs_spin_lock_contended(&amp;node-&gt;locked);} 每个 CPU 线程创建的node 是独立的，每个线程都有自己的 node 实例。但是结构体中多了一个指针使结构体变大了，导致了“内存开销问题”：MCS 锁把竞争带来的 cache-line 抖动降低了，但牺牲了一些内存和部分结构管理的成本。 4. qspinlockinclude/asm-generic/qspinlock_types.h: 锁数据结构 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758typedef struct qspinlock { union { atomic_t val; /* * By using the whole 2nd least significant byte for the * pending bit, we can allow better optimization of the lock * acquisition for the pending bit holder. */#ifdef __LITTLE_ENDIAN struct { u8 locked; u8 pending; }; struct { u16 locked_pending; u16 tail; };#else struct { u16 tail; u16 locked_pending; }; struct { u8 reserved[2]; u8 pending; u8 locked; };#endif };} arch_spinlock_t;/* * Initializier */#define __ARCH_SPIN_LOCK_UNLOCKED { { .val = ATOMIC_INIT(0) } }/* * Bitfields in the atomic value: * * When NR_CPUS &lt; 16K * 0- 7: locked byte * 8: pending * 9-15: not used * 16-17: tail index * 18-31: tail cpu (+1) * * When NR_CPUS &gt; = 16K * 0- 7: locked byte * 8: pending * 9-10: tail index * 11-31: tail cpu (+1) */#define _Q_SET_MASK(type) (((1U &lt;&lt; _Q_ ## type ## _BITS) - 1)\\&lt;&lt; _Q_ ## type ## _OFFSET)#define _Q_LOCKED_OFFSET 0#define _Q_LOCKED_BITS 8#define _Q_LOCKED_MASK _Q_SET_MASK(LOCKED) When NR_CPUS &lt; 16K： locked：用来表示这个锁是否被人持有（0：无，1：有） pending：可以理解为最优先持锁位，即当unlock之后只有这个位的CPU最先持锁，也有1和0 tail：有idx+CPU构成，用来标识等待队列的最后一个节点。 tail_idx：就是index，它作为mcs_nodes数组的下标使用 tail_CPU：用来表示CPU的编号+1，+1因为规定tail为0的时候表示等待队列中没有成员 kernel/locking/mcs_spinlock.h 12345struct mcs_spinlock { struct mcs_spinlock *next; int locked; /* 1 if lock acquired */ int count; /* nesting count, see qspinlock.c */}; locked = 1:只是说锁传到了当前加节点，但是当前节点还需要主动申请锁(qspinlock -&gt; locked = 1)count：针对四种上下文用于追踪当前用了第几个 node（即 idx），最大为4,不够用时就fallback不排队直接自旋 kernel/locking/qspinlock.c: 123456789101112131415161718#define MAX_NODES 4struct qnode { struct mcs_spinlock mcs;#ifdef CONFIG_PARAVIRT_SPINLOCKS long reserved[2];#endif};/* * Per-CPU queue node structures; we can never have more than 4 nested * contexts: task, softirq, hardirq, nmi. * * Exactly fits one 64-byte cacheline on a 64-bit architecture. * * PV doubles the storage and uses the second cacheline for PV state. */static DEFINE_PER_CPU_ALIGNED(struct qnode, qnodes[MAX_NODES]); 一个 CPU 上可能嵌套多个锁, qnodes针对四种上下文情况下，例：进程上下文中发生中断后再次获取锁 PER_CPU的优点是快，可防止抢锁时再mallock或临时分配导致延迟，成本等问题 申请锁： 快速申请include/asm-generic/qspinlock.h 12345678910111213/** * queued_spin_lock - acquire a queued spinlock * @lock: Pointer to queued spinlock structure */static __always_inline void queued_spin_lock(struct qspinlock *lock){ int val = 0; if (likely(atomic_try_cmpxchg_acquire(&amp;lock-&gt;val, &amp;val, _Q_LOCKED_VAL))) return; queued_spin_lock_slowpath(lock, val);} 中速申请 快速申请失败，queue中为空时，设置锁的pending位 再次检测（检查中间是否有其它cpu进入） 一直循环检测locked位 当locked位为0时，清除pending位获得锁 慢速申请 申请 操作 快速申请 这个锁当前没有人持有，直接通过cmpxchg()设置locked域即可获取了锁 中速申请 锁已经被人持有，但是MCS链表没有其他人，有且仅有一个人在等待这个锁。设置pending域，表示是第一顺位继承者，自旋等待lock-&gt; locked清0，即锁持有者释放锁 慢速申请 进入到queue中自旋等待，若为队列头（队列中没有等待的cpu），说明它已排到最前，可以开始尝试获取锁；否则，它会自旋等待前一个节点释放锁，并通知它可以尝试获取锁了 end: 如果只有1个或2个CPU试图获取锁，那么只需要一个4字节的qspinlock就可以了，其所占内存的大小和ticket spinlock一样。当有3个以上的CPU试图获取锁，则需要(N-2)个MCS node qspinlock中加入”pending”位域的意义，如果是两个CPU试图获取锁，那么第二个CPU只需要简单地设置”pending”为1，而不用创建一个MCS node 试图加锁的CPU数目超过3个，使用ticket spinlock机制就会造成多个CPU的cache line刷新的问题，而qspinlock可以利用MCS node队列来解决这个问题 在多核争用严重场景下，qspinlock 让等待者在本地内存区域自旋，减少了锁的缓存抖动和对总线的竞争消耗 RISCV_QUEUED_SPINLOCKS 只应在平台(RISC-V)具有 Zabha 或 Ziccrse 时启用，不支持的情况不要选用 优先级反转问题，queue会保证了FIFO提高了公平性，但它无法感知任务的优先级，可能因为排在队列前方的低优先级任务未释放锁而发生等待，从而导致 优先级反转","link":"/post/qspinlock.html"},{"title":"riscv 工具链的了解和使用","text":"在 RISC-V 开发中, 交叉编译工具链允许我们在一个平台（如 x86 主机）上，为另一个平台（如RISC-V 开发板）生成可执行代码。 1. 核心概念：工具链的“三元组” (Triplet)你经常会看到像 riscv64-unknown-linux-gnu- 这样的名称，这就是工具链的“三元组”，其标准格式为： 1&lt;arch&gt;-&lt;vendor&gt;-&lt;os&gt; &lt;arch&gt; (架构)：指定目标 CPU 架构，例如 riscv64 或 riscv32。 &lt;vendor&gt; (供应商)：通常是 unknown 或公司名。 &lt;os&gt; (操作系统/环境)：这是最关键的部分，它决定了工具链的目标环境和使用的 C 标准库 (libc)。最常见的两个是： elf: 面向裸机 (Bare-metal) 或嵌入式实时操作系统 (RTOS)。 linux-gnu: 面向完整的 GNU/Linux 操作系统。 2. 两大主流工具链详解1. riscv64-unknown-elf用于裸机和嵌入式开发的标准工具链。 目标系统: 没有任何操作系统的环境（裸机），或者使用了轻量级实时操作系统（如 FreeRTOS, RT-Thread）的环境。 C 标准库 (Libc): 使用 Newlib。 Newlib 是一个轻量级的 C 库，专为嵌入式系统设计。它只提供最基础的 C 语言函数（如 strcpy, printf），并且不依赖任何操作系统的系统调用（Syscall）。如果需要文件操作或内存管理，需要实现底层的“桩函数”(stubs)。 应用场景: 编写 Bootloader（如 U-Boot）。 开发 RISC-V 的“特权二进制接口”固件（如 OpenSBI）。 为微控制器 (MCU) 编写固件。 开发简单的操作系统内核。 2. riscv64-linux-gnu用于在 RISC-V 平台上开发 Linux 应用的工具链。 目标系统: 运行完整 Linux 内核的系统。 C 标准库 (Libc): 使用 glibc (GNU C Library)。 glibc 是功能完备的标准 C 库，提供了丰富的 POSIX API 支持（如 fork, pthread, 文件系统操作等）。它深度依赖 Linux 内核提供的系统调用来完成工作。 典型应用场景: 编译一个标准的 C/C++ 应用程序（如 Nginx, Redis），让它运行在 RISC-V 架构的 Linux 发行版上（如 Ubuntu, Debian for RISC-V）。 开发 Linux 用户态驱动或服务程序。 Tip: riscv64-unknown-linux-gnu- 和 riscv64-linux-gnu- 在功能上是等价的，可以互换使用。unknown 字段在这里没有实际影响。 3. 如何获取和安装工具链方式一：使用包管理器 (简单快捷)对于 linux-gnu 工具链，这是最简单的方法。以 Ubuntu/Debian 为例： 123# 安装 C 和 C++ 交叉编译器sudo apt updatesudo apt install gcc-riscv64-linux-gnu g++-riscv64-linux-gnu 优点: 安装简单 缺点: 版本可能不是最新的 方式二：从源码编译 (推荐，灵活且最新)获取最新版本工具链（包括 elf 和 linux-gnu）的最佳方式。 安装相关依赖 12sudo apt install libncurses-dev libncursesw5-dev pkg-config autoconf automake bison flex gawk gcc g++ libtool make patch python3-dev texinfo wgetsudo apt-get install autoconf automake autotools-dev curl libmpc-dev libmpfr-dev libgmp-dev gawk build-essential bison flex texinfo gperf libtool patchutils bc zlib1g-dev libexpat-dev make bison flex texinfo gawk libncurses5-dev libexpat1-dev libgmp-dev libmpfr-dev libmpc-dev libgmp-dev libmpfr-dev libmpc-dev 克隆官方仓库 123#`--recursive` 参数至关重要，它会同时下载 `gcc`, `binutils` 等所有子模块。git clone --recursive https://github.com/riscv-collab/riscv-gnu-toolchaincd riscv-gnu-toolchain 1234567* 检查当前子模块情况。git submodule status* 拉取子模块(init: 子模块未初始化时初始化，recursive: 嵌套子模块也一起拉取)* 主仓库换分支时同步子模块git submodule update --init --recursive 配置与编译需要指定安装路径 (--prefix) 和目标架构 (--with-arch, --with-abi)。 编译 linux-gnu 工具链 (用于Linux): 12345678# 创建安装目录mkdir -p /opt/riscv-linux# 配置: 目录，目标是为linux构建工具链./configure --prefix=/opt/riscv-linux --enable-linux# `make linux` 会自动处理多阶段编译的复杂流程（构建临时gcc-&gt;构建glibc-&gt;构建最终gcc）time make -j$(nproc) linux# 安装sudo make install 编译 elf 工具链 (用于裸机):riscv64-unknown-elf- 123456789# 创建一个安装目录mkdir -p /opt/riscv-elf# 配置: 其中 `rv64gc` 指支持 64 位基础整数指令集（I）、乘除法（M）、原子（A）、浮点（F、D）、压缩（C）等扩展；# `lp64d` 表示 long 和 pointer 为 64 位，使用 double 精度浮点。./configure --prefix=/opt/riscv-elf --with-arch=rv64gc --with-abi=lp64d# 编译 (-j`nproc` 使用所有CPU核心加速)time make -j$(nproc)# 安装sudo make install 添加到环境变量为了方便使用，将工具链的 bin 目录添加到 PATH。编辑 ~/.bashrc 或 ~/.zshrc 文件： 123456# 添加这行到文件末尾 (根据编译的类型选择)export PATH=&quot;/opt/riscv-elf/bin:$PATH&quot; # For elf toolchainexport PATH=&quot;/opt/riscv-linux/bin:$PATH&quot; # For linux toolchain# 使配置生效source ~/.bashrc 4. 简单使用1234567// hello.c#include &lt;stdio.h&gt;int main() { printf(&quot;Hello, RISC-V World!\\n&quot;); return 0;} 使用 elf 工具链编译1234567# 编译riscv64-unknown-elf-gcc -o hello.elf hello.c# 查看文件类型file hello.elf# 输出会类似:# hello.elf: ELF 64-bit LSB executable, UCB RISC-V, version 1 (SYSV), statically linked, not stripped 这个 hello.elf 是一个静态链接的裸机程序。它不能直接在 x86 Linux 主机上运行，也不能在 RISC-V Linux 系统上直接运行，因为它缺少操作系统加载器所需的信息。它需要被烧录到裸机环境或通过模拟器（如 QEMU-system）加载执行。 这个 hello.elf 文件虽然是标准的 ELF 格式，但它与 Linux 可执行文件有本质区别： 不含 INTERP 段：它不指定动态链接器，因为它不依赖任何动态库。 静态链接: 它静态链接了轻量级的 newlib C 库，而非 glibc。 无系统调用: 其中的 printf 函数最终依赖开发者实现的底层 I/O 桩函数（如通过 UART 发送字符），而不是 Linux 的 write 系统调用。 不同的程序入口: 它的启动代码 (_start) 负责初始化 C 运行环境后调用 main，但 main 返回后程序通常会进入死循环，因为它没有“退出”到操作系统的概念。 使用 linux-gnu 工具链编译1234567# 编译riscv64-linux-gnu-gcc -o hello.linux hello.c# 查看文件类型file hello.linux# 输出会类似:# hello.linux: ELF 64-bit LSB executable, UCB RISC-V, version 1 (SYSV), dynamically linked, interpreter /lib/ld-linux-riscv64-lp64d.so.1, for GNU/Linux 4.15.0, not stripped 这个 hello.linux 是一个动态链接的 Linux 程序。它需要一个 RISC-V Linux 环境来运行，因为它依赖于该环境中的动态链接器 (ld-linux-riscv64-lp64d.so.1) 和 glibc 库。 总结： 特性 riscv64-unknown-elf riscv64-linux-gnu 目标平台 裸机 (Bare-metal)、RTOS GNU/Linux 系统 C 库 newlib (轻量级，无 OS 依赖) glibc (功能完备，依赖 Linux 内核) 核心用途 固件、Bootloader、RTOS 应用、简单操作系统内核 编译可在 RISC-V Linux 上运行的应用程序 选择场景 “为一块开发板从零开始写程序。” “在启动的 Linux 上面运行软件。”","link":"/post/riscv-toolchains.html"},{"title":"三种常见的 linux 设备的驱动介绍及框架","text":"按照读写存储数据方式，我们可以把设备分为以下几种：字符设备、块设备和网络设备。而Linux三大驱动就是指对这些设备的驱动，即字符设备、块设备驱动和网络设备驱动。 1. 字符设备 (Character Devices) 字符设备是一种按字节流（character stream）进行访问的设备，不可寻址，没有缓冲。你请求 5 个字节，它就给你 5 个字节（如果设备里有的话）。它不支持随机访问，数据只能顺序读写。 原理 核心： file_operations 结构体。里面定义了当用户空间程序对设备文件调用 open(), read(), write(), ioctl() 等系统调用时，内核应该执行的对应驱动函数。 VFS (虚拟文件系统)： 当 open(&quot;/dev/mydevice&quot;, ...) 时，VFS 会根据路径找到对应的 inode（索引节点），inode 中包含了设备号（主设备号和次设备号）。 驱动注册： 驱动在加载时，会通过 register_chrdev() 或 alloc_chrdev_region() + cdev_add() 来告诉内核能处理主设备号为 X 的设备，操作函数菜单是file_operations 结构体。 连接： VFS 通过主设备号找到驱动和 file_operations，然后调用实现的 my_open(), my_read() 等函数，从而将用户空间的操作连接到了驱动代码上。 典型例子： 串口 (/dev/ttyS*)、控制台 (/dev/console)、鼠标 (/dev/input/mouse0)、键盘 (/dev/input/event*)。 I2C/SPI 设备： 虽然挂在特定总线上，但最终给用户提供的接口往往是字符设备，如一个 I2C 接口的温湿度传感器，可能会表现为 /dev/i2c-1 或通过 sysfs 访问。 裸设备驱动： 各种自定义的、简单的控制类设备 代码示例 CLICK 2. 块设备 (Block Devices) 块设备是按“块”（Block）为单位进行数据访问的设备，块是固定大小的（如 512 字节、4KB）。与字符设备最大的不同是： 支持随机寻址它可以随机访问（直接读写第 N 个块），并且 有内核I/O缓冲区。 原理 核心： block_device_operations 结构体和 请求队列 (Request Queue)。 I/O 调度器： 当用户程序请求读写数据时，请求不会立即发送给硬件。而是被分解成一个个对“块”的操作请求（struct request），放入一个请求队列中。内核的 I/O 调度器 会对队列里的请求进行合并、排序，以提高磁盘寻道效率（比如把对相邻块的请求放在一起处理）。 缓冲/缓存 (Buffer Cache)： 内核会把频繁访问的块设备数据缓存在内存中（Page Cache/Buffer Cache）。当用户请求读取数据时，如果缓存里有，就直接从内存返回，速度极快，根本不需要访问物理设备。写操作也可能先写入缓存，稍后再“刷”到磁盘上。 驱动的角色： 块设备驱动的主要工作不是直接处理 read/write，而是从请求队列中取出已经由 I/O 调度器优化好的 request，然后根据 request 里的信息（起始块号、块数量、方向），操作硬件来完成真正的数据传输。 使用： 通常不直接用 read/write 对 /dev/sda 这样的裸设备进行操作（虽然也可以）。但更常见的用法是：在块设备上创建文件系统（mkfs.ext4 /dev/sda1），然后 mount 到一个目录上。之后，用户和程序就通过文件系统来访问，享受到了文件系统和块设备层共同带来的高效和便利。 典型例子： 硬盘 (HDD/SSD)(/dev/sda)、U盘 (/dev/sdb)、SD卡 (/dev/mmcblk0)、RAM disk（内存模拟的块设备）、Flash 存储 (通过 MTD): NAND/NOR Flash 在 MTD 层之上也可以表现为块设备。 代码示例 CLICK 3. 网络设备 (Network Devices) 网络设备是用于收发数据包（Packet）的设备。它和其他两类设备有本质区别，它不对应 /dev 目录下的文件节点。而是通过单独的网络接口来代表。 原理 核心： net_device_ops 结构体和 sk_buff (Socket Buffer)。 接口而非文件： 网络设备在内核中被抽象成一个接口（Interface），如 eth0, wlan0。用户空间程序通过 Socket API（socket(), bind(), sendto(), recvfrom()）等内核协议栈来与内核的 TCP/IP 协议栈交互，而不是操作设备文件。 数据流： 发送： 用户数据通过 Socket API 进入内核协议栈，被层层打包（加上 TCP/UDP 头、IP 头等），最终形成一个 sk_buff 结构体。这个 sk_buff 被交给网络设备驱动。驱动的 ndo_start_xmit 函数（定义在 net_device_ops 中）负责将 sk_buff 里的数据包通过物理网卡发送出去。 接收： 网卡收到一个数据包，产生硬件中断。驱动的中断处理程序把数据从硬件接收到内存，封装成一个新的 sk_buff，然后把它交给内核网络协议栈。协议栈逐层解包，最后通过 Socket 将数据送达正确的应用程序。 驱动的角色： 网络设备驱动是硬件和内核协议栈之间的“搬运工”，主要负责：初始化网卡、启动/停止数据收发、在 sk_buff 和硬件之间传递数据包。 典型例子： 有线网卡 (eth0, enp3s0)、无线网卡 (wlan0)虚拟网络接口、 CAN总线设备、 USB网络适配器。 代码示例 CLICK 对比 特性 字符设备 (Char) 块设备 (Block) 网络设备 (Net) 平台驱动 (Platform) 数据单位 字节流 (Stream) 数据块 (Block) 数据包 (Packet) 不直接处理数据流 访问方式 顺序访问 随机访问 Socket API N/A I/O 缓冲 无 (或很简单) 有内核缓冲/缓存和I/O调度 有 Socket 缓冲 N/A 用户接口 /dev 文件节点 /dev 文件节点, 文件系统 Socket 接口, ifconfig 通常是为其他驱动提供服务 核心结构体 file_operations block_device_operations net_device_ops platform_driver 核心机制 VFS 文件操作映射 请求队列和I/O调度 协议栈和sk_buff 设备与驱动的分离、匹配、探测 主要用途 简单、串行 I/O 设备 存储设备 网络通信 SoC 内部集成外设的管理框架 字符设备代码示例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177#include &lt;linux/module.h&gt;#include &lt;linux/kernel.h&gt;#include &lt;linux/init.h&gt;#include &lt;linux/fs.h&gt; // 包含 file_operations 结构体#include &lt;linux/cdev.h&gt; // 包含 cdev 结构体和相关函数#include &lt;linux/device.h&gt; // 包含 class_create 和 device_create#include &lt;linux/uaccess.h&gt; // 包含 copy_to_user 和 copy_from_user#include &lt;linux/slab.h&gt; // 包含 kmalloc 和 kfree#define DEVICE_NAME &quot;mymem_char&quot;#define CLASS_NAME &quot;mymem_class&quot;#define MAX_BUFFER_SIZE 1024// --- 驱动核心数据结构 ---static int major_number; // 主设备号static char *kernel_buffer; // 内核数据缓冲区static struct class* my_class = NULL; // 设备类static struct cdev my_cdev; // 字符设备结构// --- file_operations 函数实现 ---// open 函数：当设备文件被打开时调用static int my_open(struct inode *inode, struct file *file){ printk(KERN_INFO &quot;MyCharDev: Device opened.\\n&quot;); // 通常可以在这里为每个打开实例分配私有数据 // file-&gt;private_data = ... return 0;}// release 函数：当设备文件被关闭时调用static int my_release(struct inode *inode, struct file *file){ printk(KERN_INFO &quot;MyCharDev: Device closed.\\n&quot;); // 清理 open 时分配的私有数据 // kfree(file-&gt;private_data); return 0;}// read 函数：从设备读取数据static ssize_t my_read(struct file *filp, char __user *user_buf, size_t len, loff_t *offset){ int bytes_to_read; // 检查读取长度是否有效 if (*offset &gt;= MAX_BUFFER_SIZE) return 0; // End of file if (*offset + len &gt; MAX_BUFFER_SIZE) len = MAX_BUFFER_SIZE - *offset; bytes_to_read = len; // 使用 copy_to_user 将内核数据拷贝到用户空间 if (copy_to_user(user_buf, kernel_buffer + *offset, bytes_to_read) != 0) { printk(KERN_ERR &quot;MyCharDev: Failed to copy data to user.\\n&quot;); return -EFAULT; } *offset += bytes_to_read; // 更新文件偏移 printk(KERN_INFO &quot;MyCharDev: Read %d bytes.\\n&quot;, bytes_to_read); return bytes_to_read;}// write 函数：向设备写入数据static ssize_t my_write(struct file *filp, const char __user *user_buf, size_t len, loff_t *offset){ int bytes_to_write; // 检查写入位置是否有效 if (*offset &gt;= MAX_BUFFER_SIZE) { printk(KERN_WARNING &quot;MyCharDev: No space left on device.\\n&quot;); return -ENOSPC; // No space left on device } if (*offset + len &gt; MAX_BUFFER_SIZE) len = MAX_BUFFER_SIZE - *offset; bytes_to_write = len; // 使用 copy_from_user 将用户数据拷贝到内核空间 if (copy_from_user(kernel_buffer + *offset, user_buf, bytes_to_write) != 0) { printk(KERN_ERR &quot;MyCharDev: Failed to copy data from user.\\n&quot;); return -EFAULT; } *offset += bytes_to_write; // 更新文件偏移 printk(KERN_INFO &quot;MyCharDev: Wrote %d bytes.\\n&quot;, bytes_to_write); return bytes_to_write;}// --- file_operations 结构体定义 ---// 将实现的函数与标准文件操作关联起来static struct file_operations fops = { .owner = THIS_MODULE, .open = my_open, .release = my_release, .read = my_read, .write = my_write,};// --- 模块初始化函数 ---static int __init memchar_init(void){ dev_t dev_num; // 1. 分配内核缓冲区 kernel_buffer = kmalloc(MAX_BUFFER_SIZE, GFP_KERNEL); if (!kernel_buffer) { printk(KERN_ERR &quot;MyCharDev: Failed to allocate kernel buffer.\\n&quot;); return -ENOMEM; } // 2. 动态分配主设备号 if (alloc_chrdev_region(&amp;dev_num, 0, 1, DEVICE_NAME) &lt; 0) { printk(KERN_ERR &quot;MyCharDev: Failed to allocate major number.\\n&quot;); kfree(kernel_buffer); return -1; } major_number = MAJOR(dev_num); printk(KERN_INFO &quot;MyCharDev: Major number allocated: %d\\n&quot;, major_number); // 3. 初始化 cdev 结构体，并与 file_operations 关联 cdev_init(&amp;my_cdev, &amp;fops); my_cdev.owner = THIS_MODULE; // 4. 将 cdev 添加到内核 if (cdev_add(&amp;my_cdev, dev_num, 1) &lt; 0) { printk(KERN_ERR &quot;MyCharDev: Failed to add cdev to the kernel.\\n&quot;); unregister_chrdev_region(dev_num, 1); kfree(kernel_buffer); return -1; } // 5. 创建设备类 my_class = class_create(THIS_MODULE, CLASS_NAME); if (IS_ERR(my_class)) { printk(KERN_ERR &quot;MyCharDev: Failed to create device class.\\n&quot;); cdev_del(&amp;my_cdev); unregister_chrdev_region(dev_num, 1); kfree(kernel_buffer); return PTR_ERR(my_class); } // 6. 创建设备文件 (/dev/mymem_char) if (device_create(my_class, NULL, dev_num, NULL, DEVICE_NAME) == NULL) { printk(KERN_ERR &quot;MyCharDev: Failed to create device file.\\n&quot;); class_destroy(my_class); cdev_del(&amp;my_cdev); unregister_chrdev_region(dev_num, 1); kfree(kernel_buffer); return -1; } printk(KERN_INFO &quot;MyCharDev: Driver loaded successfully.\\n&quot;); return 0;}// --- 模块卸载函数 ---static void __exit memchar_exit(void){ dev_t dev_num = MKDEV(major_number, 0); // 逆序清理资源 device_destroy(my_class, dev_num); // 销毁设备文件 class_destroy(my_class); // 销毁设备类 cdev_del(&amp;my_cdev); // 从内核移除 cdev unregister_chrdev_region(dev_num, 1); // 释放设备号 kfree(kernel_buffer); // 释放内核缓冲区 printk(KERN_INFO &quot;MyCharDev: Driver unloaded.\\n&quot;);}module_init(memchar_init);module_exit(memchar_exit);MODULE_LICENSE(&quot;GPL&quot;);MODULE_AUTHOR(&quot;Your Name&quot;);MODULE_DESCRIPTION(&quot;A simple character device driver for memory simulation.&quot;); 块设备代码示例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128#include &lt;linux/module.h&gt;#include &lt;linux/kernel.h&gt;#include &lt;linux/init.h&gt;#include &lt;linux/genhd.h&gt; // 包含 gendisk#include &lt;linux/fs.h&gt; // 包含 block_device_operations#include &lt;linux/blkdev.h&gt; // 包含请求队列相关函数#include &lt;linux/vmalloc.h&gt; // 使用 vmalloc 分配大块内存#define DEVICE_NAME &quot;myram_block&quot;#define SECTOR_SIZE 512#define DEVICE_SECTORS 20480 // 10MB (20480 * 512 bytes)// --- 驱动核心数据结构 ---static int major_number; // 主设备号static u8 *device_data; // 模拟磁盘的内存区域static struct gendisk *my_disk; // gendisk 结构，代表一个独立的磁盘static struct request_queue *my_queue; // 请求队列static spinlock_t lock; // 用于保护请求队列的自旋锁// --- 请求处理函数 ---// 这是块设备驱动的核心，处理来自I/O调度器的请求static void my_request_fn(struct request_queue *q){ struct request *req; // 循环处理队列中的所有请求 while ((req = blk_fetch_request(q)) != NULL) { // 检查请求是否合法（这里简化处理，只检查读写请求） if (req == NULL || (rq_data_dir(req) != READ &amp;&amp; rq_data_dir(req) != WRITE)) { printk(KERN_NOTICE &quot;MyRamBlock: Skipping non-RW request\\n&quot;); __blk_end_request_all(req, -EIO); continue; } // 计算物理地址和大小 // blk_rq_pos(req) 返回起始扇区号 // blk_rq_cur_bytes(req) 返回请求的总字节数 unsigned long offset = blk_rq_pos(req) * SECTOR_SIZE; unsigned long num_bytes = blk_rq_cur_bytes(req); // 模拟数据传输 if (rq_data_dir(req) == WRITE) { // bio_for_each_segment 遍历请求中的所有段 (segment) // 将请求缓冲区中的数据拷贝到我们的模拟磁盘内存 memcpy(device_data + offset, bio_data(req-&gt;bio), num_bytes); } else { // 将模拟磁盘内存中的数据拷贝到请求缓冲区 memcpy(bio_data(req-&gt;bio), device_data + offset, num_bytes); } // 标记请求完成 __blk_end_request_all(req, 0); // 0 表示成功 }}// --- block_device_operations ---// 对于简单的驱动，这个结构体可以为空static struct block_device_operations my_bops = { .owner = THIS_MODULE,};// --- 模块初始化函数 ---static int __init ramblock_init(void){ // 1. 分配模拟磁盘的内存 device_data = vmalloc(DEVICE_SECTORS * SECTOR_SIZE); if (!device_data) { return -ENOMEM; } // 2. 注册块设备，获取主设备号 major_number = register_blkdev(0, DEVICE_NAME); if (major_number &lt; 0) { vfree(device_data); return major_number; } // 3. 初始化自旋锁和请求队列 spin_lock_init(&amp;lock); my_queue = blk_init_queue(my_request_fn, &amp;lock); if (!my_queue) { unregister_blkdev(major_number, DEVICE_NAME); vfree(device_data); return -ENOMEM; } // 4. 分配和初始化 gendisk 结构 my_disk = alloc_disk(1); // 1个次设备 (分区) if (!my_disk) { blk_cleanup_queue(my_queue); unregister_blkdev(major_number, DEVICE_NAME); vfree(device_data); return -ENOMEM; } // 5. 填充 gendisk 信息 my_disk-&gt;major = major_number; my_disk-&gt;first_minor = 0; my_disk-&gt;fops = &amp;my_bops; my_disk-&gt;queue = my_queue; snprintf(my_disk-&gt;disk_name, 32, DEVICE_NAME); set_capacity(my_disk, DEVICE_SECTORS); // 设置磁盘容量（以扇区为单位） // 6. 将 gendisk 添加到系统，使其可见 add_disk(my_disk); printk(KERN_INFO &quot;MyRamBlock: Driver loaded. Major: %d\\n&quot;, major_number); return 0;}// --- 模块卸载函数 ---static void __exit ramblock_exit(void){ del_gendisk(my_disk); // 从系统移除 gendisk put_disk(my_disk); // 释放 gendisk 引用 blk_cleanup_queue(my_queue); // 清理请求队列 unregister_blkdev(major_number, DEVICE_NAME); // 注销块设备 vfree(device_data); // 释放内存 printk(KERN_INFO &quot;MyRamBlock: Driver unloaded.\\n&quot;);}module_init(ramblock_init);module_exit(ramblock_exit);MODULE_LICENSE(&quot;GPL&quot;);MODULE_AUTHOR(&quot;Your Name&quot;);MODULE_DESCRIPTION(&quot;A simple RAM-based block device driver.&quot;); 网络设备代码示例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133#include &lt;linux/module.h&gt;#include &lt;linux/kernel.h&gt;#include &lt;linux/init.h&gt;#include &lt;linux/netdevice.h&gt; // 包含 net_device 和相关函数#include &lt;linux/etherdevice.h&gt; // 包含 alloc_etherdev#define DEVICE_NAME &quot;mynet&quot;// --- 驱动核心数据结构 ---// 我们将自定义的统计信息和设备指针放在一个结构体中struct mynet_priv { struct net_device_stats stats; struct net_device *dev;};static struct net_device *my_net_dev;// --- net_device_ops 函数实现 ---// open 函数：当接口被 &quot;ifconfig up&quot; 启动时调用static int mynet_open(struct net_device *dev){ // 启动传输队列 netif_start_queue(dev); printk(KERN_INFO &quot;%s: Device opened.\\n&quot;, dev-&gt;name); return 0;}// stop 函数：当接口被 &quot;ifconfig down&quot; 关闭时调用static int mynet_stop(struct net_device *dev){ // 停止传输队列 netif_stop_queue(dev); printk(KERN_INFO &quot;%s: Device stopped.\\n&quot;, dev-&gt;name); return 0;}// 发包函数：这是网络驱动的核心，负责发送数据包static netdev_tx_t mynet_start_xmit(struct sk_buff *skb, struct net_device *dev){ struct mynet_priv *priv = netdev_priv(dev); printk(KERN_INFO &quot;%s: Transmitting packet (len: %u)\\n&quot;, dev-&gt;name, skb-&gt;len); // 更新统计信息 priv-&gt;stats.tx_packets++; priv-&gt;stats.tx_bytes += skb-&gt;len; // --- 模拟环回 --- // 正常驱动会在这里把 skb 的数据通过 DMA 发送到硬件 // 我们直接将 skb 重新送回收包路径 skb-&gt;protocol = eth_type_trans(skb, dev); // 设置协议类型 skb-&gt;dev = dev; netif_rx(skb); // 将 skb 传递给内核协议栈的接收部分 // 告诉内核数据包已发送，可以释放 skb // dev_kfree_skb(skb); // 真实驱动中发送完会释放 skb // 但因为我们环回了，协议栈会负责释放它 return NETDEV_TX_OK; // 返回 OK 表示发送成功}// 获取统计信息函数static struct net_device_stats *mynet_get_stats(struct net_device *dev){ struct mynet_priv *priv = netdev_priv(dev); return &amp;priv-&gt;stats;}// --- net_device_ops 结构体定义 ---static const struct net_device_ops mynet_ops = { .ndo_open = mynet_open, .ndo_stop = mynet_stop, .ndo_start_xmit = mynet_start_xmit, .ndo_get_stats = mynet_get_stats,};// --- setup 函数，用于初始化设备 ---void mynet_setup(struct net_device *dev){ // 设置为以太网设备 ether_setup(dev); // 关联我们的操作函数 dev-&gt;netdev_ops = &amp;mynet_ops; // 分配一个随机的 MAC 地址 eth_hw_addr_random(dev); // 其他设备特性标志 dev-&gt;flags |= IFF_NOARP;}// --- 模块初始化函数 ---static int __init netloop_init(void){ struct mynet_priv *priv; // 1. 分配 net_device 结构体，并为私有数据分配空间 my_net_dev = alloc_netdev(sizeof(struct mynet_priv), DEVICE_NAME, NET_NAME_UNKNOWN, mynet_setup); if (!my_net_dev) { return -ENOMEM; } // 获取私有数据指针 priv = netdev_priv(my_net_dev); priv-&gt;dev = my_net_dev; // 2. 注册网络设备到内核 if (register_netdev(my_net_dev)) { printk(KERN_ERR &quot;Failed to register net device\\n&quot;); free_netdev(my_net_dev); return -1; } printk(KERN_INFO &quot;%s: Driver loaded.\\n&quot;, my_net_dev-&gt;name); return 0;}// --- 模块卸载函数 ---static void __exit netloop_exit(void){ printk(KERN_INFO &quot;%s: Unloading driver.\\n&quot;, my_net_dev-&gt;name); unregister_netdev(my_net_dev); // 从内核注销 free_netdev(my_net_dev); // 释放 net_device}module_init(netloop_init);module_exit(netloop_exit);MODULE_LICENSE(&quot;GPL&quot;);MODULE_AUTHOR(&quot;Your Name&quot;);MODULE_DESCRIPTION(&quot;A simple loopback network device driver.&quot;);","link":"/post/the-Three-Basic-Linux-Driver-Models.html"},{"title":"bpftrace环境的配置和使用(基于riscv-k1)","text":"在k1上配置并使用bpftrace,基于上一篇的配置 bpftrace配置过程大部分就是依赖的安装和编译问题，不再详细说明 安装依赖 1234567891011sudo apt-get update &amp;&amp; sudo apt-get install -y \\cmake flex bison \\python3-dev python3-setuptools \\libclang-20-dev libedit-dev libcurl4-openssl-dev libdebuginfod-dev liblzma-dev \\zip unzip \\llvm-20-tools llvm-20-dev libpolly-20-dev \\libbpf-dev libcereal-dev \\binutils-dev libdw-dev libpcap-dev \\curl \\dwarves \\libgtest-dev 配置编译和安装BCC（因为全部编译bpftrace需要依赖BCC的文件） 12345678910111213cd ~/repogit clone https://github.com/iovisor/bcc.gitcd bccgit submodule update --init --recursive# 创建一个干净的编译目录rm -rf build &amp;&amp; mkdir build &amp;&amp; cd build# 运行 cmake，并禁用示例以防止内存耗尽cmake -DCMAKE_INSTALL_PREFIX=/usr -DENABLE_EXAMPLES=OFF ..# 以较低的并行度进行编译，防止系统卡死make -j4 # 安装sudo make installsudo ldconfig 配置编译和安装bpftrace GTest需要手动编译123456cd /usr/src/googletestsudo cmake .sudo make -j$(nproc)sudo cp lib/*.a /usr/lib/sudo cp -r googletest/include/gtest /usr/include/sudo cp -r googlemock/include/gmock /usr/include/ 编译并安装bpftool123cd bpftool/src # 从 libbpf/src 目录回到 bpftool/srcmakesudo make install 编译并配置 blazesym1234567891011121314# 安装 Rust 环境(网速慢可用国内镜像)export RUSTUP_DIST_SERVER=https://mirrors.tuna.tsinghua.edu.cn/rustupexport RUSTUP_UPDATE_ROOT=https://mirrors.tuna.tsinghua.edu.cn/rustup/rustupwget https://sh.rustup.rs -O rustup-init.shchmod +x rustup-init.sh sudo ./rustup-init.sh -yrustc --versioncargo --versioncd ~/repo/libbpf-bootstrap/blazesym/capicargo build --release sudo cp target/release/libblazesym_c.a /usr/local/lib/sudo cp capi/include/blazesym.h /usr/local/include/blazesym.h sudo ldconfig clong bpftrace并编译安装123456789cd ~/repogit clone https://github.com/iovisor/bpftrace.gitcd bpftracegit submodule update --init --recursiverm -rf build &amp;&amp; mkdir build &amp;&amp; cd buildcmake ..make -j$(nproc)sudo make installbpftrace --version 安装完成！ 命令测试12# 输出sudo bpftrace -e 'BEGIN { printf(&quot;Hello, World! I have conquered eBPF on RISC-V!\\n&quot;); }' 12# 追踪所有打开文件的操作sudo bpftrace -e 'tracepoint:syscalls:sys_enter_openat { printf(&quot;PID %d opening: %s\\n&quot;, pid, str(args-&gt;filename)); }'","link":"/post/use-bpftrace-on-k1.html"},{"title":"U-boot 移植及源码分析( ARM )","text":"在s5p6818上移植uboot并分析其启动过程 1. 系统移植初步1. 嵌入式Linux构成与移植概述 嵌入式Linux移植定义: 移植就是将Bootloader的源代码、Linux内核源代码、文件系统中用户态程序代码根据硬件做少量修改，使其能够在目标硬件平台上运行起来的过程。 Linux内核功能及本质: 功能: 进程管理、文件系统、内存管理、网络协议。 2. Bootloader定义及重要性 定义: Bootloader是在操作系统内核运行之前运行，初始化硬件设备、建立内存空间映射，从而将系统的软硬件环境带到一个合适状态，以便为最终调用操作系统内核准备好正确的环境。 为什么需要Bootloader: 在嵌入式系统中，Bootloader是一段短小的启动程序，因为没有BIOS那样的固件程序，因此整个系统的加载启动任务就完全由Bootloader来完成。 使Linux内核可以在系统主存中跑起来，系统必须符合Linux内核启动的必要条件。 3. Bootloader、Kernel、应用程序之间的关系 Bootloader: 硬件上电后跳到固定位置执行相应代码 初始化相应的硬件设备 加载操作系统内核代码到内存 跳到内核代码起始位置执行 Kernel (uImage): 内核自解压 (uImage) 初始化相应的硬件设备 初始化静态编译进内核的驱动模块 挂载根文件系统 直接执行第一个用户空间程序 第一个用户空间程序: 配置用户环境和执行服务进程 2. 开发板烧写并启动(先对emmc进行分区,了解整个系统在emmc上的启动过程)1. emmc分区布局12340M - 1M(0x80) : Bootloader (U-Boot)1M - 65M(0x20800) : Kernel (uImage + dtb)65M - 819M : RootFS (ext4)819M - 末尾 : 其他数据 2. 假设当前通过软件烧录uboot后启动在uboot中，(uboot支持emmc操作) 查看emmc信息 mmc list / mmc dev 0 / mmc info 按需擦除数据 mmc erase 0 1000000 分区(help fdisk) fdisk [part table counts] start:length…. fdisk 2 3 0x100000:0x4000000 0x4100000:2f200000 0x33300000:0 // 从bootloader地址之后开始分区的话 通过uboot中的 loadaddr 和 bdinfo 查看DRAM信息, 然后重新烧写uboot tftp 0x48000000 ubootpak.bin Bytes transferred = 342960 (53bb0 hex) update_mmc 2 2ndboot 0x48000000 0x200 53bb0 重启进入uboot re Linux内核烧写 tftp 48000000 uImage mmc write 48000000 0x800 0x3000 (写入的扇区位置和数量) 3. 配置NFS服务器 rootfs一般都是从网络启动和挂载，因为方便调试，主机需要配置NFS服务器。 服务端配置 编辑/etc/exports文件: sudo vi /etc/exports 添加NFS共享目录配置: /opt/rootfs *(rw,sync,no_root_squash) rw: 读写权限。 sync: 数据同步写入磁盘。 no_root_squash: 客户端以root身份访问文件。 重启NFS服务使配置生效: sudo /etc/init.d/nfs-kernel-server restart 客户端配置 (U-boot环境变量) 设置bootargs:setenv bootargs root=/dev/nfs nfsroot=192.168.1.8:/opt/rootfs ip=192.168.1.6:192.168.1.8:192.168.1.255:255.0 console=ttySAC0 maxcpus=1 lcd=wy070ml tp=gslx680-linux root=/dev/nfs: 指定根文件系统类型为NFS。 nfsroot=192.168.1.8:/opt/rootfs: NFS服务器IP地址和共享目录。 ip=...: 开发板的IP地址配置。 console=ttySAC0: 指定控制台输出设备。 3. U-boot源码分析1. U-boot U-Boot（Universal Boot Loader）是一个开源、跨平台的 Bootloader，主要用于嵌入式系统启动。它独立于操作系统，运行在裸机环境，支持多种 CPU 架构（ARM、PowerPC、MIPS、x86 等）和操作系统（Linux、VxWorks 等）。 特点：模块化设计，易于移植；支持多种存储、外设和网络协议；提供丰富的命令行操作。 2. U-boot编译 清理环境（防止残留配置）： make distclean 加载目标板默认配置： make _defconfig 示例：make x6818_defconfig 可选修改配置（菜单界面）： make menuconfig 编译（指定交叉编译器）： make CROSS_COMPILE=arm-linux-gnueabihf- -j$(nproc) 查看产物： u-boot：ELF 格式，可调试,为可执行的ELF格式文件。 ubootpak.bin为u-boot通过objcopy转换得到的二进制文件（用于烧写）。 u-boot-dtb.bin：带设备树版本（部分平台）。 3. 入口点文件分析编译链接过程 (make V=1)当执行 make V=1 时，可以看到详细的编译链接过程。例如：make -f scripts/Makefile.build obj=examples/standalone arm-cortex_a9-linux-gnueabi-ld -pte --gc-sections -Bstatic -Ttext 0x43C0000 -o u-boot u-boot.lds arch/arm/cpu/slsiap/start.o --start-group arch/arm/cpu/slsiap/built-in.o arch/arm/cpu/slsiap/s5p6818/built-in.o arch/arm/cpu/slsiap/s5 这里关键的是 -Ttext 0x43C0000 -o u-boot u-boot.lds，它指定了U-boot的起始地址和链接脚本。 u-boot.lds 链接脚本链接脚本定义了U-boot各段在内存中的布局。 1234567891011. = 0x00000000;. = ALIGN(4);.rodata : { *(SORT_BY_ALIGNMENT(SORT_BY_NAME(.rodata))) }.data : { }.text :{ *(.__image_copy_start) arch/arm/cpu/slsiap/s5p6818/start.o (.text*) arch/arm/cpu/slsiap/s5p6818/vectors.o (.text*) *(.text*)} . = 0x00000000;: 指定代码的加载起始地址为0x00000000 (通常是内存或Flash的起始地址)。 arch/arm/cpu/slsiap/s5p6818/start.o (.text*): 这表明start.o文件中的.text段（代码段）是U-boot的第一个执行部分。 start.S (入口点汇编文件)start.o是由start.S汇编文件编译而来，其中包含了U-boot的真正入口点。 12345678910.globl _start_start: b reset // 跳转到reset函数 ldr pc, _undefined_instruction ldr pc, _software_interrupt ldr pc, _prefetch_abort ldr pc, _data_abort ldr pc, _not_used ldr pc, _irq ldr pc, _fiq _start: 这是U-boot的入口点符号。 b reset: 上电后CPU执行的第一个指令，直接跳转到reset函数。reset函数是U-boot初始化过程的起点。 4. U-boot启动过程 U-boot启动分为两个主要阶段： 阶段一: 最底层初始化 (汇编代码)U-Boot 启动的第一阶段：从复位到 C 运行环境 异常向量表设置 CPU 模式切换 缓存与 MMU 初始化 代码重定位 BSS 段清零 栈和全局数据结构（gd）初始化 跳转到 board_init_f()（进入 C 语言环境） start.S主要代码分析: 异常向量表设置 ARM 处理器上电后，从 0x00000000 开始取指（或由 VBAR 指定的基地址），所以必须先建立异常向量表。 12345678910.globl _stext_stext: b reset @ 复位时跳到 reset ldr pc, _undefined_instruction ldr pc, _software_interrupt ldr pc, _prefetch_abort ldr pc, _data_abort ldr pc, _not_used ldr pc, _irq ldr pc, _fiq 定义 _stext 作为代码的起始地址 ARM 异常类型：reset, undefined, SWI, prefetch abort, data abort, IRQ, FIQ 通过 ldr pc, xxx 跳转到对应处理函数地址 这一步的目的：保证 CPU 出现异常不会直接崩溃，有跳转入口。 ARM 架构规定，当某种异常发生时，硬件会自动做两件事： 切换到对应的异常模式（如 Supervisor、IRQ、FIQ）。 把 PC 设置到异常向量表的固定偏移地址（通常是 0x00000000 或者 VBAR 里的值）。 例如： 发生 Reset → PC = 0x00000000 → 执行 b reset 发生 Undefined → PC = 0x00000004 → 执行 ldr pc, _undefined_instruction 发生 IRQ → PC = 0x00000018 → 执行 ldr pc, _irq b handler 是 相对跳转，需要 handler 在当前 32MB 范围内。 ldr pc, _xxx 是 绝对跳转，通过取一个常量地址，可以跳到 U-Boot 任意地方。 这样设计是为了灵活，后面如果重定位代码，这个表不用改，直接改 _xxx 变量的值。 reset：从复位到基本初始化 reset 是启动的第一个执行函数，主要工作： 保存启动参数 切换到 SVC 模式 关闭看门狗 初始化 CPU 关键寄存器 如果需要，跳到低级硬件初始化 123456789101112131415161718reset: bl save_boot_params @ 保存启动参数（调用 save_boot_params 函数，保存启动时的参数） /* 切换到 SVC32 模式 */ mrs r0, cpsr @ 读取当前的 CPSR（Current Program Status Register，当前程序状态寄存器） bic r0, r0, #0x1f @ 清除 CPSR 中的模式字段（低 5 位）以准备切换模式 orr r0, r0, #0xd3 @ 设置 CPSR 的模式为 SVC 模式 (SVC32)，并禁用所有中断 msr cpsr, r0 @ 将修改后的 CPSR 写回，切换到 SVC 模式并禁用中断 /* 禁用看门狗 */ ldr r0, =0xC0019000 @ 加载看门狗控制寄存器的地址（地址是 0xC0019000） mov r1, #0 @ 将寄存器 r1 设置为 0，表示禁用看门狗 str r1, [r0] @ 将 r1 的值（0）存储到看门狗控制寄存器，禁用看门狗#ifndef CONFIG_SKIP_LOWLEVEL_INIT bl cpu_init_cp15 @ 调用 cpu_init_cp15 函数，关闭 MMU、清除缓存等 bl cpu_init_crit @ 调用 cpu_init_crit 函数，执行板级低级初始化，如 PLL 和 DDR 配置#endif 关键点： cpsr 切换 CPU 模式为 SVC（超级用户模式），同时关中断 关闭看门狗，避免启动过程中被复位 cpu_init_cp15：处理 CP15 寄存器，关闭 MMU，invalidate I/D cache cpu_init_crit：执行板级初始化（比如 DRAM 控制器），确保后面可以使用内存 代码重定位（Relocation）-&gt; 如果平台 开启 CONFIG_RELOC_TO_TEXT_BASE 很多 ARM 平台启动时，U-Boot 最初加载在 NOR Flash 或 SRAM，执行地址并不是最终运行地址。为了能正确访问全局变量、函数指针，必须把 U-Boot 拷贝到 DRAM 的 TEXT_BASE。 1234567891011121314151617relocate_to_text: adr r0, _stext @ 获取当前代码位置（即 _stext 地址），并将其存入寄存器 r0 ldr r1, TEXT_BASE @ 加载目标地址（TEXT_BASE，即代码重定位后存放的位置） cmp r0, r1 @ 比较当前代码地址（r0）与目标地址（r1） beq clear_bss @ 如果当前地址已经是目标地址，则跳转到 clear_bss，避免重复搬迁 ldr r2, _bss_start_ofs @ 加载 BSS 段起始地址的偏移量（_bss_start_ofs） add r2, r0, r2 @ 计算拷贝结束地址（当前地址 + BSS 段的偏移量）copy_loop_text: ldmia r0!, {r3-r10} @ 从 r0（当前代码地址）加载 8 个寄存器的内容，并将 r0 地址增加 8 字节 stmia r1!, {r3-r10} @ 将加载的寄存器内容存储到 r1（目标地址），并将 r1 地址增加 8 字节 cmp r0, r2 @ 比较当前地址（r0）与拷贝结束地址（r2） ble copy_loop_text @ 如果 r0 小于等于 r2，说明还没有到达结束地址，继续拷贝 ldr r1, TEXT_BASE @ 重新加载目标地址 TEXT_BASE mov pc, r1 @ 跳转到 DRAM 中的 TEXT_BASE 继续执行 比较当前执行地址和目标地址（TEXT_BASE） 如果不一样，循环拷贝整个 U-Boot 代码段 最后 mov pc, r1，跳转到 DRAM 里的代码继续执行 清空 BSS 段 BSS 段用于存放未初始化的全局变量，必须清零。 12345678910111213clear_bss: ldr r0, _bss_start_ofs @ 加载 BSS 段的起始偏移地址 ldr r1, _bss_end_ofs @ 加载 BSS 段的结束偏移地址 ldr r4, TEXT_BASE @ 加载 TEXT_BASE 地址 add r0, r0, r4 @ 计算实际的 BSS 段起始地址（BSS 起始地址 + TEXT_BASE） add r1, r1, r4 @ 计算实际的 BSS 段结束地址（BSS 结束地址 + TEXT_BASE） mov r2, #0 @ 设置 r2 为 0，准备清空 BSS 段clbss_l: str r2, [r0] @ 将 0 存储到 BSS 段中的当前地址 add r0, r0, #4 @ 移动到下一个 4 字节地址（每次清 4 字节） cmp r0, r1 @ 比较当前地址（r0）与 BSS 结束地址（r1） bne clbss_l @ 如果没有到达结束地址，继续清空 BSS 段 把 BSS 段所有字节清 0 避免后面 C 代码访问到脏数据 初始化栈和 GD（global data）结构 C 代码需要栈，U-Boot 还需要 gd_t 全局数据结构保存运行状态。 12345ldr sp, =(CONFIG_SYS_INIT_SP_ADDR) @ 设置栈指针 sp 为初始化的栈地址（CONFIG_SYS_INIT_SP_ADDR）bic sp, sp, #7 @ 确保栈指针 sp 8 字节对齐，清除 sp 的最低三位sub sp, #GD_SIZE @ 给全局数据（GD）预留空间，调整栈指针，分配 GD 所需的内存bic sp, sp, #7 @ 再次确保栈指针 8 字节对齐mov r9, sp @ 将栈指针保存到 r9，r9 将作为全局数据（GD）的基地址 CONFIG_SYS_INIT_SP_ADDR：通常是 DRAM 高地址（安全） GD_SIZE：U-Boot 的全局数据结构 r9（IP 寄存器）专门存放 gd 指针 board_init_f函数执行，跳转到 board_init_f 所在的board_f.c 文件中 board_init_f 阶段 运行位置：Flash 或 SRAM（未 relocation 前） 作用： 初始化 PLL/时钟。 初始化 DDR 控制器（重点）。 初始化 串口（用于输出 early log）。 计算 relocation 参数： gd-&gt;relocaddr：U-Boot 在 DRAM 的目标地址。 gd-&gt;start_addr_sp：新的栈顶地址（在 DRAM）。 gd-&gt;new_gd：新全局数据区域（DRAM）。 调用 jump_to_copy() → relocate_code()。 relocate_code 阶段 运行位置：仍然在 Flash/SRAM（搬家前）。 作用： 将 .text、.data 从 当前位置 复制到 gd-&gt;relocaddr（DRAM）。 清空 .bss 段（DRAM）。 切换栈到 DRAM（sp = gd-&gt;start_addr_sp）。 跳转到 board_init_r 的地址（现在在 DRAM 中）。 这一步才是真正把 U-Boot 搬到 DRAM 并切换执行位置。 board_init_r函数执行，跳转到 board_init_r 所在的board_r.c 文件中 运行位置：DRAM（relocation 完成）。 作用： cif (initcall_run_list(init_sequence_r)) hang(); 初始化全局变量。 初始化设备驱动（I2C、SPI、USB、存储设备）。 初始化环境变量（env）。 初始化控制台。 调用 run_main_loop() 进入命令行。 for (;;) main_loop(); // to common/Main.c 从这里开始，整个 U-Boot 都在 DRAM 执行，C 语言代码为主。 这部分汇编代码完成了CPU模式设置（SVC模式）、栈初始化、关闭看门狗以及L1 cache、MMU、TLB的初步配置。 common/main.c 1234567891011121314151617181920212223void main_loop(void){ const char *s; modem_init(); cli_init(); run_preboot_environment_command(); // get bootdelay of uboot and store it in a global variable: stored_bootdelay // return bootcmd through: getenv(&quot;bootcmd&quot;) s = bootdelay_process(); // abortboot(stored_bootdelay)进行倒计时 // 未打断：run_command_list(s, -1, 0); 启动内核 // 打断： 返回 autoboot_command(s); // 调用cli_simple_loop(): for(;;) cli_readline(CONFIG_SYS_PROMPT);// 接受用户端命令:common/cli_readline.c // ... run_command_repeatable(lastcommand, flag); // 执行用户端命令 cli_loop();} cli_loop时执行输入命令的核心函数(详解) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107int cli_simple_run_command(const char *cmd, int flag){ // 定义一个缓冲区，用来存放传入的命令字符串的副本。 // 不能直接操作 cmd，因为 getenv() 返回的环境变量字符串可能在执行过程中被修改。 char cmdbuf[CONFIG_SYS_CBSIZE]; /* working copy of cmd */ // token 指向当前解析的命令起始位置。 char *token; /* start of token in cmdbuf */ // sep 用于找到命令分隔符（比如 ';'）的位置。 char *sep; /* end of token (separator) in cmdbuf */ // finaltoken 存放替换宏后的命令（比如把 $bootcmd 替换为实际值）。 char finaltoken[CONFIG_SYS_CBSIZE]; // str 是解析循环中当前处理位置的指针。 char *str = cmdbuf; // argv 保存分解后的参数列表，argv[0] 是命令名，argv[1..n] 是参数。 char *argv[CONFIG_SYS_MAXARGS + 1]; /* NULL terminated */ int argc, inquotes; int repeatable = 1; // 标记命令是否可以重复执行（大多数命令是可以的）。 int rc = 0; // 返回值，默认成功。 debug_parser(&quot;[RUN_COMMAND] cmd[%p]=\\&quot;&quot;, cmd); if (DEBUG_PARSER) { /* 打印调试信息，显示传入的命令内容 */ puts(cmd ? cmd : &quot;NULL&quot;); puts(&quot;\\&quot;\\n&quot;); } // 清除 Ctrl-C 标志，防止上一次命令中断状态影响当前命令。 clear_ctrlc(); /* forget any previous Control C */ // 如果命令为空或是空字符串，直接返回 -1。 if (!cmd || !*cmd) return -1; /* empty command */ // 检查命令长度是否超出缓冲区大小。 if (strlen(cmd) &gt;= CONFIG_SYS_CBSIZE) { puts(&quot;## Command too long!\\n&quot;); return -1; } // 把命令复制到 cmdbuf 中，后续操作基于这个副本。 strcpy(cmdbuf, cmd); /* * 主循环，处理命令分隔符 ';'，即一行命令可以包含多个子命令。 * 比如： &quot;setenv bootargs console=ttyS0; bootm 0x80007fc0&quot; */ debug_parser(&quot;[PROCESS_SEPARATORS] %s\\n&quot;, cmd); while (*str) { /* * 查找命令分隔符 ';'，如果命令中有引号，要忽略引号中的 ';' * 允许用 &quot;\\;&quot; 转义分号。 */ for (inquotes = 0, sep = str; *sep; sep++) { if ((*sep == '\\'') &amp;&amp; (*(sep - 1) != '\\\\')) inquotes = !inquotes; if (!inquotes &amp;&amp; (*sep == ';') &amp;&amp; /* 找到分隔符 */ (sep != str) &amp;&amp; /* 确保不是第一个字符 */ (*(sep - 1) != '\\\\')) /* 并且没有被转义 */ break; } /* * token = 当前命令字符串（不包括 ';'） */ token = str; if (*sep) { // 如果找到 ';'，终止当前命令，并移动 str 到下一个命令起始位置。 str = sep + 1; /* start of command for next pass */ *sep = '\\0'; /* 把 ';' 替换成 '\\0'，形成单独的字符串 */ } else { // 如果没有 ';'，说明这是最后一个命令。 str = sep; /* no more commands for next pass */ } debug_parser(&quot;token: \\&quot;%s\\&quot;\\n&quot;, token); /* * 处理宏替换，比如 $bootcmd 替换成 &quot;bootm 0x80007fc0&quot;。 */ process_macros(token, finaltoken); /* * 把替换后的命令按空格切分成 argv[]。 * 例如：&quot;bootm 0x80007fc0&quot; → argv[0]=&quot;bootm&quot;, argv[1]=&quot;0x80007fc0&quot; */ argc = cli_simple_parse_line(finaltoken, argv); if (argc == 0) { rc = -1; /* 没有有效命令 */ continue; } /* * 调用命令处理函数： * - 根据 argv[0] 查找命令表 cmd_tbl_t * - 找到后调用对应的执行函数，比如 do_bootm() */ if (cmd_process(flag, argc, argv, &amp;repeatable, NULL)) rc = -1; /* * 如果用户按了 Ctrl-C，立即中止，不再执行后续命令。 */ if (had_ctrlc()) return -1; /* if stopped then not repeatable */ } /* * 返回 rc，如果 rc 为 0，则返回 repeatable，表示该命令是否可以重复执行。 */ return rc ? rc : repeatable;} 核心为： 调用命令处理函数：根据 argv[0] 查找命令表 cmd_tbl_t, 找到后调用对应的执行函数，比如 do_bootm() cmd_process(flag, argc, argv, &amp;repeatable, NULL) cmdtp = find_cmd(argv[0]); /* Look up command in command table */ for (cmdtp = table; cmdtp != table + table_len; cmdtp++) { if (strncmp (cmd, cmdtp-&gt;name, len) == 0) { if (len == strlen (cmdtp-&gt;name)) return cmdtp; /* full match */ cmdtp_temp = cmdtp; /* abbreviated command ? */ n_found++; } } if (argc &gt; cmdtp-&gt;maxargs) /* found - check max args */ rc = cmd_call(cmdtp, flag, argc, argv); /* If OK so far, then do the command */ // 直接调用命令函数指针，相当于执行 do_bootm(cmdtp, flag, argc, argv) result = (cmdtp-&gt;cmd)(cmdtp, flag, argc, argv); U-Boot bootm 命令流程解析 bootm 命令是 U-Boot 用来启动 Linux 内核或其他操作系统映像的入口命令。内部核心是 do_bootm()，它会依次执行几个状态，每个状态对应不同的操作。 1234567do_bootm()└─&gt; do_bootm_states() ├─ BOOTM_STATE_START ├─ BOOTM_STATE_FINDOS ├─ BOOTM_STATE_FINDOTHER ├─ BOOTM_STATE_LOADOS └─ BOOTM_STATE_OS_GO BOOTM_STATE_START → bootm_start() 初始化启动上下文 (bootm_headers 等结构体)。 检查镜像类型（uImage、FIT、Legacy 等）。 验证镜像合法性（如 magic number、checksum）。 确定后续加载流程是否需要解压。 关键点： 这里是整个 bootm 流程的“起点”，决定后续镜像解析路径。 如果启动 FIT 镜像，还会解析 FIT 描述符。 BOOTM_STATE_FINDOS → bootm_find_os() → boot_get_kernel() 找到操作系统内核镜像在内存中的位置。 设置内核映像地址（image-&gt;load）和类型。 对不同内核格式（zImage、uImage）做处理： uImage：提取 header，获取 load address。 zImage：直接获取 load 地址，通常包含自解压代码。 关键点： 这个阶段不执行实际加载，只是确定内核在哪，大小是多少。 对于 FIT 镜像，会解析 kernel 节点及其 load 地址。 BOOTM_STATE_FINDOTHER → bootm_find_other() 查找其他启动必需镜像： ramdisk（initrd/initramfs） FDT（Device Tree Blob，硬件描述） 根据镜像类型和配置设置内存地址。 为内核启动准备参数。 关键点： 确保内核启动时能够找到根文件系统和硬件信息。 FIT 镜像会在这里解析 DTB 节点、ramdisk 节点。 BOOTM_STATE_LOADOS → bootm_load_os() → decomp_image() 将内核镜像从存储介质（flash、SD、TFTP 等）加载到内存。 如果内核是压缩的（如 gzip/zImage/uImage），调用 decomp_image() 解压到目标地址。 同时加载 ramdisk 和 FDT 到内存指定位置。 关键点： 这里是真正把内核和相关资源搬到内存执行区的步骤。 decomp_image() 内部处理压缩算法和解压回调。 BOOTM_STATE_OS_GO → bootm_os_get_boot_func() → 跳转内核 获取内核启动入口（kernel_entry）。 根据内核类型和架构，设置启动参数（如 ATAGs、FDT 指针、ramdisk 地址）。 调用函数指针，真正跳转到内核执行。 关键点： U-Boot 在这里退出，CPU 权限切换到内核环境。 后续流程由内核接管。 初始化本阶段要使用的硬件设备。 检测系统内存映射。 将内核映像和根文件系统映像从Flash读到RAM空间。 为内核设置启动参数。 调用内核。 系统上电后的完整流程 (示意):系统上电 -&gt; 设置为SVC工作模式 -&gt; 关闭看门狗 -&gt; 清空CACHE -&gt; 禁止MMU -&gt; 清空BSS段 -&gt; 一系列硬件的初始化 -&gt; 执行bootcmd中的命令 (加载linux内核) -&gt; 设置为SVC工作模式 -&gt; 检查CPUid是否支持 -&gt; 创建页表 -&gt; 开启MMU -&gt; 创建子线程 -&gt; 子线程中挂载指定的根文件系统 -&gt; 启动用户空间1号进程 -&gt; 开启后续用户空间进程 -&gt; 启动一个shell -&gt; 用户可以输入命令","link":"/post/u-boot.html"}],"tags":[{"name":"compiler","slug":"compiler","link":"/tags/compiler/"},{"name":"platform_bus","slug":"platform-bus","link":"/tags/platform-bus/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"libbpf","slug":"libbpf","link":"/tags/libbpf/"},{"name":"kernel_start","slug":"kernel-start","link":"/tags/kernel-start/"},{"name":"bpftrace","slug":"bpftrace","link":"/tags/bpftrace/"},{"name":"bcc","slug":"bcc","link":"/tags/bcc/"},{"name":"qspinlock","slug":"qspinlock","link":"/tags/qspinlock/"},{"name":"toolchain","slug":"toolchain","link":"/tags/toolchain/"},{"name":"driver_struct","slug":"driver-struct","link":"/tags/driver-struct/"},{"name":"uboot_start","slug":"uboot-start","link":"/tags/uboot-start/"}],"categories":[{"name":"note","slug":"note","link":"/categories/note/"},{"name":"linux","slug":"linux","link":"/categories/linux/"},{"name":"compiler","slug":"note/compiler","link":"/categories/note/compiler/"},{"name":"kernel","slug":"linux/kernel","link":"/categories/linux/kernel/"},{"name":"hexo","slug":"note/hexo","link":"/categories/note/hexo/"},{"name":"toolchain","slug":"note/toolchain","link":"/categories/note/toolchain/"},{"name":"uboot","slug":"linux/uboot","link":"/categories/linux/uboot/"},{"name":"kernel_start","slug":"linux/kernel/kernel-start","link":"/categories/linux/kernel/kernel-start/"},{"name":"driver","slug":"linux/kernel/driver","link":"/categories/linux/kernel/driver/"},{"name":"trace","slug":"linux/kernel/trace","link":"/categories/linux/kernel/trace/"},{"name":"lock","slug":"linux/kernel/lock","link":"/categories/linux/kernel/lock/"},{"name":"uboot_start","slug":"linux/uboot/uboot-start","link":"/categories/linux/uboot/uboot-start/"},{"name":"platform_bus","slug":"linux/kernel/driver/platform-bus","link":"/categories/linux/kernel/driver/platform-bus/"},{"name":"ebpf","slug":"linux/kernel/trace/ebpf","link":"/categories/linux/kernel/trace/ebpf/"},{"name":"qspinlock","slug":"linux/kernel/lock/qspinlock","link":"/categories/linux/kernel/lock/qspinlock/"},{"name":"three_driver_struct","slug":"linux/kernel/driver/three-driver-struct","link":"/categories/linux/kernel/driver/three-driver-struct/"},{"name":"libbpf","slug":"linux/kernel/trace/libbpf","link":"/categories/linux/kernel/trace/libbpf/"},{"name":"bpftrace","slug":"linux/kernel/trace/bpftrace","link":"/categories/linux/kernel/trace/bpftrace/"}],"pages":[{"title":"","text":"Troy's Blog >>> 欢迎交换友链~ 请通过邮件联系我。","link":"/friend/index.html"}]}