{"posts":[{"title":"hexo博客搭建","text":"使用hexo和GitHub Pagtes部署一个自己的博客 1. 安装并初始化Hexo 安装 Hexo CLI 1npm install -g hexo-cli 初始化博客项目目录 123mkdir my-blog &amp;&amp; cd my-bloghexo initnpm install 本地预览 1hexo server 启动本地服务：在浏览器访问 http://localhost:4000 查看效果 2. 配置 GitHub Pages 部署 创建GitHub仓库 创建一个仓库，名字叫 你的GitHub用户名.github.io 比如你是 goko，就叫 goko.github.io 安装部署插件 1npm install hexo-deployer-git --save 修改 _config.yml（根目录下）添加部署配置： 12345deploy: type: git # repo建议使用SSH, SSH免密 repo: https://github.com/你的GitHub用户名/你的GitHub用户名.github.io.git branch: main # 或者 master，看你的默认分支 生成并部署博客 123hexo cleanhexo generatehexo deploy 3. 域名(.com)绑定 添加域名(在my-blog下) 123echo &quot;&lt;xxxx&gt;.com&quot; &gt; source/CNAME# 或者可以：echo &quot;www.&lt;xxxx&gt;.com&quot; &gt; source/CNAME# 只能添加一个，而且两个需要添加不同的域名解析（如下） 重新部署 1hexo clean &amp;&amp; hexo g &amp;&amp; hexo d 设置 DNS 解析指向 GitHub Pages A. 使用裸域名（apex 域名）goku72.com 记录类型 主机记录 记录值 说明 A @ 185.199.108.153 GitHub Pages IP A @ 185.199.109.153 GitHub Pages IP A @ 185.199.110.153 GitHub Pages IP A @ 185.199.111.153 GitHub Pages IP example aliyun: 选择业务需求: 将网站域名解析到服务器IPv4地址 选择网站域名(主机记录): .com（对应设置“@”主机记录） 填写 IP（记录值）： 在输入框里粘贴以下四行（每一行一个 IP）： &gt; 185.199.109.153 &gt; 185.199.108.153 &gt; 185.199.110.153 &gt; 185.199.111.153 B. 使用 www.goku72.com 作为主域名 记录类型 主机记录 记录值 说明 CNAME www &lt;github用户名&gt;.github.io. 指向你的 GitHub 用户页仓库 example aliyun: 选择业务需求: 将网站域名解析到另外的目标域名 选择网站域名(主机记录): www..com（对应设置“www”主机记录） 填写 IP（记录值）：&lt;github用户名&gt;.github.io. (最有有一个符号”.”) 4. 设置主题 cd my-blog/themes git clone -b master https://github.com/jerryc127/hexo-theme-butterfly.git butterfly 修改_config.yml: theme: butterfly hexo clean &amp;&amp; hexo g &amp;&amp; hexo d 更多主题：https://hexo.io/themes/ 注： 如果AB两个方式都添加了，只需要在 Hexo 项目的 source/CNAME 文件中写 www..com，GitHub Pages 就会自动把 goku72.com 重定向过去，无需额外设置！ 后续换域名只需要：阿里云重新解析 + 修改 source/CNAME + 重新部署 Hexo，就能完成域名迁移。 有些主题可能需要下载插件","link":"/post/hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA.html"},{"title":"start-kerneling","text":"前言介绍….","link":"/post/start-kerneling.html"},{"title":"qspinlock","text":"qspinlock is a hybrid spinlock combining the fairness of ticket locks with the scalability of MCS locks: it uses only 4 bytes under low contention, falls back to an MCS queue under heavy load, and optimizes the second contender with a pending bit. It improves fairness and scalability but should not be enabled on RISC-V platforms lacking Ziccrse or Zabha. 1. 传统spinlock： 多个等待的 CPU 核心中，谁先获得锁并无保证，存在公平性问题，同时缓存一致性开销大（如MESI），CPU核心越大，cache需求越厉害，缺乏可扩展性 2. Ticket spinlock1234567891011121314151617#define TICKET_NEXT 16typedef struct { union { u32 lock; struct __raw_tickets { /* little endian */ u16 owner; u16 next; } tickets; };} arch_spinlock_t;my_ticket = atomic_fetch_inc(&amp;lock-&gt;tickets.next); while (lock-&gt;tickets.owner != my_ticket) cpu_relax(); 解决了公平问题，防止某些 CPU 永远得不到锁，但所有核都轮询同一个owner变量，read cache line成热点，限制扩展性 3. MCS lock 本质上是一种基于链表结构的自旋锁，每个CPU有一个对应的节点(锁的副本)，基于各自不同的副本变量进行等待，锁本身是共享的，但队列节点是线程自己维护的，每个CPU只需要查询自己对应的本地cache line，仅在这个变量发生变化的时候，才需要读取内存和刷新这条cache line, 不像 classic/ticket对共享变量进行spin 123456789101112131415161718192021222324struct mcs_spinlock { struct mcs_spinlock *next; int locked; /* 1 if lock acquired */ int count; /* nesting count, see qspinlock.c */};static inlinevoid mcs_spin_lock(struct mcs_spinlock **lock, struct mcs_spinlock *node){ struct mcs_spinlock *prev; /* Init node */ node-&gt;locked = 0; node-&gt;next = NULL; prev = xchg(lock, node); if (likely(prev == NULL)) { return; } WRITE_ONCE(prev-&gt;next, node); /* Wait until the lock holder passes the lock down. */ arch_mcs_spin_lock_contended(&amp;node-&gt;locked);} 每个 CPU 线程创建的node 是独立的，每个线程都有自己的 node 实例。但是结构体中多了一个指针使结构体变大了，导致了“内存开销问题”：MCS 锁把竞争带来的 cache-line 抖动降低了，但牺牲了一些内存和部分结构管理的成本。 4. qspinlockinclude/asm-generic/qspinlock_types.h: 锁数据结构 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758typedef struct qspinlock { union { atomic_t val; /* * By using the whole 2nd least significant byte for the * pending bit, we can allow better optimization of the lock * acquisition for the pending bit holder. */#ifdef __LITTLE_ENDIAN struct { u8 locked; u8 pending; }; struct { u16 locked_pending; u16 tail; };#else struct { u16 tail; u16 locked_pending; }; struct { u8 reserved[2]; u8 pending; u8 locked; };#endif };} arch_spinlock_t;/* * Initializier */#define __ARCH_SPIN_LOCK_UNLOCKED { { .val = ATOMIC_INIT(0) } }/* * Bitfields in the atomic value: * * When NR_CPUS &lt; 16K * 0- 7: locked byte * 8: pending * 9-15: not used * 16-17: tail index * 18-31: tail cpu (+1) * * When NR_CPUS &gt; = 16K * 0- 7: locked byte * 8: pending * 9-10: tail index * 11-31: tail cpu (+1) */#define _Q_SET_MASK(type) (((1U &lt;&lt; _Q_ ## type ## _BITS) - 1)\\ &lt;&lt; _Q_ ## type ## _OFFSET)#define _Q_LOCKED_OFFSET 0#define _Q_LOCKED_BITS 8#define _Q_LOCKED_MASK _Q_SET_MASK(LOCKED) When NR_CPUS &lt; 16K： locked：用来表示这个锁是否被人持有（0：无，1：有） pending：可以理解为最优先持锁位，即当unlock之后只有这个位的CPU最先持锁，也有1和0 tail：有idx+CPU构成，用来标识等待队列的最后一个节点。 tail_idx：就是index，它作为mcs_nodes数组的下标使用 tail_CPU：用来表示CPU的编号+1，+1因为规定tail为0的时候表示等待队列中没有成员 kernel/locking/mcs_spinlock.h 12345struct mcs_spinlock { struct mcs_spinlock *next; int locked; /* 1 if lock acquired */ int count; /* nesting count, see qspinlock.c */}; locked = 1:只是说锁传到了当前加节点，但是当前节点还需要主动申请锁(qspinlock -&gt; locked = 1)count：针对四种上下文用于追踪当前用了第几个 node（即 idx），最大为4,不够用时就fallback不排队直接自旋 kernel/locking/qspinlock.c: 123456789101112131415161718#define MAX_NODES 4struct qnode { struct mcs_spinlock mcs;#ifdef CONFIG_PARAVIRT_SPINLOCKS long reserved[2];#endif};/* * Per-CPU queue node structures; we can never have more than 4 nested * contexts: task, softirq, hardirq, nmi. * * Exactly fits one 64-byte cacheline on a 64-bit architecture. * * PV doubles the storage and uses the second cacheline for PV state. */static DEFINE_PER_CPU_ALIGNED(struct qnode, qnodes[MAX_NODES]); 一个 CPU 上可能嵌套多个锁, qnodes针对四种上下文情况下，例：进程上下文中发生中断后再次获取锁 PER_CPU的优点是快，可防止抢锁时再mallock或临时分配导致延迟，成本等问题 申请锁： 快速申请include/asm-generic/qspinlock.h 12345678910111213/** * queued_spin_lock - acquire a queued spinlock * @lock: Pointer to queued spinlock structure */static __always_inline void queued_spin_lock(struct qspinlock *lock){ int val = 0; if (likely(atomic_try_cmpxchg_acquire(&amp;lock-&gt;val, &amp;val, _Q_LOCKED_VAL))) return; queued_spin_lock_slowpath(lock, val);} 中速申请 快速申请失败，queue中为空时，设置锁的pending位 再次检测（检查中间是否有其它cpu进入） 一直循环检测locked位 当locked位为0时，清除pending位获得锁 慢速申请 申请 操作 快速申请 这个锁当前没有人持有，直接通过cmpxchg()设置locked域即可获取了锁 中速申请 锁已经被人持有，但是MCS链表没有其他人，有且仅有一个人在等待这个锁。设置pending域，表示是第一顺位继承者，自旋等待lock-&gt; locked清0，即锁持有者释放锁 慢速申请 进入到queue中自旋等待，若为队列头（队列中没有等待的cpu），说明它已排到最前，可以开始尝试获取锁；否则，它会自旋等待前一个节点释放锁，并通知它可以尝试获取锁了 end: 如果只有1个或2个CPU试图获取锁，那么只需要一个4字节的qspinlock就可以了，其所占内存的大小和ticket spinlock一样。当有3个以上的CPU试图获取锁，则需要(N-2)个MCS node qspinlock中加入”pending”位域的意义，如果是两个CPU试图获取锁，那么第二个CPU只需要简单地设置”pending”为1，而不用创建一个MCS node 试图加锁的CPU数目超过3个，使用ticket spinlock机制就会造成多个CPU的cache line刷新的问题，而qspinlock可以利用MCS node队列来解决这个问题 在多核争用严重场景下，qspinlock 让等待者在本地内存区域自旋，减少了锁的缓存抖动和对总线的竞争消耗 RISCV_QUEUED_SPINLOCKS 只应在平台具有 Zabha 或 Ziccrse 时启用，不支持的情况不要选用 优先级反转问题，queue会保证了FIFO提高了公平性，但它无法感知任务的优先级，可能因为排在队列前方的低优先级任务未释放锁而发生等待，从而导致 优先级反转","link":"/post/qspinlock.html"},{"title":"riscv-toolchains","text":"riscv-toolchain介绍 GNU 工具链的三元组（triplet），格式是： 1&lt;目标架构&gt;-&lt;供应商&gt;-&lt;目标系统&gt; riscv64-unknown-elf-： 适用于 riscv64 架构、面向嵌入式/裸机平台（使用 newlib 标准库）的工具链。 可从 riscv-collab/riscv-gnu-toolchain 构建。 不推荐通过包管理工具安装此编译链。发行版软件源中的此工具链常常缺少关键的 newlib 标准库。riscv64-unknown-linux-gnu-： 适用于 riscv64 架构、面向 linux-gnu 平台（使用 glibc 标准库）的工具链，可以与riscv64-linux-gnu- 相互替换。可从 riscv-collab/riscv-gnu-toolchain 构建。riscv64-linux-gnu-： 适用于 riscv64 架构、面向 linux-gnu 平台（使用 glibc 标准库）的工具链。 可通过包管理工具（如 Ubuntu 的 apt）安装。受限于发行版软件源限制，其编译器版本可能较老。","link":"/post/riscv-toolchains.html"},{"title":"IIC","text":"EEPROM(IIC) ROM(Read Only Memory): 制造和升级不便 PROM(Programmable ROM): 但是只能写入一次，后续无法修改 EPROM(Erasable Programmable ROM): 紫外线透过玻璃窗口照射内部芯片就可以擦除其内部的数据 EEPROM: 带电可擦除可编程只读存储器，以电子信号来修改其内容，它属于双电压芯片。借助于EEPROM芯片的双电压特性，可以使BIOS具有良好的防毒功能，在升级时，把跳线开关打至“on”的位置，即给芯片加上相应的编程电压，就可以方便地升级；平时使用时，则把跳线开关打至“off”的位置，防止CIH类的病毒对BIOS芯片的非法修改。 读取IIC设备： 控制器 设备地址：根据芯片手册和模块原理图查询查找IIC设备：i2cdetect -y 0/1/2… Platform 总线 (Platform Bus): 特点: 这是一种虚拟总线，用于连接那些没有硬件总线（如PCI、USB等）但又需要与CPU直接通信的设备。这些设备通常是SoC（System on Chip）内部的各种控制器，例如GPIO控制器、UART、SPI、I2C控制器本身等等。 作用: 它提供了一种统一的机制来管理和抽象这些片上设备，使得驱动开发者不需要关心具体的硬件地址和中断号，而是通过 Platform 总线提供的接口来注册和操作设备。 原理: Platform 设备和 Platform 驱动通过 platform_device 和 platform_driver 结构体进行描述。当设备和驱动的名称匹配时，内核就会将它们关联起来。 匹配过程: 通常是基于 name 字段的字符串匹配。platform_device 中的 name 字段与 platform_driver 中 driver.name 字段进行比较。如果匹配成功，就会调用驱动的 probe 函数。 I2C 总线 (I2C Bus): 特点: I2C 是一种串行通信协议，用于连接低速外设，如传感器、EEPROM、实时时钟 (RTC) 等。在 Linux 中，I2C 总线管理着I2C控制器和I2C从设备。 作用: 它为I2C设备提供了一套标准的API，使得驱动开发者可以方便地读写I2C设备寄存器，而无需关心I2C协议的底层细节。 原理: I2C 总线包含I2C适配器（I2C Adapter，即I2C控制器）和I2C客户端（I2C Client，即I2C从设备）。适配器提供I2C通信能力，客户端则代表具体的I2C设备。 匹配过程: I2C设备的匹配通常有两种方式： 基于名称匹配: i2c_client 中的 name 字段与 i2c_driver 中的 id_table 里的 name 字段进行匹配。 基于compatible字符串匹配 (更常用和推荐): i2c_client 中的 of_node-&gt;compatible 属性与 i2c_driver 中的 of_match_table 里的 compatible 字符串进行匹配。这种方式常用于设备树 (Device Tree) 中。 除了这两者，还有： PCI 总线 (PCI Bus): 用于连接高性能外设，如显卡、网卡、声卡等。 USB 总线 (USB Bus): 用于连接各种USB设备，如U盘、键盘、鼠标、摄像头等。 SPI 总线 (SPI Bus): 另一种串行通信协议，常用于连接传感器、FLASH存储器等。 MMC/SD 总线 (MMC/SD Bus): 用于连接SD卡、eMMC存储器等。 Input 总线 (Input Bus): 用于管理各种输入设备，如键盘、鼠标、触摸屏等。 等等… 2. 它们之间有什么区别和联系？都有什么用？原理是什么？区别： 物理特性: I2C: 是一种串行通信协议，有SDA（数据线）和SCL（时钟线）两根线。 Platform: 是一种虚拟总线，没有对应的物理连接线，它抽象的是CPU内部或直接连接到CPU的设备。 其他总线 (PCI, USB等): 都有各自的物理连接方式和通信协议。 设备类型: I2C: 专注于低速外设。 Platform: 专注于SoC内部或直接连接的片上设备。 PCI: 专注于高性能、高带宽设备。 USB: 专注于即插即用、通用性强的设备。 联系： 统一的设备模型: 尽管有各种不同的总线，但 Linux 内核提供了一个统一的设备模型 (Device Model)。这个模型的目标是将设备和驱动进行分离，实现通用化和可移植性。 Bus-Device-Driver 架构: 所有的总线都遵循 Bus-Device-Driver 架构。 Bus (总线): 负责管理其上的设备和驱动，提供匹配机制。 Device (设备): 代表具体的硬件设备，包含设备的各种信息（地址、中断号、名称、ID等）。 Driver (驱动): 负责与特定类型的设备进行交互，实现设备的各种功能。 Client-Driver 适配: 你说的没错！无论是 I2C、Platform 还是其他总线，它们的核心都是 Client (设备) 和 Driver (驱动) 进行适配。这里的 “Client” 对应于 “Device”。 都有什么用？ 抽象硬件差异: 不同的总线屏蔽了底层硬件的复杂性，提供统一的编程接口。 代码复用: 驱动可以独立于具体的硬件平台开发，只要设备模型支持，就可以在不同的平台上运行。 模块化管理: 允许设备和驱动作为独立的模块加载和卸载，提高了系统的灵活性。 即插即用 (Plug-and-Play): 对于PCI、USB等支持热插拔的总线，设备模型可以实现设备的动态识别和加载驱动。 原理： Linux 设备模型的核心思想是 “将设备和驱动分离”。它通过 struct device 和 struct device_driver 这两个核心结构体来表示设备和驱动。 struct bus_type: 定义了总线的属性和操作，包括设备的注册、驱动的注册、设备的遍历、驱动的匹配函数等。 struct device: 代表一个具体的硬件设备。它包含设备的通用属性（如名称、父设备、设备树节点等），以及特定总线的私有数据。 struct device_driver: 代表一个设备驱动。它包含驱动的通用属性（如名称、驱动所支持的设备ID表），以及驱动的操作函数（如 probe、remove 等）。 当一个设备被注册到总线上时，总线会遍历所有已注册的驱动，尝试找到能够与该设备匹配的驱动。一旦匹配成功，就会调用驱动的 probe 函数来初始化设备。 3. 驱动不就是一份代码吗？为什么还有driver的代码和client的代码，两份？这是一个很好的问题，也是很多初学者容易混淆的地方。 驱动 (Driver) 确实是“一份代码”，但这份代码是为了管理一类特定功能的硬件设备。 为了实现驱动的通用性和可移植性，Linux 设备模型将驱动分成了两个逻辑部分： 设备 (Device) 的描述代码 (Client/Platform Device): 这部分代码主要负责 描述硬件设备的信息，而不是实现设备的功能。它告诉内核：“这里有一个设备，它的类型是什么，它连接在哪个总线上，它的地址是多少，它需要哪些资源（如中断、内存区域）”。 这部分代码通常位于 板级文件 (board-specific file) 或 设备树 (Device Tree) 中。 例如： 对于 Platform 设备，你会看到 struct platform_device 的定义，里面包含了设备的名称、资源等。 对于 I2C 设备，你会看到 struct i2c_client 的定义，或者在设备树中描述I2C设备的节点。 驱动 (Driver) 的功能实现代码 (Platform Driver/I2C Driver): 这部分代码才是真正意义上的 “驱动”。它负责 实现与特定类型设备进行交互的逻辑。 它包含了设备初始化（probe 函数）、数据传输、中断处理、电源管理等核心功能。 这部分代码通常位于 独立的驱动文件 中（例如：drivers/char/xxx.c, drivers/i2c/chips/yyy.c, drivers/platform/zzz.c）。 例如： 对于 Platform 驱动，你会看到 struct platform_driver 的定义，其中包含了 probe、remove 等函数指针。 对于 I2C 驱动，你会看到 struct i2c_driver 的定义，同样包含 probe、remove 等函数指针。 为什么会有两份？ 分离关注点 (Separation of Concerns): 将设备描述和驱动功能分离，使得驱动代码更加通用。同一个驱动可以在不同的硬件平台上使用，只要这些平台能够正确描述出该设备。 板级厂商只需要描述其硬件设备的特性，而驱动开发者可以专注于实现设备功能。 可移植性 (Portability): 驱动代码可以独立于具体的硬件平台编译和加载。 当硬件平台发生变化时，可能只需要修改设备描述部分（如设备树），而无需修改驱动代码本身。 模块化 (Modularity): 设备和驱动可以作为独立的模块动态加载和卸载，方便开发和调试。 即插即用 (Plug and Play): 当设备被发现时（例如插入USB设备），内核可以根据设备的描述信息自动寻找并加载对应的驱动。 4. Match 的过程是什么样子的呢？匹配 (Matching) 是设备模型中最核心的机制之一。当一个设备被注册到总线上时，内核会触发匹配过程，寻找能够驱动该设备的驱动。 通用匹配流程 (以 Platform 总线为例)： 设备注册: 当内核或某个模块发现并注册一个 platform_device 到 platform_bus 上时，匹配过程开始。 这通常发生在内核启动时，或者通过设备树动态解析设备。 platform_device_register() 函数会被调用。 遍历驱动: platform_bus 会遍历所有已经注册到它上面的 platform_driver。 调用匹配函数: 对于每一个 platform_driver，总线会调用其内部的 match 函数（对于 Platform 总线，通常是 platform_match()）。 匹配逻辑: match 函数会根据预定的规则（通常是名称匹配或 compatible 字符串匹配）来判断当前设备是否与当前驱动兼容。 名称匹配: platform_device-&gt;name == platform_driver-&gt;driver.name。 设备树 compatible 匹配 (更常用): platform_device-&gt;of_node-&gt;compatible 属性与 platform_driver-&gt;of_match_table 中的 compatible 字符串进行比较。 成功匹配: 如果 match 函数返回成功（表示设备和驱动匹配），那么总线就会将该设备与该驱动关联起来。 调用 probe 函数: 接着，内核会调用匹配成功的 platform_driver 的 probe 函数。 probe 函数是驱动的核心，它负责初始化设备、请求资源、注册中断、创建设备节点等等。 如果 probe 函数成功返回，表示设备驱动加载成功；如果返回错误码，则表示加载失败。 I2C 总线匹配流程： I2C 总线的匹配与 Platform 类似，但它有自己的 i2c_bus_type 和匹配函数。 I2C 适配器注册: I2C 控制器作为 Platform 设备注册后，其 Platform 驱动会注册 i2c_adapter，这代表了一个可用的 I2C 总线。 I2C 客户端注册: I2C 设备的描述（通常在设备树中）被解析后，会创建一个 i2c_client 结构体并将其注册到对应的 i2c_adapter 上。 遍历 I2C 驱动: I2C 总线会遍历所有已经注册的 i2c_driver。 调用匹配函数: I2C 总线会调用其内部的匹配函数 (i2c_device_match())。 匹配逻辑: ID 表匹配: i2c_driver-&gt;id_table 字段中的 name 成员与 i2c_client-&gt;name 进行匹配。 设备树 compatible 匹配 (更常用): i2c_client-&gt;dev.of_node-&gt;compatible 属性与 i2c_driver-&gt;driver.of_match_table 中的 compatible 字符串进行比较。 成功匹配: 匹配成功后，调用 i2c_driver 的 probe 函数。 总结： Linux 设备模型的核心是 Bus-Device-Driver 架构，旨在将设备和驱动分离，实现代码的通用性和可移植性。 总线 (Bus) 负责管理设备和驱动的注册与匹配。 设备 (Device/Client) 描述了硬件的特性和资源。 驱动 (Driver) 实现了与设备交互的逻辑。 匹配过程是总线根据预定的规则（名称或 compatible 字符串）将设备和驱动关联起来的过程，成功后会调用驱动的 probe 函数。 希望这些解释能帮助你更好地理解 Linux 设备驱动的 Bus-Device-Driver 架构！这是一个值得深入学习的领域，一旦掌握，你会对操作系统如何与硬件交互有更深刻的理解。","link":"/post/wds-IIC.html"}],"tags":[{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"kernel-start","slug":"kernel-start","link":"/tags/kernel-start/"},{"name":"riscv","slug":"riscv","link":"/tags/riscv/"},{"name":"riscv-toolchain","slug":"riscv-toolchain","link":"/tags/riscv-toolchain/"},{"name":"linux-dirvers","slug":"linux-dirvers","link":"/tags/linux-dirvers/"}],"categories":[],"pages":[{"title":"","text":"Troy's Blog >>> 欢迎交换友链~ 请通过邮件联系我。","link":"/friend/index.html"}]}